{
  "total_examples": 155,
  "valid_syntax": 120,
  "invalid_syntax": 35,
  "examples": [
    {
      "file_path": "docs/deployment/production.md",
      "line_number": 322,
      "language": "python",
      "code": "# Health check endpoint\nfrom flask import Flask, jsonify\nfrom codomyrmex.logging_monitoring import get_system_metrics\n\napp = Flask(__name__)\n\n@app.route('/health')\ndef health_check():\n    \"\"\"Detailed health check for production monitoring.\"\"\"\n    try:\n        # Check database connectivity\n        db_status = check_database_connection()\n\n        # Check cache connectivity\n        cache_status = check_redis_connection()\n\n        # Check API key validity\n        api_status = check_api_keys()\n\n        # Check disk space\n        disk_status = check_disk_space()\n\n        # Check memory usage\n        memory_status = check_memory_usage()\n\n        all_healthy = all([\n            db_status['healthy'],\n            cache_status['healthy'],\n            api_status['healthy'],\n            disk_status['healthy'],\n            memory_status['healthy']\n        ])\n\n        return jsonify({\n            'status': 'healthy' if all_healthy else 'unhealthy',\n            'timestamp': datetime.utcnow().isoformat(),\n            'checks': {\n                'database': db_status,\n                'cache': cache_status,\n                'apis': api_status,\n                'disk': disk_status,\n                'memory': memory_status\n            },\n            'version': get_version(),\n            'uptime': get_uptime()\n        }), 200 if all_healthy else 503\n\n    except Exception as e:\n        return jsonify({\n            'status': 'error',\n            'error': str(e),\n            'timestamp': datetime.utcnow().isoformat()\n        }), 503",
      "syntax_valid": true,
      "import_errors": [],
      "api_mismatches": []
    },
    {
      "file_path": "docs/deployment/production.md",
      "line_number": 379,
      "language": "python",
      "code": "# metrics.py - Production metrics collection\nfrom prometheus_client import Counter, Histogram, Gauge, generate_latest\nfrom codomyrmex.logging_monitoring import get_logger\n\nlogger = get_logger(__name__)\n\n# Define metrics\nREQUEST_COUNT = Counter('codomyrmex_requests_total', 'Total requests', ['method', 'endpoint', 'status'])\nREQUEST_DURATION = Histogram('codomyrmex_request_duration_seconds', 'Request duration')\nACTIVE_CONNECTIONS = Gauge('codomyrmex_active_connections', 'Active connections')\nQUEUE_SIZE = Gauge('codomyrmex_queue_size', 'Current queue size', ['queue_name'])\n\n# AI API metrics\nAI_API_CALLS = Counter('codomyrmex_ai_api_calls_total', 'AI API calls', ['provider', 'model', 'status'])\nAI_API_DURATION = Histogram('codomyrmex_ai_api_duration_seconds', 'AI API call duration', ['provider'])\nAI_API_TOKENS = Histogram('codomyrmex_ai_api_tokens_used', 'AI API tokens used', ['provider', 'type'])\n\n# Module-specific metrics\nMODULE_EXECUTIONS = Counter('codomyrmex_module_executions_total', 'Module executions', ['module', 'status'])\nMODULE_DURATION = Histogram('codomyrmex_module_duration_seconds', 'Module execution duration', ['module'])\n\n@app.route('/metrics')\ndef metrics():\n    \"\"\"Prometheus metrics endpoint.\"\"\"\n    return generate_latest()",
      "syntax_valid": true,
      "import_errors": [],
      "api_mismatches": []
    },
    {
      "file_path": "docs/deployment/production.md",
      "line_number": 408,
      "language": "python",
      "code": "# production_logging.py\nimport logging\nimport logging.config\nfrom pythonjsonlogger import jsonlogger\n\nLOGGING_CONFIG = {\n    'version': 1,\n    'disable_existing_loggers': False,\n    'formatters': {\n        'json': {\n            '()': 'pythonjsonlogger.jsonlogger.JsonFormatter',\n            'format': '%(asctime)s %(name)s %(levelname)s %(message)s %(pathname)s %(lineno)d'\n        }\n    },\n    'handlers': {\n        'console': {\n            'class': 'logging.StreamHandler',\n            'formatter': 'json',\n            'stream': 'ext://sys.stdout'\n        },\n        'file': {\n            'class': 'logging.handlers.RotatingFileHandler',\n            'formatter': 'json',\n            'filename': '/app/logs/codomyrmex.log',\n            'maxBytes': 100 * 1024 * 1024,  # 100MB\n            'backupCount': 10\n        }\n    },\n    'root': {\n        'level': 'INFO',\n        'handlers': ['console', 'file']\n    },\n    'loggers': {\n        'codomyrmex': {\n            'level': 'INFO',\n            'handlers': ['console', 'file'],\n            'propagate': False\n        },\n        'codomyrmex.ai_code_editing': {\n            'level': 'DEBUG',  # More verbose for AI operations\n            'handlers': ['file'],\n            'propagate': True\n        }\n    }\n}\n\nlogging.config.dictConfig(LOGGING_CONFIG)",
      "syntax_valid": true,
      "import_errors": [],
      "api_mismatches": []
    },
    {
      "file_path": "docs/deployment/production.md",
      "line_number": 461,
      "language": "python",
      "code": "# caching.py - Production caching\nimport redis\nfrom functools import wraps\nimport pickle\nimport hashlib\n\nredis_client = redis.Redis.from_url(os.getenv('REDIS_URL'))\n\ndef cache_result(expiry=3600, key_prefix='codomyrmex'):\n    \"\"\"Cache function results in Redis.\"\"\"\n    def decorator(func):\n        @wraps(func)\n        def wrapper(*args, **kwargs):\n            # Generate cache key\n            key_data = f\"{func.__module__}.{func.__name__}:{args}:{kwargs}\"\n            cache_key = f\"{key_prefix}:{hashlib.md5(key_data.encode()).hexdigest()}\"\n\n            # Try to get from cache\n            cached = redis_client.get(cache_key)\n            if cached:\n                return pickle.loads(cached)\n\n            # Execute function and cache result\n            result = func(*args, **kwargs)\n            redis_client.setex(cache_key, expiry, pickle.dumps(result))\n\n            return result\n        return wrapper\n    return decorator\n\n# Usage example\n@cache_result(expiry=1800)  # 30 minutes\ndef analyze_large_codebase(codebase_path):\n    \"\"\"Cache expensive static analysis results.\"\"\"\n    from codomyrmex.static_analysis import analyze_codebase\n    return analyze_codebase(codebase_path)",
      "syntax_valid": true,
      "import_errors": [],
      "api_mismatches": []
    },
    {
      "file_path": "docs/deployment/production.md",
      "line_number": 501,
      "language": "python",
      "code": "# async_workers.py - Background task processing\nfrom celery import Celery\nfrom codomyrmex.ai_code_editing import enhance_code\nfrom codomyrmex.logging_monitoring import get_logger\n\n# Configure Celery for async processing\ncelery_app = Celery('codomyrmex',\n                   broker=os.getenv('REDIS_URL'),\n                   backend=os.getenv('REDIS_URL'))\n\nlogger = get_logger(__name__)\n\n@celery_app.task(bind=True, max_retries=3)\ndef async_code_enhancement(self, code, enhancement_options):\n    \"\"\"Process code enhancement asynchronously.\"\"\"\n    try:\n        result = enhance_code(code, **enhancement_options)\n        logger.info(f\"Code enhancement completed for task {self.request.id}\")\n        return result\n\n    except Exception as exc:\n        logger.error(f\"Code enhancement failed: {exc}\")\n        # Retry with exponential backoff\n        raise self.retry(exc=exc, countdown=60 * (2 ** self.request.retries))\n\n@celery_app.task\ndef batch_analysis_task(file_paths):\n    \"\"\"Process multiple files in batch.\"\"\"\n    from codomyrmex.static_analysis import analyze_file\n    results = {}\n\n    for file_path in file_paths:\n        try:\n            results[file_path] = analyze_file(file_path)\n        except Exception as e:\n            results[file_path] = {'error': str(e)}\n\n    return results",
      "syntax_valid": true,
      "import_errors": [],
      "api_mismatches": []
    },
    {
      "file_path": "docs/deployment/production.md",
      "line_number": 696,
      "language": "python",
      "code": "# state_backup.py - Backup application state\nimport json\nimport boto3\nfrom datetime import datetime\nfrom codomyrmex.logging_monitoring import get_logger\n\nlogger = get_logger(__name__)\n\ndef backup_application_state():\n    \"\"\"Backup critical application state to S3.\"\"\"\n    s3_client = boto3.client('s3')\n    timestamp = datetime.utcnow().strftime('%Y%m%d_%H%M%S')\n\n    # Collect state information\n    state = {\n        'timestamp': timestamp,\n        'active_configurations': get_active_configurations(),\n        'user_preferences': export_user_preferences(),\n        'module_settings': export_module_settings(),\n        'api_rate_limits': get_rate_limit_state(),\n        'cache_keys': list_important_cache_keys()\n    }\n\n    # Upload to S3\n    backup_key = f\"application-state/backup_{timestamp}.json\"\n    s3_client.put_object(\n        Bucket='codomyrmex-backups',\n        Key=backup_key,\n        Body=json.dumps(state, indent=2),\n        ServerSideEncryption='AES256'\n    )\n\n    logger.info(f\"Application state backup completed: {backup_key}\")",
      "syntax_valid": true,
      "import_errors": [],
      "api_mismatches": []
    },
    {
      "file_path": "docs/development/documentation.md",
      "line_number": 140,
      "language": "python",
      "code": "# \u2705 Good example\nfrom codomyrmex.logging_monitoring import setup_logging, get_logger\nfrom codomyrmex.data_visualization import create_line_plot\nimport numpy as np\n\n# Initialize logging (required)\nsetup_logging()\nlogger = get_logger(__name__)\n\ntry:\n    # Generate sample data\n    x = np.linspace(0, 10, 100)\n    y = np.sin(x)\n    \n    # Create visualization\n    create_line_plot(\n        x_data=x, \n        y_data=y, \n        title=\"Sine Wave\",\n        output_path=\"sine_wave.png\"\n    )\n    logger.info(\"Plot created successfully\")\n    \nexcept Exception as e:\n    logger.error(f\"Failed to create plot: {e}\")\n    raise\n\n# \u274c Bad example\nresult = some_function()\nprint(result)",
      "syntax_valid": true,
      "import_errors": [],
      "api_mismatches": []
    },
    {
      "file_path": "docs/development/environment-setup.md",
      "line_number": 210,
      "language": "python",
      "code": "# testing/unit/test_my_module.py\nimport pytest\nfrom unittest.mock import patch, MagicMock\n\nfrom codomyrmex.my_module import my_function\n\n\nclass TestMyModule:\n    \"\"\"Test suite for my_module\"\"\"\n    \n    def test_my_function_success(self):\n        \"\"\"Test successful execution of my_function\"\"\"\n        result = my_function(\"input\")\n        assert result == \"expected_output\"\n    \n    def test_my_function_error_handling(self):\n        \"\"\"Test error handling in my_function\"\"\"\n        with pytest.raises(ValueError):\n            my_function(\"invalid_input\")\n    \n    @patch('codomyrmex.my_module.external_dependency')\n    def test_my_function_with_mock(self, mock_dep):\n        \"\"\"Test my_function with mocked dependencies\"\"\"\n        mock_dep.return_value = \"mocked_result\"\n        result = my_function(\"input\")\n        assert result == \"mocked_result\"\n        mock_dep.assert_called_once_with(\"input\")",
      "syntax_valid": true,
      "import_errors": [],
      "api_mismatches": []
    },
    {
      "file_path": "docs/development/environment-setup.md",
      "line_number": 411,
      "language": "python",
      "code": "# Use the logging system for debugging\nfrom codomyrmex.logging_monitoring import get_logger\nlogger = get_logger(__name__)\n\n# Add debug logging\nlogger.debug(f\"Function called with args: {args}\")\nlogger.info(f\"Processing {len(items)} items\")\n\n# Use breakpoint() for interactive debugging\ndef my_function(data):\n    breakpoint()  # Python 3.7+ built-in debugger\n    return process(data)",
      "syntax_valid": true,
      "import_errors": [],
      "api_mismatches": []
    },
    {
      "file_path": "docs/development/testing-strategy.md",
      "line_number": 36,
      "language": "python",
      "code": "# Example: Testing data visualization functions (ACTUAL IMPLEMENTATION)\nimport pytest\nfrom codomyrmex.data_visualization.line_plot import create_line_plot\nimport matplotlib\nmatplotlib.use('Agg')  # Non-interactive backend for testing\nfrom pathlib import Path\n\ndef test_create_line_plot_basic():\n    \"\"\"Test basic line plot creation with real data and real function.\"\"\"\n    # Real data from actual test files\n    x_data = [1, 2, 3, 4, 5]\n    y_data = [2, 4, 6, 8, 10]\n\n    # Call actual implemented function with exact signature\n    fig = create_line_plot(\n        x_data=x_data,\n        y_data=y_data,\n        title=\"Real Test Plot\",\n        output_path=\"test_plot.png\",\n        markers=True\n    )\n\n    # Real assertions based on actual return values\n    assert fig is not None  # Returns matplotlib Figure object\n    assert Path(\"test_plot.png\").exists()  # File actually created\n\n    # Cleanup\n    Path(\"test_plot.png\").unlink(missing_ok=True)",
      "syntax_valid": true,
      "import_errors": [],
      "api_mismatches": []
    },
    {
      "file_path": "docs/development/testing-strategy.md",
      "line_number": 76,
      "language": "python",
      "code": "# Example: Static Analysis integration (ACTUAL IMPLEMENTATION)\ndef test_static_analysis_real():\n    \"\"\"Test static analysis with real Pyrefly integration.\"\"\"\n    from codomyrmex.static_analysis.pyrefly_runner import run_pyrefly_analysis, parse_pyrefly_output\n    import tempfile\n    from pathlib import Path\n\n    # Create real test file\n    with tempfile.TemporaryDirectory() as temp_dir:\n        test_file = Path(temp_dir) / \"test_code.py\"\n        test_file.write_text(\"\"\"\ndef calculate_total(items):\n    total = 0\n    for item in items:\n        total += item\n    return total\n\n# This will cause a Pyrefly error if undefined_var is used\n# result = undefined_var + 1\n\"\"\")\n\n        # Test real Pyrefly output parsing\n        pyrefly_output = f\"{test_file}:8:10: error: Undefined name 'undefined_var'\"\n        issues = parse_pyrefly_output(pyrefly_output, temp_dir)\n\n        # Real assertions\n        assert len(issues) >= 0  # May be 0 if no real issues\n        if issues:\n            assert issues[0][\"file_path\"] == \"test_code.py\"\n            assert issues[0][\"line_number\"] == 8\n            assert \"undefined_var\" in issues[0][\"message\"]",
      "syntax_valid": true,
      "import_errors": [],
      "api_mismatches": []
    },
    {
      "file_path": "docs/development/testing-strategy.md",
      "line_number": 113,
      "language": "python",
      "code": "# Example: Complete development workflow (ACTUAL IMPLEMENTATION)\ndef test_complete_development_workflow():\n    \"\"\"Test full development cycle with real implemented functions.\"\"\"\n    import tempfile\n    from pathlib import Path\n    from codomyrmex.static_analysis.pyrefly_runner import parse_pyrefly_output\n    from codomyrmex.code_execution_sandbox.code_executor import execute_code\n    from codomyrmex.data_visualization.line_plot import create_line_plot\n\n    with tempfile.TemporaryDirectory() as tmp_dir:\n        project_path = Path(tmp_dir)\n\n        # 1. Create real sample project\n        test_file = project_path / \"test_module.py\"\n        test_file.write_text(\"\"\"\ndef calculate_fibonacci(n):\n    if n <= 1:\n        return n\n    return calculate_fibonacci(n-1) + calculate_fibonacci(n-2)\n\ndef main():\n    result = calculate_fibonacci(10)\n    print(f\"Fibonacci(10) = {result}\")\n\nif __name__ == \"__main__\":\n    main()\n\"\"\")\n\n        # 2. Test static analysis parsing (real function)\n        sample_output = f\"{test_file}:2:5: info: Function definition\"\n        issues = parse_pyrefly_output(sample_output, str(project_path))\n        assert isinstance(issues, list)  # Real function returns list\n\n        # 3. Test code execution (real function)\n        execution_result = execute_code(\n            code=\"print('Testing workflow')\",\n            language=\"python\",\n            timeout=10\n        )\n        assert execution_result['success'] == True\n        assert 'Testing workflow' in execution_result['output']\n\n        # 4. Test visualization (real function)\n        x_data = [1, 2, 3, 4, 5]\n        y_data = [1, 1, 2, 3, 5]  # First few Fibonacci numbers\n        fig = create_line_plot(\n            x_data=x_data,\n            y_data=y_data,\n            title=\"Fibonacci Sequence\",\n            output_path=str(project_path / \"fibonacci.png\")\n        )\n        assert fig is not None\n        assert (project_path / \"fibonacci.png\").exists()",
      "syntax_valid": true,
      "import_errors": [],
      "api_mismatches": []
    },
    {
      "file_path": "docs/development/testing-strategy.md",
      "line_number": 206,
      "language": "python",
      "code": "def test_data_visualization_line_plot():\n    \"\"\"\n    \u2705 GOOD TEST EXAMPLE\n\n    - Clear name describing what's being tested\n    - Tests one specific behavior\n    - Uses real data, no mocks\n    - Has clear assertions\n    - Includes error cases\n    \"\"\"\n    # Arrange: Setup real test data\n    x_data = [1, 2, 3, 4, 5]\n    y_data = [2, 4, 6, 8, 10]\n\n    # Act: Execute the function\n    result = create_line_plot(\n        x=x_data,\n        y=y_data,\n        title=\"Test Linear Data\",\n        output_path=\"test_linear.png\"\n    )\n\n    # Assert: Verify expected outcomes\n    assert result.success == True\n    assert Path(\"test_linear.png\").exists()\n    assert result.metadata['correlation'] > 0.95  # Strong linear correlation\n\n    # Test error case\n    with pytest.raises(ValueError, match=\"Empty data\"):\n        create_line_plot(x=[], y=[], title=\"Empty\")",
      "syntax_valid": true,
      "import_errors": [],
      "api_mismatches": []
    },
    {
      "file_path": "docs/development/testing-strategy.md",
      "line_number": 240,
      "language": "python",
      "code": "# \u274c BAD: Vague test name\ndef test_plotting():\n    pass\n\n# \u274c BAD: Testing multiple behaviors\ndef test_plotting_and_analysis_and_ai():\n    pass\n\n# \u274c BAD: Mock everything (against our principles)\n@mock.patch('matplotlib.pyplot')\ndef test_with_mocks():\n    pass\n\n# \u274c BAD: No clear assertions\ndef test_something():\n    result = do_something()\n    # What are we actually testing?",
      "syntax_valid": true,
      "import_errors": [],
      "api_mismatches": []
    },
    {
      "file_path": "docs/development/testing-strategy.md",
      "line_number": 320,
      "language": "python",
      "code": "# Example: environment_setup testing (ACTUAL IMPLEMENTATION)\ndef test_environment_validation():\n    \"\"\"Test environment setup with real system checks.\"\"\"\n    from codomyrmex.environment_setup.env_checker import (\n        is_uv_available,\n        is_uv_environment,\n        check_docker_available\n    )\n    from codomyrmex.code_execution_sandbox.code_executor import check_docker_available\n\n    # Test real UV availability check\n    uv_available = is_uv_available()\n    assert isinstance(uv_available, bool)  # Function returns bool\n\n    # Test real UV environment check\n    in_uv_env = is_uv_environment()\n    assert isinstance(in_uv_env, bool)  # Function returns bool\n\n    # Test real Docker availability check\n    docker_available = check_docker_available()\n    assert isinstance(docker_available, bool)  # Function returns bool\n\n    # Log results for debugging\n    print(f\"UV available: {uv_available}\")\n    print(f\"In UV environment: {in_uv_env}\")\n    print(f\"Docker available: {docker_available}\")",
      "syntax_valid": true,
      "import_errors": [],
      "api_mismatches": []
    },
    {
      "file_path": "docs/development/testing-strategy.md",
      "line_number": 350,
      "language": "python",
      "code": "# Example: Code execution testing (ACTUAL IMPLEMENTATION - AI not yet implemented)\ndef test_code_execution_real():\n    \"\"\"Test real code execution functionality.\"\"\"\n    from codomyrmex.code_execution_sandbox.code_executor import execute_code, validate_language\n\n    # Test language validation (real function)\n    assert validate_language(\"python\") == True\n    assert validate_language(\"javascript\") == True\n    assert validate_language(\"nonexistent\") == False\n\n    # Test real code execution\n    result = execute_code(\n        code=\"def add(a, b):\\n    return a + b\\n\\nprint(add(2, 3))\",\n        language=\"python\",\n        timeout=10\n    )\n\n    # Real assertions based on actual return structure\n    assert result['success'] == True\n    assert '5' in result['output']  # Result of add(2, 3)\n    assert result['execution_time'] > 0\n    assert result['language'] == 'python'",
      "syntax_valid": true,
      "import_errors": [],
      "api_mismatches": []
    },
    {
      "file_path": "docs/development/testing-strategy.md",
      "line_number": 376,
      "language": "python",
      "code": "# Example: Testing build_synthesis (ACTUAL IMPLEMENTATION)\ndef test_build_synthesis_integration():\n    \"\"\"Test build synthesis using real implemented functions.\"\"\"\n    from codomyrmex.build_synthesis.build_orchestrator import (\n        check_build_environment,\n        validate_build_output,\n        synthesize_build_artifact\n    )\n    import tempfile\n    from pathlib import Path\n\n    # Test real build environment check\n    env_result = check_build_environment()\n    assert isinstance(env_result, dict)\n    assert 'python_available' in env_result\n\n    # Test real build artifact synthesis\n    with tempfile.TemporaryDirectory() as temp_dir:\n        source_path = Path(temp_dir) / \"src\"\n        source_path.mkdir()\n\n        # Create sample Python file\n        (source_path / \"main.py\").write_text(\"\"\"\ndef main():\n    print(\"Hello from build test!\")\n\nif __name__ == \"__main__\":\n    main()\n\"\"\")\n\n        output_path = Path(temp_dir) / \"output\"\n\n        # Test real synthesis\n        success = synthesize_build_artifact(\n            source_path=str(source_path),\n            output_path=str(output_path),\n            artifact_type=\"package\"\n        )\n\n        # Real assertions\n        assert isinstance(success, bool)\n        if success:\n            validation_result = validate_build_output(str(output_path))\n            assert isinstance(validation_result, dict)",
      "syntax_valid": true,
      "import_errors": [],
      "api_mismatches": []
    },
    {
      "file_path": "docs/development/testing-strategy.md",
      "line_number": 426,
      "language": "python",
      "code": "import time\nimport pytest\n\ndef test_large_dataset_visualization_performance():\n    \"\"\"Test visualization performance with large datasets.\"\"\"\n    import numpy as np\n    from codomyrmex.data_visualization import create_line_plot\n\n    # Large dataset (100k points)\n    x = np.linspace(0, 1000, 100000)\n    y = np.sin(x) * np.random.random(100000)\n\n    start_time = time.time()\n    result = create_line_plot(x, y, title=\"Large Dataset Test\")\n    duration = time.time() - start_time\n\n    assert result.success == True\n    assert duration < 10.0  # Should complete within 10 seconds\n    assert result.memory_usage_mb < 500  # Memory efficiency check",
      "syntax_valid": true,
      "import_errors": [],
      "api_mismatches": []
    },
    {
      "file_path": "docs/development/testing-strategy.md",
      "line_number": 451,
      "language": "python",
      "code": "def test_error_handling_comprehensive():\n    \"\"\"Test all error scenarios for robust error handling.\"\"\"\n    from codomyrmex.data_visualization import create_line_plot\n\n    # Test empty data\n    with pytest.raises(ValueError, match=\"Empty data\"):\n        create_line_plot([], [], \"Empty Test\")\n\n    # Test mismatched data lengths\n    with pytest.raises(ValueError, match=\"Data length mismatch\"):\n        create_line_plot([1, 2, 3], [1, 2], \"Mismatch Test\")\n\n    # Test invalid file path\n    with pytest.raises(PermissionError):\n        create_line_plot([1, 2], [3, 4], \"Test\", output_path=\"/root/invalid.png\")\n\n    # Test resource exhaustion scenarios\n    import numpy as np\n    huge_array = np.random.random(10**8)  # Very large array\n    with pytest.raises(MemoryError):\n        create_line_plot(huge_array, huge_array, \"Memory Test\")",
      "syntax_valid": true,
      "import_errors": [],
      "api_mismatches": []
    },
    {
      "file_path": "docs/getting-started/README.md",
      "line_number": 57,
      "language": "python",
      "code": "# First steps with Codomyrmex\nfrom codomyrmex.data_visualization import create_line_plot\nimport numpy as np\n\nx = np.linspace(0, 10, 100)\ny = np.sin(x)\ncreate_line_plot(x, y, title=\"My First Plot\", output_path=\"plot.png\")",
      "syntax_valid": true,
      "import_errors": [],
      "api_mismatches": []
    },
    {
      "file_path": "docs/getting-started/installation.md",
      "line_number": 215,
      "language": "python",
      "code": "# Test data visualization (should create PNG files)\nfrom codomyrmex.data_visualization import create_line_plot\nimport numpy as np\n\nx = np.linspace(0, 10, 100)\ny = np.sin(x)\nresult = create_line_plot(x, y, title=\"Test Plot\", output_path=\"test_plot.png\")\nprint(f\"\u2705 Visualization test: {result is not None}\")\n\n# Test AI code generation (requires API key)\nfrom codomyrmex.ai_code_editing import generate_code_snippet\n\ntry:\n    ai_result = generate_code_snippet(\"Create a hello world function\", \"python\")\n    print(f\"\u2705 AI test: {ai_result['status'] == 'success'}\")\nexcept Exception as e:\n    print(f\"\u26a0\ufe0f AI test skipped (no API key): {e}\")\n\n# Test code execution sandbox\nfrom codomyrmex.code_execution_sandbox import execute_code\n\nsandbox_result = execute_code(\"python\", \"print('Hello from sandbox!')\")\nprint(f\"\u2705 Sandbox test: {sandbox_result['success']}\")",
      "syntax_valid": true,
      "import_errors": [],
      "api_mismatches": []
    },
    {
      "file_path": "docs/getting-started/quickstart.md",
      "line_number": 79,
      "language": "python",
      "code": "from codomyrmex.data_visualization import create_line_plot, create_bar_chart\nimport numpy as np\n\nprint(\"\ud83c\udfa8 Creating beautiful data visualizations...\")\n\n# Create sample data\nx = np.linspace(0, 10, 100)\ny1 = np.sin(x)  # Sine wave\ny2 = np.cos(x)  # Cosine wave\n\n# Create a professional line plot\ncreate_line_plot(\n    x_data=x,\n    y_data=y1,\n    title=\"Beautiful Sine Wave Visualization\",\n    x_label=\"Time (seconds)\",\n    y_label=\"Amplitude\",\n    output_path=\"sine_wave.png\",\n    show_plot=False,  # Save to file instead of showing\n    color=\"blue\",\n    linewidth=2\n)\nprint(\"\u2705 Sine wave visualization saved!\")\n\n# Create a bar chart comparing programming languages\nlanguages = [\"Python\", \"JavaScript\", \"Java\", \"C++\", \"Go\"]\npopularity = [85, 72, 65, 58, 45]\n\ncreate_bar_chart(\n    categories=languages,\n    values=popularity,\n    title=\"Programming Language Popularity (2024)\",\n    x_label=\"Programming Language\",\n    y_label=\"Popularity Score\",\n    output_path=\"language_popularity.png\",\n    color_palette=\"viridis\"\n)\nprint(\"\u2705 Programming language comparison saved!\")\n\nprint(\"\ud83c\udf89 Check your output files: sine_wave.png and language_popularity.png\")",
      "syntax_valid": true,
      "import_errors": [],
      "api_mismatches": []
    },
    {
      "file_path": "docs/getting-started/quickstart.md",
      "line_number": 126,
      "language": "python",
      "code": "from codomyrmex.ai_code_editing import generate_code_snippet\n\nprint(\"\ud83e\udd16 Generating code with AI assistance...\")\n\n# Generate a complete function with AI\nresult = generate_code_snippet(\n    prompt=\"Create a secure REST API endpoint for user registration with input validation\",\n    language=\"python\",\n    provider=\"openai\"  # or \"anthropic\", \"google\"\n)\n\nif result[\"status\"] == \"success\":\n    print(\"\ud83e\udd16 AI Generated Code:\")\n    print(\"=\" * 60)\n    print(result[\"generated_code\"])\n    print(\"=\" * 60)\n    print(f\"\u23f1\ufe0f Generated in {result['execution_time']:.2f} seconds\")\n    print(f\"\ud83d\udd22 Tokens used: {result.get('tokens_used', 'N/A')}\")\nelse:\n    print(f\"\u274c Generation failed: {result['error_message']}\")\n\n# You can also refactor existing code\nfrom codomyrmex.ai_code_editing import refactor_code_snippet\n\ncode_to_refactor = \"\"\"\ndef calculate_total(items):\n    total = 0\n    for item in items:\n        total += item\n    return total\n\"\"\"\n\nrefactored = refactor_code_snippet(\n    code=code_to_refactor,\n    refactoring_type=\"optimize\",\n    language=\"python\"\n)\n\nif refactored[\"status\"] == \"success\":\n    print(\"\ud83d\udd27 Refactored Code:\")\n    print(refactored[\"refactored_code\"])",
      "syntax_valid": true,
      "import_errors": [],
      "api_mismatches": []
    },
    {
      "file_path": "docs/getting-started/quickstart.md",
      "line_number": 174,
      "language": "python",
      "code": "from codomyrmex.code_execution_sandbox import execute_code\n\nprint(\"\ud83c\udfc3 Testing code in secure sandbox...\")\n\n# Test a simple Python function\npython_code = \"\"\"\ndef fibonacci(n):\n    if n <= 1:\n        return n\n    return fibonacci(n-1) + fibonacci(n-2)\n\n# Calculate first 10 Fibonacci numbers\nfor i in range(10):\n    print(f\"fib({i}) = {fibonacci(i)}\")\n\"\"\"\n\nresult = execute_code(\n    language=\"python\",\n    code=python_code,\n    timeout=30  # 30 second timeout\n)\n\nprint(\"\ud83d\udcca Execution Results:\")\nprint(f\"\u2705 Success: {result['success']}\")\nprint(f\"\ud83d\udcc4 Output: {result['output']}\")\nprint(f\"\u23f1\ufe0f Execution time: {result['execution_time']:.3f}s\")\n\n# Test JavaScript code too!\njs_code = \"\"\"\nfunction greet(name) {\n    return \\`Hello, \\${name}! Welcome to Codomyrmex!\\`;\n}\n\nconsole.log(greet('Developer'));\n\"\"\"\n\njs_result = execute_code(\n    language=\"javascript\",\n    code=js_code\n)\n\nprint(f\"JavaScript Output: {js_result['output']}\")",
      "syntax_valid": true,
      "import_errors": [],
      "api_mismatches": []
    },
    {
      "file_path": "docs/getting-started/quickstart.md",
      "line_number": 223,
      "language": "python",
      "code": "from codomyrmex.static_analysis import run_pyrefly_analysis\n\nprint(\"\ud83d\udd0d Analyzing code quality...\")\n\n# Analyze your current project\nanalysis_result = run_pyrefly_analysis(\n    target_paths=[\"src/codomyrmex/\"],  # Analyze the main source code\n    project_root=\".\"\n)\n\nprint(\"\ud83d\udcca Analysis Summary:\")\nprint(f\"\ud83d\udcc1 Files analyzed: {analysis_result.get('files_analyzed', 0)}\")\nprint(f\"\ud83d\udea8 Issues found: {analysis_result.get('issue_count', 0)}\")\nprint(f\"\u26a1 Performance score: {analysis_result.get('performance_score', 'N/A')}\")\n\n# You can also analyze specific files\nsingle_file_result = run_pyrefly_analysis(\n    target_paths=[\"README.md\"],  # This won't have Python issues\n    project_root=\".\"\n)\nprint(f\"\ud83d\udcc4 Single file analysis completed\")",
      "syntax_valid": true,
      "import_errors": [],
      "api_mismatches": []
    },
    {
      "file_path": "docs/getting-started/quickstart.md",
      "line_number": 326,
      "language": "python",
      "code": "from codomyrmex.static_analysis import run_pyrefly_analysis\nfrom codomyrmex.code_execution_sandbox import execute_code\nfrom codomyrmex.ai_code_editing import refactor_code_snippet\n\n# 1. Analyze code quality\nissues = run_pyrefly_analysis([\"src/\"], \"/project\")\n\n# 2. Test code execution\nresult = execute_code(\"python\", \"print('test')\")\n\n# 3. Refactor if needed\nrefactored = refactor_code_snippet(\n    code_snippet=\"def func():\\n    return True\",\n    refactoring_instruction=\"Add type hints\",\n    language=\"python\"\n)",
      "syntax_valid": true,
      "import_errors": [],
      "api_mismatches": []
    },
    {
      "file_path": "docs/getting-started/quickstart.md",
      "line_number": 347,
      "language": "python",
      "code": "from codomyrmex.data_visualization import create_histogram, create_scatter_plot\nimport pandas as pd\n\n# Load and analyze data\ndata = pd.read_csv('data.csv')\n\n# Create visualizations\ncreate_histogram(\n    data=data['values'],\n    title=\"Data Distribution\",\n    output_path=\"histogram.png\"\n)\n\ncreate_scatter_plot(\n    x_data=data['x'],\n    y_data=data['y'],\n    title=\"Data Correlation\",\n    output_path=\"scatter.png\"\n)",
      "syntax_valid": true,
      "import_errors": [],
      "api_mismatches": []
    },
    {
      "file_path": "docs/getting-started/quickstart.md",
      "line_number": 371,
      "language": "python",
      "code": "from codomyrmex.ai_code_editing import generate_code_snippet, refactor_code_snippet\nfrom codomyrmex.code_execution_sandbox import execute_code\n\n# Generate new feature\nfeature_code = generate_code_snippet(\n    \"Create a REST API endpoint for user management\",\n    \"python\"\n)\n\n# Test the generated code\ntest_result = execute_code(\"python\", feature_code[\"generated_code\"])\n\n# Refactor for production\nproduction_code = refactor_code_snippet(\n    code_snippet=feature_code[\"generated_code\"],\n    refactoring_instruction=\"Add error handling and logging\",\n    language=\"python\"\n)",
      "syntax_valid": true,
      "import_errors": [],
      "api_mismatches": []
    },
    {
      "file_path": "docs/getting-started/quickstart.md",
      "line_number": 532,
      "language": "python",
      "code": "# Test AI code generation\nfrom codomyrmex.ai_code_editing import generate_code_snippet\n\nresult = generate_code_snippet(\n    prompt=\"Create a function to calculate fibonacci numbers\",\n    language=\"python\"\n)\n\nif result[\"status\"] == \"success\":\n    print(\"\ud83e\udd16 AI Generated Code:\")\n    print(result[\"generated_code\"])",
      "syntax_valid": true,
      "import_errors": [],
      "api_mismatches": []
    },
    {
      "file_path": "docs/getting-started/setup.md",
      "line_number": 234,
      "language": "python",
      "code": "# Test data visualization (should create PNG files)\nfrom codomyrmex.data_visualization import create_line_plot\nimport numpy as np\n\nx = np.linspace(0, 10, 100)\ny = np.sin(x)\nresult = create_line_plot(x, y, title=\"Test Plot\", output_path=\"test_plot.png\")\nprint(f\"\u2705 Visualization test: {result is not None}\")\n\n# Test AI code generation (requires API key)\nfrom codomyrmex.ai_code_editing import generate_code_snippet\n\ntry:\n    ai_result = generate_code_snippet(\"Create a hello world function\", \"python\")\n    print(f\"\u2705 AI test: {ai_result['status'] == 'success'}\")\nexcept Exception as e:\n    print(f\"\u26a0\ufe0f AI test skipped (no API key): {e}\")\n\n# Test code execution sandbox\nfrom codomyrmex.code_execution_sandbox import execute_code\n\nsandbox_result = execute_code(\"python\", \"print('Hello from sandbox!')\")\nprint(f\"\u2705 Sandbox test: {sandbox_result['success']}\")",
      "syntax_valid": true,
      "import_errors": [],
      "api_mismatches": []
    },
    {
      "file_path": "docs/getting-started/tutorials/README.md",
      "line_number": 51,
      "language": "python",
      "code": "# Example from module creation tutorial\nfrom codomyrmex.logging_monitoring import get_logger\nfrom codomyrmex.module_template import create_module\n\nlogger = get_logger(__name__)\n# Follow creating-a-module.md for complete workflow",
      "syntax_valid": true,
      "import_errors": [],
      "api_mismatches": []
    },
    {
      "file_path": "docs/getting-started/tutorials/creating-a-module.md",
      "line_number": 64,
      "language": "python",
      "code": "\"\"\"\nText Analysis Module for Codomyrmex\n\nProvides comprehensive text analysis capabilities including\nword count, sentiment analysis, and readability metrics.\n\"\"\"\n\nimport re\nimport statistics\nfrom typing import Dict, List, Optional, Any\nfrom dataclasses import dataclass\nfrom pathlib import Path\n\n# Import Codomyrmex foundation modules\nfrom codomyrmex.logging_monitoring import get_logger\n\nlogger = get_logger(__name__)\n\n\n@dataclass\nclass TextAnalysisResult:\n    \"\"\"Results from text analysis operations.\"\"\"\n    word_count: int\n    character_count: int\n    character_count_no_spaces: int\n    sentence_count: int\n    paragraph_count: int\n    average_words_per_sentence: float\n    readability_score: float\n    sentiment: Optional[str] = None\n    sentiment_score: Optional[float] = None\n\n\nclass TextAnalyzer:\n    \"\"\"\n    Main text analysis class providing various text metrics and analysis.\n    \n    This class integrates with Codomyrmex logging and follows the standard\n    module patterns for error handling and configuration.\n    \"\"\"\n    \n    def __init__(self, enable_sentiment: bool = True):\n        \"\"\"Initialize the text analyzer.\n        \n        Args:\n            enable_sentiment: Whether to enable sentiment analysis (requires additional deps)\n        \"\"\"\n        self.enable_sentiment = enable_sentiment\n        logger.info(\"TextAnalyzer initialized with sentiment analysis: %s\", enable_sentiment)\n    \n    def analyze_text(self, text: str) -> TextAnalysisResult:\n        \"\"\"\n        Perform comprehensive analysis of the provided text.\n        \n        Args:\n            text: The text content to analyze\n            \n        Returns:\n            TextAnalysisResult containing all analysis metrics\n            \n        Raises:\n            ValueError: If text is empty or None\n        \"\"\"\n        if not text or not text.strip():\n            raise ValueError(\"Text cannot be empty or None\")\n        \n        logger.debug(\"Analyzing text of length: %d\", len(text))\n        \n        try:\n            # Basic text metrics\n            word_count = self._count_words(text)\n            character_count = len(text)\n            character_count_no_spaces = len(text.replace(' ', ''))\n            sentence_count = self._count_sentences(text)\n            paragraph_count = self._count_paragraphs(text)\n            \n            # Derived metrics\n            avg_words_per_sentence = (\n                word_count / sentence_count if sentence_count > 0 else 0.0\n            )\n            readability_score = self._calculate_readability(\n                word_count, sentence_count, character_count_no_spaces\n            )\n            \n            # Sentiment analysis (if enabled)\n            sentiment = None\n            sentiment_score = None\n            if self.enable_sentiment:\n                sentiment, sentiment_score = self._analyze_sentiment(text)\n            \n            result = TextAnalysisResult(\n                word_count=word_count,\n                character_count=character_count,\n                character_count_no_spaces=character_count_no_spaces,\n                sentence_count=sentence_count,\n                paragraph_count=paragraph_count,\n                average_words_per_sentence=avg_words_per_sentence,\n                readability_score=readability_score,\n                sentiment=sentiment,\n                sentiment_score=sentiment_score\n            )\n            \n            logger.info(\"Text analysis completed: %d words, %d sentences\", \n                       word_count, sentence_count)\n            return result\n            \n        except Exception as e:\n            logger.error(\"Error during text analysis: %s\", e, exc_info=True)\n            raise\n    \n    def analyze_file(self, file_path: str) -> TextAnalysisResult:\n        \"\"\"\n        Analyze text content from a file.\n        \n        Args:\n            file_path: Path to the text file\n            \n        Returns:\n            TextAnalysisResult containing analysis metrics\n            \n        Raises:\n            FileNotFoundError: If file doesn't exist\n            IOError: If file cannot be read\n        \"\"\"\n        path = Path(file_path)\n        \n        if not path.exists():\n            raise FileNotFoundError(f\"File not found: {file_path}\")\n        \n        logger.info(\"Analyzing file: %s\", file_path)\n        \n        try:\n            with path.open('r', encoding='utf-8') as f:\n                content = f.read()\n            return self.analyze_text(content)\n        except Exception as e:\n            logger.error(\"Error reading file %s: %s\", file_path, e)\n            raise IOError(f\"Cannot read file {file_path}: {e}\")\n    \n    def _count_words(self, text: str) -> int:\n        \"\"\"Count words in text.\"\"\"\n        words = re.findall(r'\\b\\w+\\b', text.lower())\n        return len(words)\n    \n    def _count_sentences(self, text: str) -> int:\n        \"\"\"Count sentences in text.\"\"\"\n        sentences = re.split(r'[.!?]+', text)\n        return len([s for s in sentences if s.strip()])\n    \n    def _count_paragraphs(self, text: str) -> int:\n        \"\"\"Count paragraphs in text.\"\"\"\n        paragraphs = re.split(r'\\n\\s*\\n', text.strip())\n        return len([p for p in paragraphs if p.strip()])\n    \n    def _calculate_readability(self, words: int, sentences: int, characters: int) -> float:\n        \"\"\"\n        Calculate readability score (simplified Flesch Reading Ease).\n        \n        Returns score between 0-100 where higher = more readable.\n        \"\"\"\n        if sentences == 0 or words == 0:\n            return 0.0\n        \n        avg_sentence_length = words / sentences\n        avg_word_length = characters / words\n        \n        # Simplified readability score\n        score = 206.835 - (1.015 * avg_sentence_length) - (84.6 * avg_word_length)\n        return max(0.0, min(100.0, score))\n    \n    def _analyze_sentiment(self, text: str) -> tuple[Optional[str], Optional[float]]:\n        \"\"\"\n        Perform basic sentiment analysis.\n        \n        In a real implementation, this would use a proper sentiment analysis library.\n        For this tutorial, we'll use a simple approach.\n        \"\"\"\n        # Simple sentiment word lists (in real implementation, use proper libraries)\n        positive_words = ['good', 'great', 'excellent', 'amazing', 'wonderful', 'love', 'like']\n        negative_words = ['bad', 'terrible', 'awful', 'hate', 'dislike', 'horrible']\n        \n        words = re.findall(r'\\b\\w+\\b', text.lower())\n        \n        positive_count = sum(1 for word in words if word in positive_words)\n        negative_count = sum(1 for word in words if word in negative_words)\n        \n        if positive_count > negative_count:\n            return \"positive\", 0.6 + (positive_count - negative_count) * 0.1\n        elif negative_count > positive_count:\n            return \"negative\", -0.6 - (negative_count - positive_count) * 0.1\n        else:\n            return \"neutral\", 0.0\n\n\n# Convenience functions for direct use\ndef analyze_text(text: str, enable_sentiment: bool = True) -> TextAnalysisResult:\n    \"\"\"\n    Convenience function to analyze text directly.\n    \n    Args:\n        text: Text content to analyze\n        enable_sentiment: Whether to include sentiment analysis\n        \n    Returns:\n        TextAnalysisResult with analysis metrics\n    \"\"\"\n    analyzer = TextAnalyzer(enable_sentiment=enable_sentiment)\n    return analyzer.analyze_text(text)\n\n\ndef analyze_file(file_path: str, enable_sentiment: bool = True) -> TextAnalysisResult:\n    \"\"\"\n    Convenience function to analyze text from a file.\n    \n    Args:\n        file_path: Path to text file\n        enable_sentiment: Whether to include sentiment analysis\n        \n    Returns:\n        TextAnalysisResult with analysis metrics\n    \"\"\"\n    analyzer = TextAnalyzer(enable_sentiment=enable_sentiment)\n    return analyzer.analyze_file(file_path)",
      "syntax_valid": true,
      "import_errors": [],
      "api_mismatches": []
    },
    {
      "file_path": "docs/getting-started/tutorials/creating-a-module.md",
      "line_number": 547,
      "language": "python",
      "code": "\"\"\"\nComprehensive tests for the text_analysis module.\n\nThis module tests all functionality including error conditions,\nedge cases, and integration with other Codomyrmex modules.\n\"\"\"\n\nimport pytest\nimport tempfile\nimport os\nfrom unittest.mock import patch, MagicMock\n\nfrom codomyrmex.text_analysis.text_analyzer import (\n    TextAnalyzer, \n    TextAnalysisResult,\n    analyze_text,\n    analyze_file\n)\n\n\nclass TestTextAnalyzer:\n    \"\"\"Test suite for TextAnalyzer class\"\"\"\n    \n    def setup_method(self):\n        \"\"\"Setup for each test method\"\"\"\n        self.analyzer = TextAnalyzer(enable_sentiment=True)\n        self.sample_text = (\n            \"This is a sample text for testing. \"\n            \"It contains multiple sentences and words. \"\n            \"The text should provide good test coverage.\"\n        )\n    \n    def test_analyzer_initialization(self):\n        \"\"\"Test TextAnalyzer initialization\"\"\"\n        analyzer = TextAnalyzer(enable_sentiment=False)\n        assert analyzer.enable_sentiment is False\n        \n        analyzer = TextAnalyzer(enable_sentiment=True)\n        assert analyzer.enable_sentiment is True\n    \n    def test_analyze_text_basic_metrics(self):\n        \"\"\"Test basic text analysis metrics\"\"\"\n        result = self.analyzer.analyze_text(self.sample_text)\n        \n        assert isinstance(result, TextAnalysisResult)\n        assert result.word_count == 16  # Count words in sample text\n        assert result.character_count == len(self.sample_text)\n        assert result.character_count_no_spaces < result.character_count\n        assert result.sentence_count == 3\n        assert result.paragraph_count == 1\n        assert result.average_words_per_sentence > 0\n        assert 0 <= result.readability_score <= 100\n    \n    def test_analyze_text_with_sentiment(self):\n        \"\"\"Test sentiment analysis functionality\"\"\"\n        positive_text = \"This is amazing and wonderful! I love it!\"\n        result = self.analyzer.analyze_text(positive_text)\n        \n        assert result.sentiment == \"positive\"\n        assert result.sentiment_score > 0\n    \n    def test_analyze_text_negative_sentiment(self):\n        \"\"\"Test negative sentiment detection\"\"\"\n        negative_text = \"This is terrible and awful! I hate it!\"\n        result = self.analyzer.analyze_text(negative_text)\n        \n        assert result.sentiment == \"negative\"\n        assert result.sentiment_score < 0\n    \n    def test_analyze_text_neutral_sentiment(self):\n        \"\"\"Test neutral sentiment detection\"\"\"\n        neutral_text = \"The weather today is cloudy.\"\n        result = self.analyzer.analyze_text(neutral_text)\n        \n        assert result.sentiment == \"neutral\"\n        assert result.sentiment_score == 0.0\n    \n    def test_analyze_text_without_sentiment(self):\n        \"\"\"Test analysis with sentiment disabled\"\"\"\n        analyzer = TextAnalyzer(enable_sentiment=False)\n        result = analyzer.analyze_text(self.sample_text)\n        \n        assert result.sentiment is None\n        assert result.sentiment_score is None\n    \n    def test_analyze_empty_text(self):\n        \"\"\"Test analysis of empty text\"\"\"\n        with pytest.raises(ValueError):\n            self.analyzer.analyze_text(\"\")\n        \n        with pytest.raises(ValueError):\n            self.analyzer.analyze_text(\"   \")  # Only whitespace\n        \n        with pytest.raises(ValueError):\n            self.analyzer.analyze_text(None)\n    \n    def test_analyze_file_success(self):\n        \"\"\"Test successful file analysis\"\"\"\n        with tempfile.NamedTemporaryFile(mode='w', suffix='.txt', delete=False) as f:\n            f.write(self.sample_text)\n            temp_path = f.name\n        \n        try:\n            result = self.analyzer.analyze_file(temp_path)\n            assert isinstance(result, TextAnalysisResult)\n            assert result.word_count > 0\n        finally:\n            os.unlink(temp_path)\n    \n    def test_analyze_file_not_found(self):\n        \"\"\"Test file analysis with non-existent file\"\"\"\n        with pytest.raises(FileNotFoundError):\n            self.analyzer.analyze_file(\"/path/that/does/not/exist.txt\")\n    \n    def test_analyze_file_read_error(self):\n        \"\"\"Test file analysis with read permission issues\"\"\"\n        with tempfile.NamedTemporaryFile(mode='w', suffix='.txt', delete=False) as f:\n            f.write(self.sample_text)\n            temp_path = f.name\n        \n        try:\n            # Change permissions to make file unreadable\n            os.chmod(temp_path, 0o000)\n            \n            with pytest.raises(IOError):\n                self.analyzer.analyze_file(temp_path)\n        finally:\n            # Restore permissions and clean up\n            os.chmod(temp_path, 0o644)\n            os.unlink(temp_path)\n    \n    def test_word_counting_accuracy(self):\n        \"\"\"Test word counting accuracy with various text formats\"\"\"\n        test_cases = [\n            (\"Hello world\", 2),\n            (\"Hello, world!\", 2),\n            (\"One two three four five\", 5),\n            (\"Hyphenated-words count as two\", 5),\n            (\"123 numbers count too\", 4),\n        ]\n        \n        for text, expected_words in test_cases:\n            result = self.analyzer.analyze_text(text)\n            assert result.word_count == expected_words, f\"Failed for: {text}\"\n    \n    def test_sentence_counting_accuracy(self):\n        \"\"\"Test sentence counting with various punctuation\"\"\"\n        test_cases = [\n            (\"Hello.\", 1),\n            (\"Hello! How are you?\", 2),\n            (\"First. Second! Third?\", 3),\n            (\"No punctuation\", 1),\n            (\"Multiple... dots... should... work.\", 4),\n        ]\n        \n        for text, expected_sentences in test_cases:\n            result = self.analyzer.analyze_text(text)\n            assert result.sentence_count == expected_sentences, f\"Failed for: {text}\"\n    \n    def test_paragraph_counting_accuracy(self):\n        \"\"\"Test paragraph counting with various formatting\"\"\"\n        single_paragraph = \"This is one paragraph.\"\n        two_paragraphs = \"First paragraph.\\n\\nSecond paragraph.\"\n        three_paragraphs = \"First.\\n\\nSecond.\\n\\nThird.\"\n        \n        assert self.analyzer.analyze_text(single_paragraph).paragraph_count == 1\n        assert self.analyzer.analyze_text(two_paragraphs).paragraph_count == 2\n        assert self.analyzer.analyze_text(three_paragraphs).paragraph_count == 3\n    \n    def test_readability_score_bounds(self):\n        \"\"\"Test readability score stays within bounds\"\"\"\n        # Very simple text should have high readability\n        simple_text = \"Cat. Dog. Run. Jump.\"\n        simple_result = self.analyzer.analyze_text(simple_text)\n        assert simple_result.readability_score >= 0\n        \n        # Complex text should have lower readability\n        complex_text = (\n            \"The implementation of sophisticated algorithmic approaches \"\n            \"necessitates comprehensive understanding of computational complexity.\"\n        )\n        complex_result = self.analyzer.analyze_text(complex_text)\n        assert complex_result.readability_score <= 100\n        assert simple_result.readability_score >= complex_result.readability_score\n\n\nclass TestConvenienceFunctions:\n    \"\"\"Test suite for module-level convenience functions\"\"\"\n    \n    def test_analyze_text_function(self):\n        \"\"\"Test the analyze_text convenience function\"\"\"\n        text = \"Test text for analysis.\"\n        result = analyze_text(text)\n        \n        assert isinstance(result, TextAnalysisResult)\n        assert result.word_count == 4\n    \n    def test_analyze_text_function_no_sentiment(self):\n        \"\"\"Test analyze_text with sentiment disabled\"\"\"\n        text = \"Test text for analysis.\"\n        result = analyze_text(text, enable_sentiment=False)\n        \n        assert result.sentiment is None\n        assert result.sentiment_score is None\n    \n    def test_analyze_file_function(self):\n        \"\"\"Test the analyze_file convenience function\"\"\"\n        text_content = \"Test file content for analysis.\"\n        \n        with tempfile.NamedTemporaryFile(mode='w', suffix='.txt', delete=False) as f:\n            f.write(text_content)\n            temp_path = f.name\n        \n        try:\n            result = analyze_file(temp_path)\n            assert isinstance(result, TextAnalysisResult)\n            assert result.word_count == 5\n        finally:\n            os.unlink(temp_path)\n\n\nclass TestIntegrationWithCodomyrmex:\n    \"\"\"Test integration with other Codomyrmex modules\"\"\"\n    \n    @patch('codomyrmex.text_analysis.text_analyzer.get_logger')\n    def test_logging_integration(self, mock_get_logger):\n        \"\"\"Test integration with Codomyrmex logging system\"\"\"\n        mock_logger = MagicMock()\n        mock_get_logger.return_value = mock_logger\n        \n        analyzer = TextAnalyzer()\n        analyzer.analyze_text(\"Test logging integration.\")\n        \n        # Verify logger was obtained and used\n        mock_get_logger.assert_called()\n        mock_logger.debug.assert_called()\n        mock_logger.info.assert_called()\n    \n    def test_error_logging(self):\n        \"\"\"Test that errors are properly logged\"\"\"\n        with patch('codomyrmex.text_analysis.text_analyzer.logger') as mock_logger:\n            analyzer = TextAnalyzer()\n            \n            try:\n                analyzer.analyze_text(\"\")  # This should raise ValueError\n            except ValueError:\n                pass  # Expected\n            \n            # Verify error was logged\n            mock_logger.error.assert_called()\n\n\nclass TestEdgeCases:\n    \"\"\"Test edge cases and unusual inputs\"\"\"\n    \n    def test_very_long_text(self):\n        \"\"\"Test analysis of very long text\"\"\"\n        # Create a long text (10,000 words)\n        long_text = \"word \" * 10000\n        \n        analyzer = TextAnalyzer()\n        result = analyzer.analyze_text(long_text)\n        \n        assert result.word_count == 10000\n        assert result.character_count > 0\n    \n    def test_unicode_text(self):\n        \"\"\"Test analysis of text with Unicode characters\"\"\"\n        unicode_text = \"Hello \u4e16\u754c! This contains \u00e9mojis \ud83c\udf1f and \u00e0cc\u00e9nts.\"\n        \n        analyzer = TextAnalyzer()\n        result = analyzer.analyze_text(unicode_text)\n        \n        assert result.word_count > 0\n        assert result.character_count > 0\n    \n    def test_only_punctuation(self):\n        \"\"\"Test text with only punctuation\"\"\"\n        punct_text = \"!@#$%^&*(),.?;:\"\n        \n        analyzer = TextAnalyzer()\n        result = analyzer.analyze_text(punct_text)\n        \n        assert result.word_count == 0\n        assert result.character_count > 0\n    \n    def test_single_character(self):\n        \"\"\"Test analysis of single character\"\"\"\n        analyzer = TextAnalyzer()\n        result = analyzer.analyze_text(\"a\")\n        \n        assert result.word_count == 1\n        assert result.character_count == 1\n        assert result.sentence_count == 1\n\n\nif __name__ == \"__main__\":\n    pytest.main([__file__])",
      "syntax_valid": true,
      "import_errors": [],
      "api_mismatches": []
    },
    {
      "file_path": "docs/getting-started/tutorials/creating-a-module.md",
      "line_number": 853,
      "language": "python",
      "code": "\"\"\"\nText Analysis Module for Codomyrmex\n\nThis module provides comprehensive text analysis capabilities including\nword counting, readability metrics, and sentiment analysis.\n\"\"\"\n\nfrom .text_analyzer import (\n    TextAnalyzer,\n    TextAnalysisResult,\n    analyze_text,\n    analyze_file\n)\n\n__version__ = \"0.1.0\"\n__author__ = \"Codomyrmex Contributors\"\n\n# Public API\n__all__ = [\n    \"TextAnalyzer\",\n    \"TextAnalysisResult\", \n    \"analyze_text\",\n    \"analyze_file\"\n]",
      "syntax_valid": true,
      "import_errors": [],
      "api_mismatches": []
    },
    {
      "file_path": "docs/getting-started/tutorials/creating-a-module.md",
      "line_number": 935,
      "language": "python",
      "code": "# test_my_module.py\nimport sys\nsys.path.insert(0, 'src')\n\nfrom codomyrmex.text_analysis import analyze_text, TextAnalyzer\n\n# Test basic functionality\ntext = \"\"\"\nThis is a sample text for testing our new text analysis module.\nIt contains multiple sentences and should provide interesting metrics.\nThe readability should be reasonable for this type of content.\n\"\"\"\n\nprint(\"=== Testing Text Analysis Module ===\")\n\n# Test convenience function\nresult = analyze_text(text.strip())\nprint(f\"Words: {result.word_count}\")\nprint(f\"Characters: {result.character_count}\")\nprint(f\"Sentences: {result.sentence_count}\")\nprint(f\"Readability: {result.readability_score:.1f}\")\nprint(f\"Sentiment: {result.sentiment} ({result.sentiment_score:.2f})\")\n\n# Test class-based approach\nanalyzer = TextAnalyzer(enable_sentiment=False)\nresult2 = analyzer.analyze_text(text.strip())\nprint(f\"\\nWithout sentiment: {result2.sentiment}\")\n\nprint(\"\\n\u2705 Module working correctly!\")",
      "syntax_valid": true,
      "import_errors": [],
      "api_mismatches": []
    },
    {
      "file_path": "docs/getting-started/tutorials/creating-a-module.md",
      "line_number": 987,
      "language": "python",
      "code": "# integration_test.py\nimport sys\nsys.path.insert(0, 'src')\n\nfrom codomyrmex.text_analysis import analyze_text\nfrom codomyrmex.data_visualization import create_bar_chart\n\n# Analyze multiple texts\ntexts = [\n    \"This is a simple text.\",\n    \"This text is more complex with sophisticated vocabulary and longer sentences.\",\n    \"Short. Quick. Fast.\"\n]\n\nreadability_scores = []\nfor i, text in enumerate(texts):\n    result = analyze_text(text)\n    readability_scores.append(result.readability_score)\n    print(f\"Text {i+1} readability: {result.readability_score:.1f}\")\n\n# Create visualization\ncreate_bar_chart(\n    categories=[f\"Text {i+1}\" for i in range(len(texts))],\n    values=readability_scores,\n    title=\"Text Readability Comparison\",\n    y_label=\"Readability Score\",\n    output_path=\"text_readability.png\",\n    show_plot=False\n)\n\nprint(\"\u2705 Integration test completed! Check text_readability.png\")",
      "syntax_valid": true,
      "import_errors": [],
      "api_mismatches": []
    },
    {
      "file_path": "docs/getting-started/tutorials/creating-a-module.md",
      "line_number": 1117,
      "language": "python",
      "code": "\"\"\"Integration tests for text_analysis module with main test suite\"\"\"\n\nimport pytest\nfrom codomyrmex.text_analysis import analyze_text, TextAnalyzer\n\n\nclass TestTextAnalysisIntegration:\n    \"\"\"Integration tests for text analysis module\"\"\"\n    \n    def test_module_import(self):\n        \"\"\"Test that module imports correctly\"\"\"\n        from codomyrmex.text_analysis import TextAnalyzer, analyze_text\n        assert TextAnalyzer is not None\n        assert analyze_text is not None\n    \n    def test_basic_functionality(self):\n        \"\"\"Test basic module functionality\"\"\"\n        result = analyze_text(\"This is a test sentence.\")\n        assert result.word_count == 5\n        assert result.sentence_count == 1\n    \n    def test_logging_integration(self):\n        \"\"\"Test integration with Codomyrmex logging\"\"\"\n        # This test ensures the module uses Codomyrmex logging\n        analyzer = TextAnalyzer()\n        result = analyzer.analyze_text(\"Test logging integration.\")\n        assert result is not None",
      "syntax_valid": true,
      "import_errors": [],
      "api_mismatches": []
    },
    {
      "file_path": "docs/integration/README.md",
      "line_number": 46,
      "language": "python",
      "code": "# PostgreSQL integration\nfrom codomyrmex.integration.database import CodomyrmexDatabase\nimport asyncio\n\nasync def main():\n    db = CodomyrmexDatabase(\"postgresql://user:pass@localhost/db\")\n    await db.initialize()\n    job_id = await db.store_job('analysis', {'path': 'src/'})\n\nasyncio.run(main())",
      "syntax_valid": true,
      "import_errors": [],
      "api_mismatches": []
    },
    {
      "file_path": "docs/integration/external-systems.md",
      "line_number": 60,
      "language": "python",
      "code": "# database_integration.py - PostgreSQL integration pattern\nimport asyncpg\nimport asyncio\nfrom contextlib import asynccontextmanager\nfrom typing import Dict, List, Optional\nfrom codomyrmex.logging_monitoring import get_logger\nimport json\n\nlogger = get_logger(__name__)\n\nclass CodomyrmexDatabase:\n    \"\"\"PostgreSQL integration for Codomyrmex data persistence.\"\"\"\n\n    def __init__(self, database_url: str, pool_size: int = 10):\n        self.database_url = database_url\n        self.pool_size = pool_size\n        self.pool = None\n\n    async def initialize(self):\n        \"\"\"Initialize database connection pool.\"\"\"\n        self.pool = await asyncpg.create_pool(\n            self.database_url,\n            min_size=2,\n            max_size=self.pool_size,\n            command_timeout=60\n        )\n\n        # Create schema if it doesn't exist\n        async with self.pool.acquire() as conn:\n            await self._create_schema(conn)\n\n        logger.info(\"Database pool initialized\")\n\n    async def _create_schema(self, conn):\n        \"\"\"Create database schema for Codomyrmex data.\"\"\"\n        await conn.execute(\"\"\"\n            CREATE TABLE IF NOT EXISTS codomyrmex_jobs (\n                id SERIAL PRIMARY KEY,\n                job_type VARCHAR(100) NOT NULL,\n                status VARCHAR(50) NOT NULL DEFAULT 'pending',\n                input_data JSONB NOT NULL,\n                result JSONB,\n                error_message TEXT,\n                created_at TIMESTAMP WITH TIME ZONE DEFAULT NOW(),\n                updated_at TIMESTAMP WITH TIME ZONE DEFAULT NOW(),\n                execution_time_ms INTEGER\n            );\n\n            CREATE TABLE IF NOT EXISTS codomyrmex_analysis_cache (\n                id SERIAL PRIMARY KEY,\n                cache_key VARCHAR(255) UNIQUE NOT NULL,\n                content_hash VARCHAR(64) NOT NULL,\n                analysis_result JSONB NOT NULL,\n                created_at TIMESTAMP WITH TIME ZONE DEFAULT NOW(),\n                expires_at TIMESTAMP WITH TIME ZONE NOT NULL\n            );\n\n            CREATE INDEX IF NOT EXISTS idx_jobs_status ON codomyrmex_jobs(status);\n            CREATE INDEX IF NOT EXISTS idx_jobs_type ON codomyrmex_jobs(job_type);\n            CREATE INDEX IF NOT EXISTS idx_cache_key ON codomyrmex_analysis_cache(cache_key);\n            CREATE INDEX IF NOT EXISTS idx_cache_expires ON codomyrmex_analysis_cache(expires_at);\n        \"\"\")\n\n    @asynccontextmanager\n    async def transaction(self):\n        \"\"\"Database transaction context manager.\"\"\"\n        async with self.pool.acquire() as conn:\n            async with conn.transaction():\n                yield conn\n\n    async def store_job(self, job_type: str, input_data: Dict) -> int:\n        \"\"\"Store a new job in the database.\"\"\"\n        async with self.transaction() as conn:\n            job_id = await conn.fetchval(\"\"\"\n                INSERT INTO codomyrmex_jobs (job_type, input_data)\n                VALUES ($1, $2)\n                RETURNING id\n            \"\"\", job_type, json.dumps(input_data))\n\n        logger.info(f\"Stored job {job_id} of type {job_type}\")\n        return job_id\n\n    async def update_job_result(self, job_id: int, result: Dict,\n                               execution_time_ms: int, success: bool = True):\n        \"\"\"Update job with execution result.\"\"\"\n        status = 'completed' if success else 'failed'\n\n        async with self.transaction() as conn:\n            await conn.execute(\"\"\"\n                UPDATE codomyrmex_jobs\n                SET status = $1, result = $2, execution_time_ms = $3,\n                    updated_at = NOW()\n                WHERE id = $4\n            \"\"\", status, json.dumps(result), execution_time_ms, job_id)\n\n        logger.info(f\"Updated job {job_id} with status {status}\")\n\n    async def get_cached_analysis(self, cache_key: str) -> Optional[Dict]:\n        \"\"\"Get cached analysis result.\"\"\"\n        async with self.pool.acquire() as conn:\n            row = await conn.fetchrow(\"\"\"\n                SELECT analysis_result\n                FROM codomyrmex_analysis_cache\n                WHERE cache_key = $1 AND expires_at > NOW()\n            \"\"\", cache_key)\n\n        return json.loads(row['analysis_result']) if row else None\n\n    async def store_cached_analysis(self, cache_key: str, content_hash: str,\n                                  result: Dict, ttl_hours: int = 24):\n        \"\"\"Store analysis result in cache.\"\"\"\n        async with self.transaction() as conn:\n            await conn.execute(\"\"\"\n                INSERT INTO codomyrmex_analysis_cache\n                (cache_key, content_hash, analysis_result, expires_at)\n                VALUES ($1, $2, $3, NOW() + INTERVAL '%d hours')\n                ON CONFLICT (cache_key)\n                DO UPDATE SET\n                    content_hash = EXCLUDED.content_hash,\n                    analysis_result = EXCLUDED.analysis_result,\n                    expires_at = EXCLUDED.expires_at\n            \"\"\" % ttl_hours, cache_key, content_hash, json.dumps(result))\n\n        logger.debug(f\"Cached analysis result for key {cache_key}\")\n\n# Usage example with static analysis\nasync def analyze_codebase_with_db_cache(codebase_path: str,\n                                       db: CodomyrmexDatabase) -> Dict:\n    \"\"\"Analyze codebase with database caching.\"\"\"\n    from codomyrmex.static_analysis import analyze_codebase\n    import hashlib\n\n    # Generate cache key\n    cache_key = f\"static_analysis:{hashlib.md5(str(codebase_path).encode()).hexdigest()}\"\n\n    # Check cache first\n    cached_result = await db.get_cached_analysis(cache_key)\n    if cached_result:\n        logger.info(f\"Using cached analysis for {codebase_path}\")\n        return cached_result\n\n    # Store job\n    job_id = await db.store_job('static_analysis', {\n        'codebase_path': str(codebase_path),\n        'timestamp': time.time()\n    })\n\n    start_time = time.time()\n    try:\n        # Perform analysis\n        result = analyze_codebase(codebase_path)\n        execution_time = int((time.time() - start_time) * 1000)\n\n        # Store results\n        await db.update_job_result(job_id, result, execution_time, success=True)\n\n        # Cache result\n        content_hash = hashlib.md5(json.dumps(result, sort_keys=True).encode()).hexdigest()\n        await db.store_cached_analysis(cache_key, content_hash, result)\n\n        return result\n\n    except Exception as e:\n        execution_time = int((time.time() - start_time) * 1000)\n        error_result = {'error': str(e)}\n        await db.update_job_result(job_id, error_result, execution_time, success=False)\n        raise",
      "syntax_valid": true,
      "import_errors": [],
      "api_mismatches": []
    },
    {
      "file_path": "docs/integration/external-systems.md",
      "line_number": 232,
      "language": "python",
      "code": "# mongodb_integration.py - MongoDB integration for flexible document storage\nfrom motor.motor_asyncio import AsyncIOMotorClient\nfrom datetime import datetime, timedelta\nimport pymongo\nfrom typing import Dict, List, Optional\nfrom bson import ObjectId\n\nclass CodomyrmexMongoDB:\n    \"\"\"MongoDB integration for document-based data storage.\"\"\"\n\n    def __init__(self, connection_string: str, database_name: str = \"codomyrmex\"):\n        self.client = AsyncIOMotorClient(connection_string)\n        self.db = self.client[database_name]\n\n    async def initialize(self):\n        \"\"\"Initialize MongoDB collections and indexes.\"\"\"\n        # Create collections\n        self.jobs = self.db.jobs\n        self.analysis_results = self.db.analysis_results\n        self.user_preferences = self.db.user_preferences\n\n        # Create indexes\n        await self.jobs.create_index([(\"status\", 1), (\"created_at\", -1)])\n        await self.jobs.create_index([(\"job_type\", 1)])\n        await self.analysis_results.create_index([(\"cache_key\", 1)], unique=True)\n        await self.analysis_results.create_index([(\"expires_at\", 1)], expireAfterSeconds=0)\n\n        logger.info(\"MongoDB collections and indexes initialized\")\n\n    async def store_analysis_result(self, analysis_type: str, input_data: Dict,\n                                  result: Dict, ttl_hours: int = 24) -> str:\n        \"\"\"Store analysis result with automatic expiration.\"\"\"\n        doc = {\n            'analysis_type': analysis_type,\n            'input_data': input_data,\n            'result': result,\n            'created_at': datetime.utcnow(),\n            'expires_at': datetime.utcnow() + timedelta(hours=ttl_hours)\n        }\n\n        result_obj = await self.analysis_results.insert_one(doc)\n        return str(result_obj.inserted_id)\n\n    async def get_analysis_history(self, analysis_type: str = None,\n                                 limit: int = 100) -> List[Dict]:\n        \"\"\"Get analysis history with optional filtering.\"\"\"\n        filter_query = {}\n        if analysis_type:\n            filter_query['analysis_type'] = analysis_type\n\n        cursor = self.analysis_results.find(\n            filter_query,\n            sort=[('created_at', pymongo.DESCENDING)]\n        ).limit(limit)\n\n        return await cursor.to_list(length=limit)\n\n    async def store_user_workflow(self, user_id: str, workflow_data: Dict) -> str:\n        \"\"\"Store user-specific workflow configuration.\"\"\"\n        doc = {\n            'user_id': user_id,\n            'workflow_data': workflow_data,\n            'created_at': datetime.utcnow(),\n            'updated_at': datetime.utcnow()\n        }\n\n        # Upsert based on user_id\n        result = await self.user_preferences.replace_one(\n            {'user_id': user_id},\n            doc,\n            upsert=True\n        )\n\n        return str(result.upserted_id or result.matched_count)\n\n# Usage with AI code editing\nasync def ai_enhancement_with_history(code: str, user_id: str,\n                                    mongo_db: CodomyrmexMongoDB):\n    \"\"\"AI code enhancement with history tracking.\"\"\"\n    from codomyrmex.ai_code_editing import enhance_code\n\n    # Enhance code\n    result = await enhance_code(code, user_context=user_id)\n\n    # Store in history\n    await mongo_db.store_analysis_result(\n        'ai_code_enhancement',\n        {\n            'user_id': user_id,\n            'original_code': code,\n            'enhancement_type': result.enhancement_type\n        },\n        {\n            'enhanced_code': result.enhanced_code,\n            'improvements': result.improvements,\n            'confidence_score': result.confidence_score\n        }\n    )\n\n    return result",
      "syntax_valid": true,
      "import_errors": [],
      "api_mismatches": []
    },
    {
      "file_path": "docs/integration/external-systems.md",
      "line_number": 339,
      "language": "python",
      "code": "# aws_integration.py - AWS services integration\nimport boto3\nimport asyncio\nfrom botocore.config import Config\nfrom typing import Dict, List, Optional\nimport json\n\nclass AWSIntegration:\n    \"\"\"Integration with AWS services for cloud-native Codomyrmex deployment.\"\"\"\n\n    def __init__(self, region: str = 'us-east-1'):\n        self.region = region\n        self.config = Config(\n            region_name=region,\n            retries={'max_attempts': 3, 'mode': 'adaptive'}\n        )\n\n        # Initialize clients\n        self.s3 = boto3.client('s3', config=self.config)\n        self.lambda_client = boto3.client('lambda', config=self.config)\n        self.sqs = boto3.client('sqs', config=self.config)\n        self.sns = boto3.client('sns', config=self.config)\n        self.cloudwatch = boto3.client('cloudwatch', config=self.config)\n\n    async def store_analysis_artifacts(self, analysis_id: str,\n                                     artifacts: Dict[str, bytes],\n                                     bucket_name: str) -> List[str]:\n        \"\"\"Store analysis artifacts in S3.\"\"\"\n        stored_keys = []\n\n        for artifact_name, artifact_data in artifacts.items():\n            key = f\"analysis-artifacts/{analysis_id}/{artifact_name}\"\n\n            # Upload to S3\n            self.s3.put_object(\n                Bucket=bucket_name,\n                Key=key,\n                Body=artifact_data,\n                ContentType=self._get_content_type(artifact_name),\n                Metadata={\n                    'analysis_id': analysis_id,\n                    'uploaded_by': 'codomyrmex',\n                    'timestamp': str(time.time())\n                }\n            )\n\n            stored_keys.append(key)\n            logger.info(f\"Stored artifact {artifact_name} at s3://{bucket_name}/{key}\")\n\n        return stored_keys\n\n    async def trigger_lambda_analysis(self, function_name: str,\n                                    payload: Dict) -> Dict:\n        \"\"\"Trigger serverless analysis via AWS Lambda.\"\"\"\n        response = self.lambda_client.invoke(\n            FunctionName=function_name,\n            InvocationType='RequestResponse',\n            Payload=json.dumps(payload)\n        )\n\n        # Parse response\n        result = json.loads(response['Payload'].read())\n\n        if response['StatusCode'] == 200:\n            logger.info(f\"Lambda function {function_name} executed successfully\")\n            return result\n        else:\n            logger.error(f\"Lambda function failed: {result}\")\n            raise Exception(f\"Lambda execution failed: {result}\")\n\n    async def send_analysis_notification(self, topic_arn: str,\n                                       analysis_result: Dict) -> str:\n        \"\"\"Send analysis completion notification via SNS.\"\"\"\n        message = {\n            'analysis_id': analysis_result.get('id'),\n            'status': analysis_result.get('status'),\n            'completion_time': analysis_result.get('completion_time'),\n            'summary': analysis_result.get('summary', {})\n        }\n\n        response = self.sns.publish(\n            TopicArn=topic_arn,\n            Subject='Codomyrmex Analysis Complete',\n            Message=json.dumps(message, indent=2),\n            MessageAttributes={\n                'analysis_type': {\n                    'DataType': 'String',\n                    'StringValue': analysis_result.get('type', 'unknown')\n                }\n            }\n        )\n\n        return response['MessageId']\n\n    async def publish_custom_metrics(self, metrics: List[Dict]):\n        \"\"\"Publish custom CloudWatch metrics.\"\"\"\n        metric_data = []\n\n        for metric in metrics:\n            metric_data.append({\n                'MetricName': metric['name'],\n                'Value': metric['value'],\n                'Unit': metric.get('unit', 'Count'),\n                'Dimensions': [\n                    {\n                        'Name': dim_name,\n                        'Value': dim_value\n                    }\n                    for dim_name, dim_value in metric.get('dimensions', {}).items()\n                ]\n            })\n\n        # Publish metrics (CloudWatch accepts up to 20 metrics per call)\n        for i in range(0, len(metric_data), 20):\n            batch = metric_data[i:i+20]\n\n            self.cloudwatch.put_metric_data(\n                Namespace='Codomyrmex',\n                MetricData=batch\n            )\n\n        logger.info(f\"Published {len(metric_data)} custom metrics\")\n\n    def _get_content_type(self, filename: str) -> str:\n        \"\"\"Get content type based on file extension.\"\"\"\n        content_types = {\n            '.json': 'application/json',\n            '.txt': 'text/plain',\n            '.py': 'text/x-python',\n            '.js': 'application/javascript',\n            '.html': 'text/html',\n            '.png': 'image/png',\n            '.jpg': 'image/jpeg',\n            '.pdf': 'application/pdf'\n        }\n\n        extension = Path(filename).suffix.lower()\n        return content_types.get(extension, 'application/octet-stream')\n\n# Usage example for cloud-native analysis workflow\nasync def cloud_analysis_workflow(codebase_path: str, aws_integration: AWSIntegration):\n    \"\"\"Complete cloud-native analysis workflow.\"\"\"\n    from codomyrmex.static_analysis import analyze_codebase\n    from codomyrmex.data_visualization import create_analysis_dashboard\n\n    analysis_id = f\"analysis_{int(time.time())}\"\n\n    try:\n        # 1. Perform static analysis\n        analysis_result = analyze_codebase(codebase_path)\n\n        # 2. Generate visualization artifacts\n        dashboard_data = create_analysis_dashboard(analysis_result)\n\n        # 3. Store artifacts in S3\n        artifacts = {\n            'analysis_report.json': json.dumps(analysis_result, indent=2).encode(),\n            'dashboard.html': dashboard_data['html'].encode(),\n            'metrics.json': json.dumps(dashboard_data['metrics'], indent=2).encode()\n        }\n\n        stored_keys = await aws_integration.store_analysis_artifacts(\n            analysis_id, artifacts, 'codomyrmex-analysis-bucket'\n        )\n\n        # 4. Trigger additional Lambda processing if needed\n        lambda_result = await aws_integration.trigger_lambda_analysis(\n            'codomyrmex-post-process',\n            {\n                'analysis_id': analysis_id,\n                'artifact_keys': stored_keys,\n                'analysis_summary': analysis_result.get('summary', {})\n            }\n        )\n\n        # 5. Publish metrics\n        await aws_integration.publish_custom_metrics([\n            {\n                'name': 'AnalysisCompleted',\n                'value': 1,\n                'dimensions': {'analysis_type': 'static_analysis'}\n            },\n            {\n                'name': 'FilesAnalyzed',\n                'value': len(analysis_result.get('files', [])),\n                'unit': 'Count'\n            },\n            {\n                'name': 'IssuesFound',\n                'value': len(analysis_result.get('issues', [])),\n                'unit': 'Count'\n            }\n        ])\n\n        # 6. Send notification\n        notification_id = await aws_integration.send_analysis_notification(\n            'arn:aws:sns:us-east-1:123456789:codomyrmex-notifications',\n            {\n                'id': analysis_id,\n                'status': 'completed',\n                'completion_time': datetime.utcnow().isoformat(),\n                'summary': analysis_result.get('summary', {}),\n                'artifacts': stored_keys\n            }\n        )\n\n        logger.info(f\"Cloud analysis workflow completed: {analysis_id}\")\n        return {\n            'analysis_id': analysis_id,\n            'artifacts': stored_keys,\n            'lambda_result': lambda_result,\n            'notification_id': notification_id\n        }\n\n    except Exception as e:\n        logger.error(f\"Cloud analysis workflow failed: {e}\")\n\n        # Send error notification\n        await aws_integration.send_analysis_notification(\n            'arn:aws:sns:us-east-1:123456789:codomyrmex-alerts',\n            {\n                'id': analysis_id,\n                'status': 'failed',\n                'error': str(e),\n                'completion_time': datetime.utcnow().isoformat()\n            }\n        )\n\n        raise",
      "syntax_valid": true,
      "import_errors": [],
      "api_mismatches": []
    },
    {
      "file_path": "docs/integration/external-systems.md",
      "line_number": 575,
      "language": "python",
      "code": "# github_integration.py - GitHub Actions and API integration\nimport aiohttp\nimport asyncio\nfrom typing import Dict, List, Optional\nimport base64\nimport json\n\nclass GitHubIntegration:\n    \"\"\"Integration with GitHub API and Actions.\"\"\"\n\n    def __init__(self, token: str, owner: str, repo: str):\n        self.token = token\n        self.owner = owner\n        self.repo = repo\n        self.base_url = \"https://api.github.com\"\n        self.headers = {\n            'Authorization': f'token {token}',\n            'Accept': 'application/vnd.github.v3+json',\n            'User-Agent': 'Codomyrmex-Integration/1.0'\n        }\n\n    async def create_pull_request_analysis(self, pr_number: int) -> Dict:\n        \"\"\"Analyze pull request and create detailed analysis.\"\"\"\n        from codomyrmex.static_analysis import analyze_diff\n        from codomyrmex.ai_code_editing import review_code_changes\n\n        async with aiohttp.ClientSession() as session:\n            # Get PR details\n            pr_url = f\"{self.base_url}/repos/{self.owner}/{self.repo}/pulls/{pr_number}\"\n            async with session.get(pr_url, headers=self.headers) as response:\n                pr_data = await response.json()\n\n            # Get PR diff\n            diff_url = f\"{self.base_url}/repos/{self.owner}/{self.repo}/pulls/{pr_number}/files\"\n            async with session.get(diff_url, headers=self.headers) as response:\n                files_data = await response.json()\n\n            # Analyze changes\n            analysis_tasks = []\n            for file_data in files_data:\n                if file_data['status'] in ['added', 'modified']:\n                    # Get file content\n                    content_url = file_data['contents_url']\n                    async with session.get(content_url, headers=self.headers) as response:\n                        content_data = await response.json()\n\n                        if content_data['encoding'] == 'base64':\n                            file_content = base64.b64decode(content_data['content']).decode()\n\n                            # Schedule analysis\n                            analysis_tasks.append(\n                                self._analyze_file_changes(\n                                    file_data['filename'],\n                                    file_content,\n                                    file_data.get('patch', ''),\n                                    session\n                                )\n                            )\n\n            # Execute all analyses in parallel\n            analysis_results = await asyncio.gather(*analysis_tasks, return_exceptions=True)\n\n            # Compile comprehensive review\n            review = {\n                'pr_number': pr_number,\n                'title': pr_data['title'],\n                'author': pr_data['user']['login'],\n                'files_analyzed': len([r for r in analysis_results if not isinstance(r, Exception)]),\n                'analysis_results': [r for r in analysis_results if not isinstance(r, Exception)],\n                'errors': [str(r) for r in analysis_results if isinstance(r, Exception)],\n                'summary': self._generate_review_summary(analysis_results),\n                'recommendations': self._generate_recommendations(analysis_results)\n            }\n\n            return review\n\n    async def _analyze_file_changes(self, filename: str, content: str,\n                                  patch: str, session: aiohttp.ClientSession) -> Dict:\n        \"\"\"Analyze individual file changes.\"\"\"\n        from codomyrmex.static_analysis import analyze_code_quality\n        from codomyrmex.pattern_matching import find_code_patterns\n\n        try:\n            # Static analysis\n            quality_analysis = analyze_code_quality(content, filename)\n\n            # Pattern analysis\n            pattern_analysis = find_code_patterns(content, filename)\n\n            # AI review (if enabled)\n            ai_review = None\n            if os.getenv('ENABLE_AI_REVIEW', 'false').lower() == 'true':\n                from codomyrmex.ai_code_editing import review_code_changes\n                ai_review = await review_code_changes(content, patch, filename)\n\n            return {\n                'filename': filename,\n                'quality_score': quality_analysis.overall_score,\n                'issues': quality_analysis.issues,\n                'patterns': pattern_analysis.patterns_found,\n                'ai_review': ai_review,\n                'recommendations': self._file_recommendations(quality_analysis, pattern_analysis)\n            }\n\n        except Exception as e:\n            logger.error(f\"Analysis failed for {filename}: {e}\")\n            return {\n                'filename': filename,\n                'error': str(e)\n            }\n\n    async def post_review_comment(self, pr_number: int, analysis_result: Dict) -> str:\n        \"\"\"Post comprehensive analysis as PR comment.\"\"\"\n        comment_body = self._format_analysis_comment(analysis_result)\n\n        async with aiohttp.ClientSession() as session:\n            comment_url = f\"{self.base_url}/repos/{self.owner}/{self.repo}/issues/{pr_number}/comments\"\n\n            async with session.post(\n                comment_url,\n                headers=self.headers,\n                json={'body': comment_body}\n            ) as response:\n                comment_data = await response.json()\n                return comment_data['html_url']\n\n    async def trigger_workflow(self, workflow_id: str, ref: str, inputs: Dict = None) -> str:\n        \"\"\"Trigger GitHub Actions workflow.\"\"\"\n        async with aiohttp.ClientSession() as session:\n            workflow_url = f\"{self.base_url}/repos/{self.owner}/{self.repo}/actions/workflows/{workflow_id}/dispatches\"\n\n            payload = {\n                'ref': ref,\n                'inputs': inputs or {}\n            }\n\n            async with session.post(\n                workflow_url,\n                headers=self.headers,\n                json=payload\n            ) as response:\n                if response.status == 204:\n                    return f\"Workflow {workflow_id} triggered successfully\"\n                else:\n                    error_data = await response.json()\n                    raise Exception(f\"Workflow trigger failed: {error_data}\")\n\n    def _format_analysis_comment(self, analysis_result: Dict) -> str:\n        \"\"\"Format analysis result as markdown comment.\"\"\"\n        comment_parts = [\n            \"## \ud83e\udd16 Codomyrmex Analysis Report\\n\",\n            f\"**Files Analyzed**: {analysis_result['files_analyzed']}\\n\",\n            f\"**Author**: @{analysis_result['author']}\\n\\n\"\n        ]\n\n        # Summary\n        if analysis_result.get('summary'):\n            comment_parts.append(\"### \ud83d\udcca Summary\\n\")\n            for key, value in analysis_result['summary'].items():\n                comment_parts.append(f\"- **{key.title()}**: {value}\\n\")\n            comment_parts.append(\"\\n\")\n\n        # File-by-file analysis\n        if analysis_result.get('analysis_results'):\n            comment_parts.append(\"### \ud83d\udcc1 File Analysis\\n\\n\")\n\n            for file_result in analysis_result['analysis_results']:\n                if file_result.get('error'):\n                    comment_parts.append(f\"\u274c **{file_result['filename']}**: Analysis failed - {file_result['error']}\\n\\n\")\n                    continue\n\n                comment_parts.append(f\"#### {file_result['filename']}\\n\")\n                comment_parts.append(f\"- **Quality Score**: {file_result.get('quality_score', 'N/A')}/100\\n\")\n\n                if file_result.get('issues'):\n                    comment_parts.append(f\"- **Issues Found**: {len(file_result['issues'])}\\n\")\n                    for issue in file_result['issues'][:3]:  # Top 3 issues\n                        comment_parts.append(f\"  - {issue['severity']}: {issue['message']}\\n\")\n\n                if file_result.get('ai_review'):\n                    comment_parts.append(f\"- **AI Insights**: {file_result['ai_review']['summary']}\\n\")\n\n                comment_parts.append(\"\\n\")\n\n        # Recommendations\n        if analysis_result.get('recommendations'):\n            comment_parts.append(\"### \ud83d\udca1 Recommendations\\n\\n\")\n            for rec in analysis_result['recommendations']:\n                comment_parts.append(f\"- {rec}\\n\")\n            comment_parts.append(\"\\n\")\n\n        comment_parts.append(\"---\\n*This analysis was generated by Codomyrmex. For more details, check the full CI/CD pipeline.*\")\n\n        return \"\".join(comment_parts)\n\n# GitHub Actions workflow file\ngithub_workflow_content = \"\"\"\n# .github/workflows/codomyrmex-analysis.yml\nname: Codomyrmex Code Analysis\n\non:\n  pull_request:\n    types: [opened, synchronize]\n  push:\n    branches: [main, develop]\n  workflow_dispatch:\n    inputs:\n      analysis_type:\n        description: 'Type of analysis to run'\n        required: true\n        default: 'comprehensive'\n        type: choice\n        options:\n        - comprehensive\n        - static-only\n        - ai-review-only\n\njobs:\n  codomyrmex-analysis:\n    runs-on: ubuntu-latest\n    timeout-minutes: 30\n\n    steps:\n    - name: Checkout code\n      uses: actions/checkout@v4\n      with:\n        fetch-depth: 0  # Full history for comprehensive analysis\n\n    - name: Set up Python\n      uses: actions/setup-python@v4\n      with:\n        python-version: '3.11'\n        cache: 'pip'\n\n    - name: Install Codomyrmex\n      run: |\n        pip install codomyrmex[all]\n        # Or install from source\n        # pip install -e .\n\n    - name: Run Static Analysis\n      if: ${{ github.event.inputs.analysis_type != 'ai-review-only' }}\n      run: |\n        codomyrmex analyze --path . --output-format json --output-file analysis.json\n\n    - name: Run AI Code Review\n      if: ${{ github.event.inputs.analysis_type != 'static-only' }}\n      env:\n        OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }}\n        ANTHROPIC_API_KEY: ${{ secrets.ANTHROPIC_API_KEY }}\n      run: |\n        codomyrmex ai-review --path . --pr-number ${{ github.event.number }}\n\n    - name: Generate Visualizations\n      run: |\n        codomyrmex visualize --analysis-file analysis.json --output-dir ./analysis-output\n\n    - name: Upload Analysis Artifacts\n      uses: actions/upload-artifact@v4\n      with:\n        name: codomyrmex-analysis-${{ github.sha }}\n        path: |\n          analysis.json\n          analysis-output/\n        retention-days: 30\n\n    - name: Post PR Comment\n      if: github.event_name == 'pull_request'\n      env:\n        GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}\n      run: |\n        python scripts/documentation/post_analysis_comment.py \\\\\n          --pr-number ${{ github.event.number }} \\\\\n          --analysis-file analysis.json \\\\\n          --repo ${{ github.repository }}\n\n    - name: Check Quality Gates\n      run: |\n        python scripts/maintenance/check_quality_gates.py \\\\\n          --analysis-file analysis.json \\\\\n          --fail-on-error \\\\\n          --min-quality-score 80\n\"\"\"",
      "syntax_valid": true,
      "import_errors": [],
      "api_mismatches": []
    },
    {
      "file_path": "docs/integration/external-systems.md",
      "line_number": 865,
      "language": "python",
      "code": "# monitoring_integration.py - Monitoring systems integration\nfrom prometheus_client import Counter, Histogram, Gauge, CollectorRegistry, generate_latest\nimport asyncio\nimport aiohttp\nfrom datetime import datetime, timedelta\nimport json\n\nclass PrometheusIntegration:\n    \"\"\"Integration with Prometheus for comprehensive monitoring.\"\"\"\n\n    def __init__(self, pushgateway_url: str = None):\n        self.registry = CollectorRegistry()\n        self.pushgateway_url = pushgateway_url\n\n        # Define Codomyrmex-specific metrics\n        self.analysis_counter = Counter(\n            'codomyrmex_analyses_total',\n            'Total number of analyses performed',\n            ['module', 'analysis_type', 'status'],\n            registry=self.registry\n        )\n\n        self.analysis_duration = Histogram(\n            'codomyrmex_analysis_duration_seconds',\n            'Time spent on analyses',\n            ['module', 'analysis_type'],\n            registry=self.registry,\n            buckets=[0.1, 0.5, 1, 5, 10, 30, 60, 300, 600]\n        )\n\n        self.active_jobs = Gauge(\n            'codomyrmex_active_jobs',\n            'Number of currently active jobs',\n            ['job_type'],\n            registry=self.registry\n        )\n\n        self.code_quality_score = Gauge(\n            'codomyrmex_code_quality_score',\n            'Latest code quality score',\n            ['project', 'module'],\n            registry=self.registry\n        )\n\n        self.ai_api_calls = Counter(\n            'codomyrmex_ai_api_calls_total',\n            'Total AI API calls',\n            ['provider', 'model', 'status'],\n            registry=self.registry\n        )\n\n        self.ai_token_usage = Counter(\n            'codomyrmex_ai_tokens_used_total',\n            'Total AI tokens consumed',\n            ['provider', 'model', 'token_type'],\n            registry=self.registry\n        )\n\n    def record_analysis(self, module: str, analysis_type: str,\n                       duration: float, success: bool = True):\n        \"\"\"Record analysis metrics.\"\"\"\n        status = 'success' if success else 'error'\n\n        self.analysis_counter.labels(\n            module=module,\n            analysis_type=analysis_type,\n            status=status\n        ).inc()\n\n        self.analysis_duration.labels(\n            module=module,\n            analysis_type=analysis_type\n        ).observe(duration)\n\n    def update_active_jobs(self, job_type: str, count: int):\n        \"\"\"Update active job count.\"\"\"\n        self.active_jobs.labels(job_type=job_type).set(count)\n\n    def record_quality_score(self, project: str, module: str, score: float):\n        \"\"\"Record code quality score.\"\"\"\n        self.code_quality_score.labels(\n            project=project,\n            module=module\n        ).set(score)\n\n    def record_ai_api_call(self, provider: str, model: str, success: bool = True,\n                          input_tokens: int = 0, output_tokens: int = 0):\n        \"\"\"Record AI API usage.\"\"\"\n        status = 'success' if success else 'error'\n\n        self.ai_api_calls.labels(\n            provider=provider,\n            model=model,\n            status=status\n        ).inc()\n\n        if input_tokens > 0:\n            self.ai_token_usage.labels(\n                provider=provider,\n                model=model,\n                token_type='input'\n            ).inc(input_tokens)\n\n        if output_tokens > 0:\n            self.ai_token_usage.labels(\n                provider=provider,\n                model=model,\n                token_type='output'\n            ).inc(output_tokens)\n\n    async def push_metrics(self, job_name: str = 'codomyrmex'):\n        \"\"\"Push metrics to Pushgateway.\"\"\"\n        if not self.pushgateway_url:\n            logger.warning(\"No Pushgateway URL configured\")\n            return\n\n        metrics_data = generate_latest(self.registry)\n        url = f\"{self.pushgateway_url}/metrics/job/{job_name}\"\n\n        async with aiohttp.ClientSession() as session:\n            async with session.post(\n                url,\n                data=metrics_data,\n                headers={'Content-Type': 'text/plain'}\n            ) as response:\n                if response.status == 200:\n                    logger.info(\"Metrics pushed to Pushgateway successfully\")\n                else:\n                    logger.error(f\"Failed to push metrics: {response.status}\")\n\nclass GrafanaIntegration:\n    \"\"\"Integration with Grafana for dashboard automation.\"\"\"\n\n    def __init__(self, grafana_url: str, api_key: str):\n        self.grafana_url = grafana_url.rstrip('/')\n        self.api_key = api_key\n        self.headers = {\n            'Authorization': f'Bearer {api_key}',\n            'Content-Type': 'application/json'\n        }\n\n    async def create_codomyrmex_dashboard(self) -> str:\n        \"\"\"Create comprehensive Codomyrmex dashboard.\"\"\"\n        dashboard_config = {\n            \"dashboard\": {\n                \"id\": None,\n                \"title\": \"Codomyrmex Analytics\",\n                \"tags\": [\"codomyrmex\", \"code-analysis\"],\n                \"timezone\": \"browser\",\n                \"refresh\": \"30s\",\n                \"time\": {\n                    \"from\": \"now-1h\",\n                    \"to\": \"now\"\n                },\n                \"panels\": [\n                    # Analysis Overview Panel\n                    {\n                        \"id\": 1,\n                        \"title\": \"Analysis Overview\",\n                        \"type\": \"stat\",\n                        \"targets\": [\n                            {\n                                \"expr\": \"sum(codomyrmex_analyses_total)\",\n                                \"refId\": \"A\"\n                            }\n                        ],\n                        \"fieldConfig\": {\n                            \"defaults\": {\n                                \"color\": {\"mode\": \"palette-classic\"},\n                                \"custom\": {\"displayMode\": \"list\", \"orientation\": \"auto\"},\n                                \"mappings\": [],\n                                \"thresholds\": {\n                                    \"mode\": \"absolute\",\n                                    \"steps\": [\n                                        {\"color\": \"green\", \"value\": None},\n                                        {\"color\": \"red\", \"value\": 80}\n                                    ]\n                                }\n                            }\n                        }\n                    },\n                    # Analysis Duration Panel\n                    {\n                        \"id\": 2,\n                        \"title\": \"Analysis Duration\",\n                        \"type\": \"graph\",\n                        \"targets\": [\n                            {\n                                \"expr\": \"histogram_quantile(0.95, codomyrmex_analysis_duration_seconds_bucket)\",\n                                \"refId\": \"A\",\n                                \"legendFormat\": \"95th percentile\"\n                            },\n                            {\n                                \"expr\": \"histogram_quantile(0.50, codomyrmex_analysis_duration_seconds_bucket)\",\n                                \"refId\": \"B\",\n                                \"legendFormat\": \"Median\"\n                            }\n                        ]\n                    },\n                    # Code Quality Trends\n                    {\n                        \"id\": 3,\n                        \"title\": \"Code Quality Trends\",\n                        \"type\": \"graph\",\n                        \"targets\": [\n                            {\n                                \"expr\": \"codomyrmex_code_quality_score\",\n                                \"refId\": \"A\",\n                                \"legendFormat\": \"{{project}}/{{module}}\"\n                            }\n                        ]\n                    },\n                    # AI API Usage\n                    {\n                        \"id\": 4,\n                        \"title\": \"AI API Usage\",\n                        \"type\": \"piechart\",\n                        \"targets\": [\n                            {\n                                \"expr\": \"sum by (provider) (codomyrmex_ai_api_calls_total)\",\n                                \"refId\": \"A\"\n                            }\n                        ]\n                    }\n                ]\n            },\n            \"overwrite\": True\n        }\n\n        async with aiohttp.ClientSession() as session:\n            url = f\"{self.grafana_url}/api/dashboards/db\"\n\n            async with session.post(\n                url,\n                headers=self.headers,\n                json=dashboard_config\n            ) as response:\n                if response.status == 200:\n                    result = await response.json()\n                    dashboard_url = f\"{self.grafana_url}/d/{result['uid']}\"\n                    logger.info(f\"Codomyrmex dashboard created: {dashboard_url}\")\n                    return dashboard_url\n                else:\n                    error = await response.text()\n                    logger.error(f\"Failed to create dashboard: {error}\")\n                    raise Exception(f\"Dashboard creation failed: {error}\")\n\n# Usage with monitoring integration\nmonitoring_prometheus = PrometheusIntegration(\"http://pushgateway:9091\")\nmonitoring_grafana = GrafanaIntegration(\n    \"http://grafana:3000\",\n    os.getenv(\"GRAFANA_API_KEY\")\n)\n\nasync def monitored_analysis_workflow(codebase_path: str):\n    \"\"\"Analysis workflow with comprehensive monitoring.\"\"\"\n    from codomyrmex.static_analysis import analyze_codebase\n    import time\n\n    start_time = time.time()\n\n    # Update active jobs\n    monitoring_prometheus.update_active_jobs('static_analysis', 1)\n\n    try:\n        # Perform analysis\n        result = analyze_codebase(codebase_path)\n        duration = time.time() - start_time\n\n        # Record successful analysis\n        monitoring_prometheus.record_analysis(\n            'static_analysis',\n            'codebase',\n            duration,\n            success=True\n        )\n\n        # Record quality score\n        monitoring_prometheus.record_quality_score(\n            Path(codebase_path).name,\n            'overall',\n            result.overall_quality_score\n        )\n\n        # Push metrics\n        await monitoring_prometheus.push_metrics()\n\n        return result\n\n    except Exception as e:\n        duration = time.time() - start_time\n\n        # Record failed analysis\n        monitoring_prometheus.record_analysis(\n            'static_analysis',\n            'codebase',\n            duration,\n            success=False\n        )\n\n        await monitoring_prometheus.push_metrics()\n        raise\n\n    finally:\n        # Update active jobs\n        monitoring_prometheus.update_active_jobs('static_analysis', 0)",
      "syntax_valid": true,
      "import_errors": [],
      "api_mismatches": []
    },
    {
      "file_path": "docs/modules/README.md",
      "line_number": 46,
      "language": "python",
      "code": "# Example: Using Codomyrmex modules programmatically\nfrom codomyrmex.data_visualization import create_line_plot\nfrom codomyrmex.ai_code_editing import generate_code_snippet\nfrom codomyrmex.static_analysis import run_pyrefly_analysis\n\n# Modules work independently and can be composed\nresult = run_pyrefly_analysis([\"src/\"], \".\")\ncode = generate_code_snippet(\"Create a function\", \"python\")\ncreate_line_plot(x_data, y_data, title=\"Analysis Results\")",
      "syntax_valid": true,
      "import_errors": [],
      "api_mismatches": []
    },
    {
      "file_path": "docs/modules/ollama_integration.md",
      "line_number": 26,
      "language": "python",
      "code": "from codomyrmex.ollama_integration import OllamaManager, ModelRunner\n\n# Initialize managers\nmanager = OllamaManager()\nrunner = ModelRunner(manager)\n\n# List available models\nmodels = manager.list_models()\nprint(f\"Available models: {len(models)}\")\n\n# Run a model\nresult = runner.run_with_options(\n    \"llama3.1:latest\",\n    \"Explain quantum computing in simple terms.\",\n    save_output=True\n)\n\nprint(f\"Response: {result.response}\")\nprint(f\"Execution time: {result.execution_time:.2f}s\")",
      "syntax_valid": true,
      "import_errors": [],
      "api_mismatches": []
    },
    {
      "file_path": "docs/modules/ollama_integration.md",
      "line_number": 50,
      "language": "python",
      "code": "from codomyrmex.ollama_integration import ConfigManager\n\n# Initialize configuration manager\nconfig = ConfigManager()\n\n# Update settings\nconfig.update_config(\n    default_model=\"llama3.1:latest\",\n    base_output_dir=\"custom/output/path\"\n)\n\n# Save configuration\nconfig.save_config()",
      "syntax_valid": true,
      "import_errors": [],
      "api_mismatches": []
    },
    {
      "file_path": "docs/modules/ollama_integration.md",
      "line_number": 105,
      "language": "python",
      "code": "# List all available models\nmodels = manager.list_models(force_refresh=True)\n\n# Check model availability\navailable = manager.is_model_available(\"llama3.1:latest\")\n\n# Get model by name\nmodel = manager.get_model_by_name(\"llama3.1:latest\")\n\n# Get model statistics\nstats = manager.get_model_stats()",
      "syntax_valid": true,
      "import_errors": [],
      "api_mismatches": []
    },
    {
      "file_path": "docs/modules/ollama_integration.md",
      "line_number": 121,
      "language": "python",
      "code": "# Basic execution\nresult = manager.run_model(\n    model_name=\"llama3.1:latest\",\n    prompt=\"Your prompt here\",\n    save_output=True,\n    output_dir=\"/custom/path\"\n)",
      "syntax_valid": true,
      "import_errors": [],
      "api_mismatches": []
    },
    {
      "file_path": "docs/modules/ollama_integration.md",
      "line_number": 135,
      "language": "python",
      "code": "from codomyrmex.ollama_integration.model_runner import ExecutionOptions\n\n# Custom execution options\noptions = ExecutionOptions(\n    temperature=0.7,\n    max_tokens=512,\n    timeout=120\n)\n\nresult = runner.run_with_options(\n    \"llama3.1:latest\",\n    \"Your prompt\",\n    options,\n    save_output=True\n)",
      "syntax_valid": true,
      "import_errors": [],
      "api_mismatches": []
    },
    {
      "file_path": "docs/modules/ollama_integration.md",
      "line_number": 155,
      "language": "python",
      "code": "# Process multiple prompts\nprompts = [\"Prompt 1\", \"Prompt 2\", \"Prompt 3\"]\nresults = runner.run_batch(\n    \"llama3.1:latest\",\n    prompts,\n    max_concurrent=3\n)",
      "syntax_valid": true,
      "import_errors": [],
      "api_mismatches": []
    },
    {
      "file_path": "docs/modules/ollama_integration.md",
      "line_number": 167,
      "language": "python",
      "code": "# Compare multiple models\ncomparison = runner.create_model_comparison(\n    [\"llama2:latest\", \"llama3.1:latest\"],\n    \"Which model is better?\"\n)",
      "syntax_valid": true,
      "import_errors": [],
      "api_mismatches": []
    },
    {
      "file_path": "docs/modules/ollama_integration.md",
      "line_number": 177,
      "language": "python",
      "code": "# Benchmark model performance\nbenchmark = runner.benchmark_model(\n    \"llama3.1:latest\",\n    [\"Test prompt 1\", \"Test prompt 2\"]\n)",
      "syntax_valid": true,
      "import_errors": [],
      "api_mismatches": []
    },
    {
      "file_path": "docs/modules/ollama_integration.md",
      "line_number": 189,
      "language": "python",
      "code": "# Save model response\noutput_path = output_manager.save_model_output(\n    model_name=\"llama3.1:latest\",\n    prompt=\"User prompt\",\n    response=\"Model response\",\n    execution_time=2.5\n)\n\n# Save execution result\nresult_path = output_manager.save_execution_result(result)\n\n# Save model configuration\nconfig_path = output_manager.save_model_config(\n    \"llama3.1:latest\",\n    {\"temperature\": 0.7, \"max_tokens\": 512}\n)",
      "syntax_valid": true,
      "import_errors": [],
      "api_mismatches": []
    },
    {
      "file_path": "docs/modules/ollama_integration.md",
      "line_number": 210,
      "language": "python",
      "code": "# Load model configuration\nconfig = output_manager.load_model_config(\"llama3.1:latest\")\n\n# Get output statistics\nstats = output_manager.get_output_stats()\n\n# List saved outputs\noutputs = output_manager.list_saved_outputs(output_type=\"outputs\")",
      "syntax_valid": true,
      "import_errors": [],
      "api_mismatches": []
    },
    {
      "file_path": "docs/modules/ollama_integration.md",
      "line_number": 225,
      "language": "python",
      "code": "# Load configuration\nconfig_manager.load_config()\n\n# Update settings\nconfig_manager.update_config(\n    default_model=\"llama3.1:latest\",\n    auto_start_server=False\n)\n\n# Save configuration\nconfig_manager.save_config()\n\n# Validate configuration\nvalidation = config_manager.validate_config()",
      "syntax_valid": true,
      "import_errors": [],
      "api_mismatches": []
    },
    {
      "file_path": "docs/modules/ollama_integration.md",
      "line_number": 244,
      "language": "python",
      "code": "# Get available presets\npresets = config_manager.get_execution_presets()\n\n# Use a preset\noptions = presets[\"creative\"]  # Temperature 0.9, max_tokens 1024",
      "syntax_valid": true,
      "import_errors": [],
      "api_mismatches": []
    },
    {
      "file_path": "docs/modules/ollama_integration.md",
      "line_number": 254,
      "language": "python",
      "code": "# Export configuration\nconfig_manager.export_config(\"backup_config.json\")\n\n# Import configuration\nconfig_manager.import_config(\"backup_config.json\")",
      "syntax_valid": true,
      "import_errors": [],
      "api_mismatches": []
    },
    {
      "file_path": "docs/modules/ollama_integration.md",
      "line_number": 311,
      "language": "python",
      "code": "from codomyrmex.ollama_integration import OllamaManager, ModelRunner\n\n# Initialize\nmanager = OllamaManager()\nrunner = ModelRunner(manager)\n\n# Simple execution\nresult = manager.run_model(\n    \"llama3.1:latest\",\n    \"What is machine learning?\",\n    save_output=True\n)\n\nprint(f\"Answer: {result.response}\")",
      "syntax_valid": true,
      "import_errors": [],
      "api_mismatches": []
    },
    {
      "file_path": "docs/modules/ollama_integration.md",
      "line_number": 330,
      "language": "python",
      "code": "from codomyrmex.ollama_integration.model_runner import ExecutionOptions\n\n# Custom options\noptions = ExecutionOptions(\n    temperature=0.8,\n    max_tokens=1024,\n    system_prompt=\"You are a helpful coding assistant.\"\n)\n\nresult = runner.run_with_options(\n    \"codellama:latest\",\n    \"Write a Python function to calculate fibonacci numbers.\",\n    options\n)",
      "syntax_valid": true,
      "import_errors": [],
      "api_mismatches": []
    },
    {
      "file_path": "docs/modules/ollama_integration.md",
      "line_number": 349,
      "language": "python",
      "code": "# Process multiple prompts\nprompts = [\n    \"Explain recursion.\",\n    \"What is object-oriented programming?\",\n    \"How does garbage collection work?\"\n]\n\nresults = runner.run_batch(\n    \"llama3.1:latest\",\n    prompts,\n    max_concurrent=2\n)\n\nfor i, result in enumerate(results):\n    print(f\"Prompt {i+1}: {result.response[:100]}...\")",
      "syntax_valid": true,
      "import_errors": [],
      "api_mismatches": []
    },
    {
      "file_path": "docs/modules/ollama_integration.md",
      "line_number": 369,
      "language": "python",
      "code": "# Compare different models\ncomparison = runner.create_model_comparison(\n    [\"llama2:latest\", \"llama3.1:latest\", \"gemma2:2b\"],\n    \"Which programming language is best for beginners?\"\n)\n\nfor model, data in comparison['results'].items():\n    print(f\"{model}: {data['response_preview']}\")",
      "syntax_valid": true,
      "import_errors": [],
      "api_mismatches": []
    },
    {
      "file_path": "docs/modules/ollama_integration.md",
      "line_number": 382,
      "language": "python",
      "code": "from codomyrmex.ollama_integration import ConfigManager, OutputManager\n\n# Setup configuration\nconfig = ConfigManager()\nconfig.update_config(\n    default_model=\"llama3.1:latest\",\n    base_output_dir=\"my_ollama_outputs\"\n)\n\n# Setup output management\noutput_manager = OutputManager(\"my_ollama_outputs\")\n\n# Run and save\nresult = manager.run_model(\n    config.config.default_model,\n    \"Your prompt here\",\n    save_output=True,\n    output_dir=output_manager.outputs_dir\n)",
      "syntax_valid": true,
      "import_errors": [],
      "api_mismatches": []
    },
    {
      "file_path": "docs/modules/ollama_integration.md",
      "line_number": 408,
      "language": "python",
      "code": "# Multi-turn conversation\nmessages = [\n    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n    {\"role\": \"user\", \"content\": \"Hello!\"},\n    {\"role\": \"assistant\", \"content\": \"Hi there! How can I help?\"},\n    {\"role\": \"user\", \"content\": \"What's the weather like?\"}\n]\n\nresult = runner.run_conversation(\"llama3.1:latest\", messages)",
      "syntax_valid": true,
      "import_errors": [],
      "api_mismatches": []
    },
    {
      "file_path": "docs/modules/ollama_integration.md",
      "line_number": 422,
      "language": "python",
      "code": "# Include additional context\ncontext_docs = [\n    \"Context: Machine learning is AI that learns from data.\",\n    \"Context: Neural networks mimic brain structure.\"\n]\n\nresult = runner.run_with_context(\n    \"llama3.1:latest\",\n    \"Explain how neural networks work.\",\n    context_docs\n)",
      "syntax_valid": true,
      "import_errors": [],
      "api_mismatches": []
    },
    {
      "file_path": "docs/modules/ollama_integration.md",
      "line_number": 438,
      "language": "python",
      "code": "# Performance benchmarking\ntest_prompts = [\n    \"What is 2+2?\",\n    \"Explain quantum physics.\",\n    \"Write a haiku about coding.\"\n]\n\nbenchmark = runner.benchmark_model(\"llama3.1:latest\", test_prompts)\nprint(f\"Average execution time: {benchmark['avg_execution_time']:.2f}s\")\nprint(f\"Average tokens/sec: {benchmark['avg_tokens_per_second']:.1f}\")",
      "syntax_valid": true,
      "import_errors": [],
      "api_mismatches": []
    },
    {
      "file_path": "docs/modules/ollama_integration.md",
      "line_number": 477,
      "language": "python",
      "code": "# Logging is automatic - no additional setup needed\nresult = manager.run_model(\"llama3.1:latest\", \"test prompt\")\n# This will be logged with timestamps, execution time, etc.",
      "syntax_valid": true,
      "import_errors": [],
      "api_mismatches": []
    },
    {
      "file_path": "docs/modules/ollama_integration.md",
      "line_number": 485,
      "language": "python",
      "code": "# Visualize model comparison results\nfrom codomyrmex.data_visualization import create_bar_chart\n\ncomparison = runner.create_model_comparison(models, prompt)\ncategories = list(comparison['results'].keys())\nexecution_times = [r['execution_time'] for r in comparison['results'].values()]\n\ncreate_bar_chart(\n    categories=categories,\n    values=execution_times,\n    title=\"Model Execution Time Comparison\",\n    output_path=\"model_comparison.png\"\n)",
      "syntax_valid": true,
      "import_errors": [],
      "api_mismatches": []
    },
    {
      "file_path": "docs/modules/ollama_integration.md",
      "line_number": 555,
      "language": "python",
      "code": "# Reset to defaults if config is corrupted\nconfig = ConfigManager()\nconfig.reset_to_defaults()\nconfig.save_config()",
      "syntax_valid": true,
      "import_errors": [],
      "api_mismatches": []
    },
    {
      "file_path": "docs/modules/overview.md",
      "line_number": 132,
      "language": "python",
      "code": "from codomyrmex.data_visualization import create_line_plot\nfrom codomyrmex.ai_code_editing import generate_code_snippet\n\n# Direct function calls\nplot_result = create_line_plot(x_data, y_data, title=\"Sample Plot\")\ncode_result = generate_code_snippet(\"Create a factorial function\", \"python\")",
      "syntax_valid": true,
      "import_errors": [],
      "api_mismatches": []
    },
    {
      "file_path": "docs/modules/overview.md",
      "line_number": 143,
      "language": "python",
      "code": "# Publishing events\nfrom codomyrmex.logging_monitoring import get_logger\nlogger = get_logger(__name__)\nlogger.info(\"Code generation completed\", extra={\"event_type\": \"code_generated\"})\n\n# Subscribing to events  \ndef on_code_generated(event_data):\n    # Automatically run static analysis on generated code\n    pass",
      "syntax_valid": true,
      "import_errors": [],
      "api_mismatches": []
    },
    {
      "file_path": "docs/modules/overview.md",
      "line_number": 157,
      "language": "python",
      "code": "# Analysis \u2192 AI Enhancement \u2192 Validation Pipeline\ndef enhance_code_pipeline(source_code):\n    # 1. Analyze existing code\n    analysis = pattern_matching.analyze_code(source_code)\n    \n    # 2. Generate improvements with AI\n    improvements = ai_code_editing.suggest_improvements(source_code, analysis)\n    \n    # 3. Validate generated code\n    validation = code_execution_sandbox.validate_code(improvements)\n    \n    return validation",
      "syntax_valid": true,
      "import_errors": [],
      "api_mismatches": []
    },
    {
      "file_path": "docs/modules/overview.md",
      "line_number": 220,
      "language": "python",
      "code": "from codomyrmex.logging_monitoring import get_logger\n   \n   logger = get_logger(__name__)\n   \n   try:\n       result = risky_operation()\n       logger.info(\"Operation completed successfully\")\n       return result\n   except SpecificError as e:\n       logger.error(f\"Known error occurred: {e}\")\n       raise\n   except Exception as e:\n       logger.error(f\"Unexpected error: {e}\", exc_info=True)\n       raise",
      "syntax_valid": false,
      "import_errors": [],
      "api_mismatches": []
    },
    {
      "file_path": "docs/modules/overview.md",
      "line_number": 238,
      "language": "python",
      "code": "import os\n   from codomyrmex.environment_setup import check_and_setup_env_vars\n   \n   # Ensure environment is configured\n   check_and_setup_env_vars(project_root)\n   \n   # Use environment variables for configuration\n   API_KEY = os.getenv(\"MY_MODULE_API_KEY\")\n   DEBUG_MODE = os.getenv(\"MY_MODULE_DEBUG\", \"false\").lower() == \"true\"",
      "syntax_valid": false,
      "import_errors": [],
      "api_mismatches": []
    },
    {
      "file_path": "docs/modules/overview.md",
      "line_number": 253,
      "language": "python",
      "code": "from codomyrmex.system_discovery import SystemDiscovery\n\ndiscovery = SystemDiscovery()\nmodules = discovery.discover_modules()\n\nfor module_name, module_info in modules.items():\n    print(f\"Module: {module_name}\")\n    print(f\"Capabilities: {len(module_info.capabilities)}\")\n    print(f\"Status: {'\u2705' if module_info.is_importable else '\u274c'}\")",
      "syntax_valid": true,
      "import_errors": [],
      "api_mismatches": []
    },
    {
      "file_path": "docs/modules/overview.md",
      "line_number": 266,
      "language": "python",
      "code": "# Import specific functionality\nfrom codomyrmex.data_visualization import create_bar_chart\nfrom codomyrmex.ai_code_editing import generate_code_snippet\n\n# Use modules in combination\ndef create_code_with_visualization():\n    # Generate code\n    code_result = generate_code_snippet(\n        \"Create a function that calculates prime numbers up to n\",\n        \"python\"\n    )\n    \n    if code_result[\"status\"] == \"success\":\n        # Create visualization of the process\n        create_bar_chart(\n            categories=[\"Generated\", \"Validated\", \"Ready\"],\n            values=[1, 1, 1],\n            title=\"Code Generation Pipeline Status\",\n            output_path=\"generation_status.png\"\n        )",
      "syntax_valid": true,
      "import_errors": [],
      "api_mismatches": []
    },
    {
      "file_path": "docs/modules/overview.md",
      "line_number": 305,
      "language": "python",
      "code": "from codomyrmex.system_discovery import SystemDiscovery\n\ndiscovery = SystemDiscovery()\nstatus = discovery.check_module_health()\n\nprint(f\"Modules working: {status['working_count']}/{status['total_count']}\")\nprint(f\"Dependencies satisfied: {status['dependencies_ok']}\")\nprint(f\"Tests passing: {status['tests_passing']}\")",
      "syntax_valid": true,
      "import_errors": [],
      "api_mismatches": []
    },
    {
      "file_path": "docs/modules/overview.md",
      "line_number": 317,
      "language": "python",
      "code": "from codomyrmex.logging_monitoring import get_logger\nimport time\n\nlogger = get_logger(__name__)\n\ndef timed_operation():\n    start_time = time.time()\n    try:\n        result = expensive_operation()\n        duration = time.time() - start_time\n        logger.info(f\"Operation completed in {duration:.2f}s\")\n        return result\n    except Exception as e:\n        duration = time.time() - start_time\n        logger.error(f\"Operation failed after {duration:.2f}s: {e}\")\n        raise",
      "syntax_valid": true,
      "import_errors": [],
      "api_mismatches": []
    },
    {
      "file_path": "docs/modules/relationships.md",
      "line_number": 76,
      "language": "python",
      "code": "# Every module imports this for setup validation\n  from environment_setup.env_checker import ensure_dependencies_installed\n\n  # Called at module initialization\n  ensure_dependencies_installed()",
      "syntax_valid": false,
      "import_errors": [],
      "api_mismatches": []
    },
    {
      "file_path": "docs/modules/relationships.md",
      "line_number": 87,
      "language": "python",
      "code": "# Universal logging interface across all modules\n  from logging_monitoring import get_logger\n  logger = get_logger(__name__)\n\n  # Consistent log format across entire project\n  logger.info(\"Module operation completed\")",
      "syntax_valid": false,
      "import_errors": [],
      "api_mismatches": []
    },
    {
      "file_path": "docs/modules/relationships.md",
      "line_number": 99,
      "language": "python",
      "code": "# AI modules implement MCP tools\n  from model_context_protocol.mcp_schemas import MCPToolCall, MCPToolResult\n\n  # Standardized request/response format\n  tool_call = MCPToolCall(tool_name=\"ai_code_editing.generate_code\", ...)",
      "syntax_valid": false,
      "import_errors": [],
      "api_mismatches": []
    },
    {
      "file_path": "docs/modules/relationships.md",
      "line_number": 113,
      "language": "python",
      "code": "# Used by pattern_matching for code understanding\n  from ai_code_editing.ai_code_helpers import generate_code_snippet\n\n  # Used by documentation for example generation\n  result = generate_code_snippet(\"Create a hello world function\", \"python\")",
      "syntax_valid": false,
      "import_errors": [],
      "api_mismatches": []
    },
    {
      "file_path": "docs/modules/relationships.md",
      "line_number": 125,
      "language": "python",
      "code": "# Used by static_analysis for comprehensive analysis\n  from pattern_matching.run_codomyrmex_analysis import analyze_repository_path\n\n  # Comprehensive analysis workflow\n  analysis_results = analyze_repository_path(path, config)",
      "syntax_valid": false,
      "import_errors": [],
      "api_mismatches": []
    },
    {
      "file_path": "docs/modules/relationships.md",
      "line_number": 139,
      "language": "python",
      "code": "# Used by build_synthesis for quality gates\n  from static_analysis.pyrefly_runner import run_pyrefly_analysis\n\n  # Quality check before build\n  issues = run_pyrefly_analysis(target_paths, project_root)",
      "syntax_valid": false,
      "import_errors": [],
      "api_mismatches": []
    },
    {
      "file_path": "docs/modules/relationships.md",
      "line_number": 151,
      "language": "python",
      "code": "# Used by ai_code_editing for code validation\n  from code_execution_sandbox.code_executor import execute_code\n\n  # Test generated code before applying\n  result = execute_code(\"print('test')\", \"python\")",
      "syntax_valid": false,
      "import_errors": [],
      "api_mismatches": []
    },
    {
      "file_path": "docs/modules/relationships.md",
      "line_number": 165,
      "language": "python",
      "code": "# Orchestrates multiple modules for complete build pipeline\n  from build_synthesis.build_orchestrator import trigger_build\n  from static_analysis.pyrefly_runner import run_pyrefly_analysis\n\n  # Quality-gated build process\n  analysis = run_pyrefly_analysis(paths, root)\n  if not analysis[\"issues\"]:\n      build_result = trigger_build(\"production\")",
      "syntax_valid": false,
      "import_errors": [],
      "api_mismatches": []
    },
    {
      "file_path": "docs/modules/relationships.md",
      "line_number": 180,
      "language": "python",
      "code": "# Used by build_synthesis for version control integration\n  from git_operations.git_wrapper import create_branch, commit_changes\n\n  # Automated release workflow\n  create_branch(\"release/v1.0.0\")\n  commit_changes(\"Release version 1.0.0\")",
      "syntax_valid": false,
      "import_errors": [],
      "api_mismatches": []
    },
    {
      "file_path": "docs/modules/relationships.md",
      "line_number": 195,
      "language": "python",
      "code": "# Used by pattern_matching for analysis visualization\n  from data_visualization.plotter import create_heatmap\n  from pattern_matching.run_codomyrmex_analysis import analyze_repository_path\n\n  # Visualize analysis results\n  analysis = analyze_repository_path(path, config)\n  create_heatmap(analysis[\"dependency_matrix\"], title=\"Code Dependencies\")",
      "syntax_valid": false,
      "import_errors": [],
      "api_mismatches": []
    },
    {
      "file_path": "docs/modules/relationships.md",
      "line_number": 209,
      "language": "python",
      "code": "# Generates documentation from all modules\n  from documentation.documentation_website import build_static_site\n\n  # Auto-generate docs from module APIs\n  build_static_site()",
      "syntax_valid": false,
      "import_errors": [],
      "api_mismatches": []
    },
    {
      "file_path": "docs/modules/relationships.md",
      "line_number": 220,
      "language": "python",
      "code": "# Standard module initialization pattern used across all modules\nfrom environment_setup.env_checker import ensure_dependencies_installed\nfrom logging_monitoring import get_logger\n\n# 1. Validate environment\nensure_dependencies_installed()\n\n# 2. Setup logging\nlogger = get_logger(__name__)\n\n# 3. Module-specific initialization\n# ... module specific setup ...",
      "syntax_valid": true,
      "import_errors": [],
      "api_mismatches": []
    },
    {
      "file_path": "docs/modules/relationships.md",
      "line_number": 236,
      "language": "python",
      "code": "# Consistent error handling across modules\ntry:\n    result = perform_operation()\n    logger.info(f\"Operation completed: {result}\")\nexcept ModuleSpecificError as e:\n    logger.error(f\"Module error: {e}\")\n    raise\nexcept Exception as e:\n    logger.error(f\"Unexpected error: {e}\", exc_info=True)\n    raise",
      "syntax_valid": true,
      "import_errors": [],
      "api_mismatches": []
    },
    {
      "file_path": "docs/modules/relationships.md",
      "line_number": 250,
      "language": "python",
      "code": "# Environment variables shared across modules\nimport os\nfrom environment_setup.env_checker import check_and_setup_env_vars\n\n# Ensure .env is loaded\ncheck_and_setup_env_vars(\"/path/to/project\")\n\n# Shared configuration\nOPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\nLOG_LEVEL = os.getenv(\"LOG_LEVEL\", \"INFO\")",
      "syntax_valid": true,
      "import_errors": [],
      "api_mismatches": []
    },
    {
      "file_path": "docs/modules/relationships.md",
      "line_number": 351,
      "language": "python",
      "code": "from ai_code_editing.ai_code_helpers import generate_code_snippet\nfrom model_context_protocol.mcp_schemas import MCPToolCall\n\ndef enhance_code_with_ai(code_snippet, enhancement_request):\n    \"\"\"Add AI enhancement capability to any module\"\"\"\n    result = generate_code_snippet(\n        prompt=f\"Enhance this code: {enhancement_request}\",\n        language=\"python\",\n        context_code=code_snippet\n    )\n    return result",
      "syntax_valid": true,
      "import_errors": [],
      "api_mismatches": []
    },
    {
      "file_path": "docs/modules/relationships.md",
      "line_number": 366,
      "language": "python",
      "code": "from data_visualization.plotter import create_bar_chart\nfrom static_analysis.pyrefly_runner import run_pyrefly_analysis\n\ndef visualize_analysis_results(target_paths, project_root):\n    \"\"\"Create visual representation of analysis results\"\"\"\n    analysis = run_pyrefly_analysis(target_paths, project_root)\n\n    # Extract issue counts by severity\n    severity_counts = {}\n    for issue in analysis[\"issues\"]:\n        severity = issue.get(\"severity\", \"unknown\")\n        severity_counts[severity] = severity_counts.get(severity, 0) + 1\n\n    # Create visualization\n    create_bar_chart(\n        categories=list(severity_counts.keys()),\n        values=list(severity_counts.values()),\n        title=\"Code Analysis Issues by Severity\",\n        x_label=\"Severity Level\",\n        y_label=\"Issue Count\",\n        output_path=\"analysis_report.png\"\n    )",
      "syntax_valid": true,
      "import_errors": [],
      "api_mismatches": []
    },
    {
      "file_path": "docs/modules/relationships.md",
      "line_number": 392,
      "language": "python",
      "code": "from environment_setup.env_checker import ensure_dependencies_installed\nfrom logging_monitoring import get_logger\nfrom ai_code_editing.ai_code_helpers import generate_code_snippet\nfrom code_execution_sandbox.code_executor import execute_code\nfrom data_visualization.plotter import create_line_plot\n\ndef complete_development_workflow():\n    \"\"\"Complete workflow using multiple modules\"\"\"\n    # 1. Setup\n    ensure_dependencies_installed()\n    logger = get_logger(__name__)\n\n    # 2. Generate code with AI\n    code_result = generate_code_snippet(\n        \"Create a function to calculate fibonacci numbers\",\n        \"python\"\n    )\n\n    # 3. Test the generated code\n    if code_result[\"status\"] == \"success\":\n        exec_result = execute_code(\n            \"python\",\n            code_result[\"generated_code\"],\n            stdin=\"10\"\n        )\n\n        # 4. Visualize results\n        create_line_plot(\n            x_data=list(range(10)),\n            y_data=[int(x) for x in exec_result[\"output\"].split()],\n            title=\"Fibonacci Sequence\",\n            output_path=\"fibonacci_plot.png\"\n        )\n\n        logger.info(\"Complete workflow executed successfully\")\n    else:\n        logger.error(\"Code generation failed\")",
      "syntax_valid": true,
      "import_errors": [],
      "api_mismatches": []
    },
    {
      "file_path": "docs/reference/README.md",
      "line_number": 57,
      "language": "python",
      "code": "from codomyrmex.data_visualization import create_bar_chart\nfrom codomyrmex.code_execution_sandbox import execute_code\n\n# See api-complete.md for full API details\nresult = execute_code(\"python\", \"print('Hello')\")\ncreate_bar_chart(categories, values, title=\"Results\")",
      "syntax_valid": true,
      "import_errors": [],
      "api_mismatches": []
    },
    {
      "file_path": "docs/reference/api-complete.md",
      "line_number": 18,
      "language": "python",
      "code": "def create_line_plot(\n    x_data: list,\n    y_data: list,\n    title: str = \"Line Plot\",\n    x_label: str = \"X-axis\",\n    y_label: str = \"Y-axis\",\n    output_path: str = None,\n    show_plot: bool = False,\n    line_labels: list = None,\n    markers: bool = False,\n    figure_size: tuple = (10, 6)  # DEFAULT_FIGURE_SIZE\n) -> matplotlib.figure.Figure",
      "syntax_valid": false,
      "import_errors": [],
      "api_mismatches": []
    },
    {
      "file_path": "docs/reference/api-complete.md",
      "line_number": 34,
      "language": "python",
      "code": "from codomyrmex.data_visualization.line_plot import create_line_plot\n\n# Simple line plot\nx_data = [1, 2, 3, 4, 5]\ny_data = [2, 3, 5, 7, 6]\nfig = create_line_plot(\n    x_data=x_data,\n    y_data=y_data,\n    title=\"Sample Line Plot\",\n    output_path=\"plot.png\",\n    markers=True\n)\n\n# Multiple lines\ny_multiple = [[1, 2, 3, 4, 5], [5, 4, 3, 2, 1]]\nline_labels = ['Ascending', 'Descending']\nfig = create_line_plot(\n    x_data=x_data,\n    y_data=y_multiple,\n    line_labels=line_labels,\n    title=\"Multiple Lines\",\n    output_path=\"multi_plot.png\"\n)",
      "syntax_valid": true,
      "import_errors": [],
      "api_mismatches": []
    },
    {
      "file_path": "docs/reference/api-complete.md",
      "line_number": 63,
      "language": "python",
      "code": "def create_bar_chart(\n    categories: list,\n    values: list,\n    title: str = \"Bar Chart\",\n    x_label: str = \"Categories\",\n    y_label: str = \"Values\",\n    output_path: str = None,\n    show_plot: bool = False,\n    horizontal: bool = False,\n    figure_size: tuple = (10, 6)\n) -> matplotlib.figure.Figure",
      "syntax_valid": false,
      "import_errors": [],
      "api_mismatches": []
    },
    {
      "file_path": "docs/reference/api-complete.md",
      "line_number": 88,
      "language": "python",
      "code": "def get_codomyrmex_logger(name: str) -> logging.Logger\ndef save_plot(fig, output_path: str, dpi: int = 300)\ndef apply_common_aesthetics(ax, title: str = None, x_label: str = None, y_label: str = None)",
      "syntax_valid": false,
      "import_errors": [],
      "api_mismatches": []
    },
    {
      "file_path": "docs/reference/api-complete.md",
      "line_number": 99,
      "language": "python",
      "code": "def run_pyrefly_analysis(target_paths: list[str], project_root: str) -> dict",
      "syntax_valid": false,
      "import_errors": [],
      "api_mismatches": []
    },
    {
      "file_path": "docs/reference/api-complete.md",
      "line_number": 104,
      "language": "python",
      "code": "from codomyrmex.static_analysis.pyrefly_runner import run_pyrefly_analysis\n\n# Analyze Python files\nresult = run_pyrefly_analysis(\n    target_paths=[\"src/my_module.py\", \"src/another_module.py\"],\n    project_root=\"/path/to/project\"\n)\n\n# Result structure follows MCP_TOOL_SPECIFICATION\nprint(f\"Found {len(result.get('issues', []))} issues\")",
      "syntax_valid": true,
      "import_errors": [],
      "api_mismatches": []
    },
    {
      "file_path": "docs/reference/api-complete.md",
      "line_number": 117,
      "language": "python",
      "code": "def parse_pyrefly_output(output: str, project_root: str) -> list",
      "syntax_valid": false,
      "import_errors": [],
      "api_mismatches": []
    },
    {
      "file_path": "docs/reference/api-complete.md",
      "line_number": 122,
      "language": "python",
      "code": "# Parse Pyrefly command output\nissues = parse_pyrefly_output(\n    output=\"src/file.py:10:5: error: Undefined variable 'x'\",\n    project_root=\"/project/root\"\n)\n# Returns list of structured issue dictionaries",
      "syntax_valid": true,
      "import_errors": [],
      "api_mismatches": []
    },
    {
      "file_path": "docs/reference/api-complete.md",
      "line_number": 136,
      "language": "python",
      "code": "def execute_code(\n    code: str,\n    language: str,\n    timeout: Optional[int] = None,\n    session_id: Optional[str] = None,\n    stdin: Optional[str] = None\n) -> Dict[str, Any]",
      "syntax_valid": false,
      "import_errors": [],
      "api_mismatches": []
    },
    {
      "file_path": "docs/reference/api-complete.md",
      "line_number": 147,
      "language": "python",
      "code": "from codomyrmex.code_execution_sandbox.code_executor import execute_code\n\n# Execute Python code\nresult = execute_code(\n    code=\"print('Hello, World!')\\nprint(2 + 2)\",\n    language=\"python\",\n    timeout=30\n)\n\nprint(result['output'])  # \"Hello, World!\\n4\\n\"\nprint(result['success'])  # True\nprint(result['execution_time'])  # Time in seconds",
      "syntax_valid": true,
      "import_errors": [],
      "api_mismatches": []
    },
    {
      "file_path": "docs/reference/api-complete.md",
      "line_number": 163,
      "language": "python",
      "code": "def check_docker_available() -> bool\ndef validate_language(language: str) -> bool\ndef validate_timeout(timeout: Optional[int]) -> int\ndef validate_session_id(session_id: Optional[str]) -> Optional[str]",
      "syntax_valid": false,
      "import_errors": [],
      "api_mismatches": []
    },
    {
      "file_path": "docs/reference/api-complete.md",
      "line_number": 171,
      "language": "python",
      "code": "def prepare_code_file(code: str, language: str) -> Tuple[str, str]\ndef prepare_stdin_file(stdin: Optional[str], temp_dir: str) -> Optional[str]\ndef cleanup_temp_files(temp_dir: str) -> None",
      "syntax_valid": false,
      "import_errors": [],
      "api_mismatches": []
    },
    {
      "file_path": "docs/reference/api-complete.md",
      "line_number": 182,
      "language": "python",
      "code": "def is_uv_available() -> bool\ndef is_uv_environment() -> bool  \ndef ensure_dependencies_installed() -> None\ndef check_and_setup_env_vars(repo_root_path: str) -> None",
      "syntax_valid": false,
      "import_errors": [],
      "api_mismatches": []
    },
    {
      "file_path": "docs/reference/api-complete.md",
      "line_number": 190,
      "language": "python",
      "code": "from codomyrmex.environment_setup.env_checker import (\n    is_uv_available, \n    ensure_dependencies_installed\n)\n\nif is_uv_available():\n    print(\"UV package manager is available\")\n\n# Ensure required dependencies are installed\nensure_dependencies_installed()",
      "syntax_valid": true,
      "import_errors": [],
      "api_mismatches": []
    },
    {
      "file_path": "docs/reference/api-complete.md",
      "line_number": 208,
      "language": "python",
      "code": "def check_git_availability() -> bool\ndef is_git_repository(path: str = None) -> bool\ndef initialize_git_repository(path: str, initial_commit: bool = True) -> bool\ndef clone_repository(url: str, destination: str, branch: str = None) -> bool",
      "syntax_valid": false,
      "import_errors": [],
      "api_mismatches": []
    },
    {
      "file_path": "docs/reference/api-complete.md",
      "line_number": 216,
      "language": "python",
      "code": "def create_branch(branch_name: str, repository_path: str = None) -> bool\ndef switch_branch(branch_name: str, repository_path: str = None) -> bool\ndef get_current_branch(repository_path: str = None) -> Optional[str]\ndef merge_branch(source_branch: str, target_branch: str = None, repository_path: str = None, fast_forward_only: bool = False) -> bool\ndef rebase_branch(target_branch: str, repository_path: str = None, interactive: bool = False) -> bool",
      "syntax_valid": false,
      "import_errors": [],
      "api_mismatches": []
    },
    {
      "file_path": "docs/reference/api-complete.md",
      "line_number": 225,
      "language": "python",
      "code": "def add_files(file_paths: List[str], repository_path: str = None) -> bool\ndef commit_changes(message: str, repository_path: str = None) -> bool\ndef push_changes(remote: str = \"origin\", branch: str = None, repository_path: str = None) -> bool\ndef pull_changes(remote: str = \"origin\", branch: str = None, repository_path: str = None) -> bool",
      "syntax_valid": false,
      "import_errors": [],
      "api_mismatches": []
    },
    {
      "file_path": "docs/reference/api-complete.md",
      "line_number": 233,
      "language": "python",
      "code": "def get_status(repository_path: str = None) -> Dict[str, any]\ndef get_commit_history(limit: int = 10, repository_path: str = None) -> List[Dict[str, str]]\ndef get_diff(file_path: str = None, staged: bool = False, repository_path: str = None) -> str",
      "syntax_valid": false,
      "import_errors": [],
      "api_mismatches": []
    },
    {
      "file_path": "docs/reference/api-complete.md",
      "line_number": 240,
      "language": "python",
      "code": "def create_tag(tag_name: str, message: str = None, repository_path: str = None) -> bool\ndef list_tags(repository_path: str = None) -> List[str]\ndef stash_changes(message: str = None, repository_path: str = None) -> bool\ndef apply_stash(stash_ref: str = None, repository_path: str = None) -> bool\ndef list_stashes(repository_path: str = None) -> List[Dict[str, str]]\ndef reset_changes(mode: str = \"mixed\", target: str = \"HEAD\", repository_path: str = None) -> bool",
      "syntax_valid": false,
      "import_errors": [],
      "api_mismatches": []
    },
    {
      "file_path": "docs/reference/api-complete.md",
      "line_number": 250,
      "language": "python",
      "code": "from codomyrmex.git_operations.git_manager import (\n    check_git_availability,\n    clone_repository,\n    create_branch,\n    add_files,\n    commit_changes,\n    push_changes\n)\n\n# Check if git is available\nif not check_git_availability():\n    print(\"Git is not available\")\n    exit(1)\n\n# Clone a repository\nsuccess = clone_repository(\n    url=\"https://github.com/user/repo.git\",\n    destination=\"./local-repo\",\n    branch=\"main\"\n)\n\nif success:\n    # Create and switch to new branch\n    create_branch(\"feature-branch\", \"./local-repo\")\n    \n    # Add files and commit\n    add_files([\"src/new_file.py\"], \"./local-repo\")\n    commit_changes(\"Add new feature\", \"./local-repo\")\n    \n    # Push changes\n    push_changes(\"origin\", \"feature-branch\", \"./local-repo\")",
      "syntax_valid": true,
      "import_errors": [],
      "api_mismatches": []
    },
    {
      "file_path": "docs/reference/api-complete.md",
      "line_number": 287,
      "language": "python",
      "code": "class Repository:\n    def __init__(self, name: str, url: str, description: str = \"\", \n                 local_path: str = \"\", repository_type: RepositoryType = RepositoryType.UNKNOWN)\n\nclass RepositoryManager:\n    def __init__(self, library_file_path: str, local_base_path: str = \"./repositories\")\n    def list_repositories(self) -> List[Repository]\n    def get_repository(self, name: str) -> Optional[Repository]\n    def search_repositories(self, query: str) -> List[Repository]\n    def clone_repository(self, name: str) -> bool\n    def update_repository(self, name: str) -> bool\n    def get_repository_status(self, name: str) -> Dict[str, Any]",
      "syntax_valid": false,
      "import_errors": [],
      "api_mismatches": []
    },
    {
      "file_path": "docs/reference/api-complete.md",
      "line_number": 307,
      "language": "python",
      "code": "def analyze_repository_path(path_to_analyze: str, relative_output_dir_name: str, \n                           config: dict, module_pbar_desc: str) -> dict\n\ndef run_full_analysis() -> None\n\ndef get_embedding_function(model_name: str = DEFAULT_EMBEDDING_MODEL)",
      "syntax_valid": false,
      "import_errors": [],
      "api_mismatches": []
    },
    {
      "file_path": "docs/reference/api-complete.md",
      "line_number": 317,
      "language": "python",
      "code": "from codomyrmex.pattern_matching.run_codomyrmex_analysis import analyze_repository_path\n\nconfig = {\n    \"repository_indexing\": True,\n    \"dependency_analysis\": True,\n    \"text_search\": True,\n    \"code_summarization\": True,\n    \"docstring_indexing\": True,\n    \"symbol_extraction\": True\n}\n\nresult = analyze_repository_path(\n    path_to_analyze=\"./my-project\",\n    relative_output_dir_name=\"analysis_output\",\n    config=config,\n    module_pbar_desc=\"Analyzing repository\"\n)",
      "syntax_valid": true,
      "import_errors": [],
      "api_mismatches": []
    },
    {
      "file_path": "docs/reference/api-complete.md",
      "line_number": 342,
      "language": "python",
      "code": "def check_build_environment() -> dict\ndef run_build_command(command: List[str], cwd: str = None) -> Tuple[bool, str, str]\ndef synthesize_build_artifact(source_path: str, output_path: str, artifact_type: str = \"executable\") -> bool\ndef validate_build_output(output_path: str) -> Dict[str, any]\ndef orchestrate_build_pipeline(build_config: Dict[str, any]) -> Dict[str, any]",
      "syntax_valid": false,
      "import_errors": [],
      "api_mismatches": []
    },
    {
      "file_path": "docs/reference/api-complete.md",
      "line_number": 351,
      "language": "python",
      "code": "from codomyrmex.build_synthesis.build_orchestrator import (\n    check_build_environment,\n    synthesize_build_artifact,\n    orchestrate_build_pipeline\n)\n\n# Check build environment\nenv_status = check_build_environment()\nprint(f\"Python available: {env_status['python_available']}\")\n\n# Create executable artifact\nsuccess = synthesize_build_artifact(\n    source_path=\"./src\",\n    output_path=\"./dist/my_app\",\n    artifact_type=\"executable\"\n)\n\n# Full build pipeline\nbuild_config = {\n    \"source_path\": \"./src\",\n    \"output_path\": \"./dist\",\n    \"artifact_type\": \"package\",\n    \"dependencies\": [\"numpy\", \"matplotlib\"]\n}\n\nresult = orchestrate_build_pipeline(build_config)",
      "syntax_valid": true,
      "import_errors": [],
      "api_mismatches": []
    },
    {
      "file_path": "docs/reference/api-complete.md",
      "line_number": 385,
      "language": "python",
      "code": "def check_doc_environment() -> dict\ndef install_dependencies(package_manager: str = \"npm\") -> bool\ndef start_dev_server(package_manager: str = \"npm\") -> bool\ndef build_static_site(package_manager: str = \"npm\") -> bool\ndef serve_static_site(package_manager: str = \"npm\") -> bool\ndef aggregate_docs(source_root: str = None, dest_root: str = None) -> bool\ndef validate_doc_versions() -> dict\ndef assess_site() -> dict",
      "syntax_valid": false,
      "import_errors": [],
      "api_mismatches": []
    },
    {
      "file_path": "docs/reference/api-complete.md",
      "line_number": 401,
      "language": "python",
      "code": "def setup_logging(\n    log_level: str = \"INFO\",\n    output_type: str = \"console\",\n    log_file: str = None,\n    detailed: bool = False\n) -> logging.Logger\n\ndef get_logger(name: str) -> logging.Logger",
      "syntax_valid": false,
      "import_errors": [],
      "api_mismatches": []
    },
    {
      "file_path": "docs/reference/api-complete.md",
      "line_number": 413,
      "language": "python",
      "code": "from codomyrmex.logging_monitoring.logger_config import setup_logging, get_logger\n\n# Setup logging system\nsetup_logging(\n    log_level=\"DEBUG\",\n    output_type=\"both\",  # console and file\n    log_file=\"./logs/codomyrmex.log\",\n    detailed=True\n)\n\n# Get module logger\nlogger = get_logger(__name__)\nlogger.info(\"Application started\")",
      "syntax_valid": true,
      "import_errors": [],
      "api_mismatches": []
    },
    {
      "file_path": "docs/reference/api-complete.md",
      "line_number": 434,
      "language": "python",
      "code": "class MCPErrorDetail(BaseModel):\n    code: str\n    message: str\n\nclass MCPToolCall(BaseModel):\n    tool_name: str\n    parameters: dict\n\nclass MCPToolResult(BaseModel):\n    success: bool\n    result: Optional[dict] = None\n    error: Optional[MCPErrorDetail] = None",
      "syntax_valid": true,
      "import_errors": [],
      "api_mismatches": []
    },
    {
      "file_path": "docs/reference/api-complete.md",
      "line_number": 454,
      "language": "python",
      "code": "class InteractiveShell(cmd.Cmd):\n    # Command-line interface implementation\n    pass",
      "syntax_valid": true,
      "import_errors": [],
      "api_mismatches": []
    },
    {
      "file_path": "docs/reference/api-complete.md",
      "line_number": 463,
      "language": "python",
      "code": "class TerminalFormatter:\n    # Terminal output formatting utilities\n    pass\n\nclass CommandRunner:\n    # Command execution utilities\n    pass\n\ndef create_ascii_art(text: str, style: str = \"simple\") -> str",
      "syntax_valid": false,
      "import_errors": [],
      "api_mismatches": []
    },
    {
      "file_path": "docs/reference/api-complete.md",
      "line_number": 480,
      "language": "python",
      "code": "class ModuleCapability:\n    name: str\n    description: str\n    available: bool\n\nclass ModuleInfo:\n    name: str\n    version: str\n    capabilities: List[ModuleCapability]\n\nclass SystemDiscovery:\n    # System introspection and capability discovery\n    pass",
      "syntax_valid": true,
      "import_errors": [],
      "api_mismatches": []
    },
    {
      "file_path": "docs/reference/api-complete.md",
      "line_number": 501,
      "language": "python",
      "code": "# From testing/unit/test_data_visualization.py\ndef test_create_line_plot_real():\n    \"\"\"Test actual line plot creation with real matplotlib.\"\"\"\n    from codomyrmex.data_visualization.line_plot import create_line_plot\n    \n    x_data = [1, 2, 3, 4, 5]\n    y_data = [2, 4, 6, 8, 10]\n    \n    # Test with real data, real function\n    fig = create_line_plot(\n        x_data=x_data,\n        y_data=y_data,\n        title=\"Real Test Plot\",\n        output_path=\"test_output.png\"\n    )\n    \n    assert fig is not None\n    assert Path(\"test_output.png\").exists()",
      "syntax_valid": true,
      "import_errors": [],
      "api_mismatches": []
    },
    {
      "file_path": "docs/reference/api-complete.md",
      "line_number": 523,
      "language": "python",
      "code": "# From testing/unit/test_static_analysis_comprehensive.py\ndef test_parse_pyrefly_real():\n    \"\"\"Test real Pyrefly output parsing.\"\"\"\n    from codomyrmex.static_analysis.pyrefly_runner import parse_pyrefly_output\n    \n    # Real Pyrefly error format\n    output = \"src/file.py:10:5: error: Undefined variable 'x'\"\n    result = parse_pyrefly_output(output, \"/project/root\")\n    \n    assert len(result) == 1\n    assert result[0][\"file_path\"] == \"src/file.py\"\n    assert result[0][\"line_number\"] == 10\n    assert result[0][\"message\"] == \"error: Undefined variable 'x'\"",
      "syntax_valid": true,
      "import_errors": [],
      "api_mismatches": []
    },
    {
      "file_path": "docs/reference/api-complete.md",
      "line_number": 540,
      "language": "python",
      "code": "# From testing/unit/test_code_execution_sandbox_comprehensive.py\ndef test_execute_code_real():\n    \"\"\"Test real code execution.\"\"\"\n    from codomyrmex.code_execution_sandbox.code_executor import execute_code\n    \n    # Real code execution test\n    result = execute_code(\n        code=\"print('Hello from test')\",\n        language=\"python\",\n        timeout=10\n    )\n    \n    assert result['success'] == True\n    assert \"Hello from test\" in result['output']",
      "syntax_valid": true,
      "import_errors": [],
      "api_mismatches": []
    },
    {
      "file_path": "docs/reference/api.md",
      "line_number": 69,
      "language": "python",
      "code": "from codomyrmex.logging_monitoring import setup_logging, get_logger\n\n# Initialize logging (call once at startup)\nsetup_logging()\n\n# Get logger for your module\nlogger = get_logger(__name__)\nlogger.info(\"Application started successfully\")",
      "syntax_valid": true,
      "import_errors": [],
      "api_mismatches": []
    },
    {
      "file_path": "docs/reference/api.md",
      "line_number": 91,
      "language": "python",
      "code": "from codomyrmex.environment_setup import ensure_dependencies_installed, validate_environment\n\n# Ensure all dependencies are available\nensure_dependencies_installed()\n\n# Validate environment configuration\nenv_status = validate_environment()\nprint(f\"Environment: {'\u2705 Good' if env_status['valid'] else '\u274c Issues found'}\")",
      "syntax_valid": true,
      "import_errors": [],
      "api_mismatches": []
    },
    {
      "file_path": "docs/reference/api.md",
      "line_number": 113,
      "language": "python",
      "code": "from codomyrmex.model_context_protocol import MCPToolCall, MCPToolResult\n\n# Create a tool call (as would be sent by an AI)\ntool_call = MCPToolCall(\n    tool_name=\"generate_code_snippet\",\n    parameters={\"prompt\": \"Create a hello world function\", \"language\": \"python\"}\n)\n\n# Execute tool and return result\nresult = MCPToolResult(\n    success=True,\n    result={\"generated_code\": \"def hello(): print('Hello, World!')\"},\n    metadata={\"execution_time\": 0.5}\n)",
      "syntax_valid": true,
      "import_errors": [],
      "api_mismatches": []
    },
    {
      "file_path": "docs/reference/api.md",
      "line_number": 144,
      "language": "python",
      "code": "from codomyrmex.ai_code_editing import generate_code_snippet\n\n# Generate a complete function\nresult = generate_code_snippet(\n    prompt=\"Create a secure user authentication system\",\n    language=\"python\",\n    provider=\"openai\"\n)\n\nif result[\"status\"] == \"success\":\n    print(\"\ud83e\udd16 Generated Code:\")\n    print(result[\"generated_code\"])\n    print(f\"\u23f1\ufe0f Generated in {result['execution_time']:.2f}s\")",
      "syntax_valid": true,
      "import_errors": [],
      "api_mismatches": []
    },
    {
      "file_path": "docs/reference/api.md",
      "line_number": 173,
      "language": "python",
      "code": "from codomyrmex.data_visualization import create_bar_chart\nimport numpy as np\n\n# Create sample data\nlanguages = [\"Python\", \"JavaScript\", \"Java\", \"C++\", \"Go\"]\npopularity = [85, 72, 65, 58, 45]\n\n# Create a beautiful bar chart\ncreate_bar_chart(\n    categories=languages,\n    values=popularity,\n    title=\"Programming Language Popularity (2024)\",\n    x_label=\"Programming Language\",\n    y_label=\"Popularity Score\",\n    output_path=\"language_popularity.png\",\n    color_palette=\"viridis\"\n)",
      "syntax_valid": true,
      "import_errors": [],
      "api_mismatches": []
    },
    {
      "file_path": "docs/reference/api.md",
      "line_number": 204,
      "language": "python",
      "code": "from codomyrmex.static_analysis import run_pyrefly_analysis\n\n# Analyze your Python project\nanalysis = run_pyrefly_analysis(\n    target_paths=[\"src/codomyrmex/\"],\n    project_root=\".\"\n)\n\nprint(f\"\ud83d\udcca Analysis Results:\")\nprint(f\"\ud83d\udcc1 Files analyzed: {analysis.get('files_analyzed', 0)}\")\nprint(f\"\ud83d\udea8 Issues found: {analysis.get('issue_count', 0)}\")\nprint(f\"\u26a1 Performance score: {analysis.get('performance_score', 'N/A')}\")",
      "syntax_valid": true,
      "import_errors": [],
      "api_mismatches": []
    },
    {
      "file_path": "docs/reference/api.md",
      "line_number": 230,
      "language": "python",
      "code": "from codomyrmex.code_execution_sandbox import execute_code\n\n# Execute Python code safely\nresult = execute_code(\n    language=\"python\",\n    code=\"print('Hello from Codomyrmex!')\",\n    timeout=10  # 10 second timeout\n)\n\nprint(f\"\u2705 Success: {result['success']}\")\nprint(f\"\ud83d\udcc4 Output: {result['output']}\")\nprint(f\"\u23f1\ufe0f Execution time: {result['execution_time']:.3f}s\")",
      "syntax_valid": true,
      "import_errors": [],
      "api_mismatches": []
    },
    {
      "file_path": "docs/reference/api.md",
      "line_number": 403,
      "language": "python",
      "code": "# Foundation setup\nfrom codomyrmex.logging_monitoring import setup_logging, get_logger\nfrom codomyrmex.environment_setup import ensure_dependencies_installed\n\n# Initialize environment\nsetup_logging()\nlogger = get_logger(__name__)\nensure_dependencies_installed()\n\n# Use core modules\nfrom codomyrmex.data_visualization import create_line_plot\nfrom codomyrmex.ai_code_editing import generate_code_snippet\n\n# Create visualization\nplot_result = create_line_plot(x_data, y_data, title=\"My Plot\")\n\n# Generate code with AI\ncode_result = generate_code_snippet(\"Create a sorting function\", \"python\")",
      "syntax_valid": true,
      "import_errors": [],
      "api_mismatches": []
    },
    {
      "file_path": "docs/reference/api.md",
      "line_number": 425,
      "language": "python",
      "code": "from codomyrmex.model_context_protocol import MCPToolCall, MCPToolResult\n\n# Create tool call\ntool_call = MCPToolCall(\n    tool_name=\"generate_code_snippet\",\n    parameters={\n        \"prompt\": \"Create a data processing function\",\n        \"language\": \"python\",\n        \"provider\": \"openai\"\n    }\n)\n\n# Execute tool (handled by MCP framework)\nresult = mcp_framework.execute_tool(tool_call)",
      "syntax_valid": true,
      "import_errors": [],
      "api_mismatches": []
    },
    {
      "file_path": "docs/reference/api.md",
      "line_number": 453,
      "language": "python",
      "code": "from codomyrmex.system_discovery import SystemDiscovery\n\ndiscovery = SystemDiscovery()\nmodules = discovery.discover_modules()\n\nfor module_name, module_info in modules.items():\n    print(f\"Module: {module_name}\")\n    print(f\"API Functions: {len(module_info.functions)}\")\n    print(f\"MCP Tools: {len(module_info.mcp_tools)}\")\n    print(f\"Status: {'\u2705' if module_info.is_importable else '\u274c'}\")",
      "syntax_valid": true,
      "import_errors": [],
      "api_mismatches": []
    },
    {
      "file_path": "docs/reference/migration-guide.md",
      "line_number": 52,
      "language": "python",
      "code": "# New APIs (additive, backward compatible)\nfrom codomyrmex.ai_code_editing import enhance_code_batch\nfrom codomyrmex.data_visualization import create_interactive_dashboard\nfrom codomyrmex.static_analysis import analyze_security_vulnerabilities\n\n# New configuration options\nfrom codomyrmex.environment_setup import configure_advanced_caching",
      "syntax_valid": true,
      "import_errors": [],
      "api_mismatches": []
    },
    {
      "file_path": "docs/reference/migration-guide.md",
      "line_number": 160,
      "language": "python",
      "code": "# Before (v0.x.x) - DEPRECATED\nfrom codomyrmex.code.data_visualization import create_plot\nfrom codomyrmex.analysis.static import run_analysis\n\n# After (v1.0.0) - NEW STRUCTURE\nfrom codomyrmex.data_visualization import create_line_plot\nfrom codomyrmex.static_analysis import analyze_codebase\n\n# Function signature changes\n# Before\nresult = run_analysis(path, options={'detailed': True})\n\n# After\nresult = analyze_codebase(path, detailed=True, cache=True)",
      "syntax_valid": true,
      "import_errors": [],
      "api_mismatches": []
    },
    {
      "file_path": "docs/reference/migration-guide.md",
      "line_number": 192,
      "language": "python",
      "code": "#!/usr/bin/env python3\n# migrate_to_v1.py - Automated migration script\n\nimport os\nimport sys\nimport json\nimport shutil\nfrom pathlib import Path\nfrom typing import Dict, List, Tuple\nimport ast\nimport argparse\n\nclass CodomyrmexMigrator:\n    \"\"\"Automated migration tool for Codomyrmex version upgrades.\"\"\"\n\n    def __init__(self, source_version: str, target_version: str):\n        self.source_version = source_version\n        self.target_version = target_version\n        self.migration_log = []\n\n    def migrate_project(self, project_path: Path) -> Dict:\n        \"\"\"Migrate entire project to new version.\"\"\"\n        migration_report = {\n            'source_version': self.source_version,\n            'target_version': self.target_version,\n            'project_path': str(project_path),\n            'changes_made': [],\n            'warnings': [],\n            'manual_steps': []\n        }\n\n        try:\n            # 1. Backup project\n            backup_path = self._create_backup(project_path)\n            migration_report['backup_path'] = str(backup_path)\n\n            # 2. Update imports\n            import_changes = self._migrate_imports(project_path)\n            migration_report['changes_made'].extend(import_changes)\n\n            # 3. Update function calls\n            function_changes = self._migrate_function_calls(project_path)\n            migration_report['changes_made'].extend(function_changes)\n\n            # 4. Update configuration files\n            config_changes = self._migrate_config_files(project_path)\n            migration_report['changes_made'].extend(config_changes)\n\n            # 5. Update requirements\n            req_changes = self._migrate_requirements(project_path)\n            migration_report['changes_made'].extend(req_changes)\n\n            # 6. Generate migration summary\n            self._generate_migration_summary(migration_report)\n\n        except Exception as e:\n            migration_report['error'] = str(e)\n            print(f\"Migration failed: {e}\")\n            # Restore backup\n            self._restore_backup(project_path, backup_path)\n\n        return migration_report\n\n    def _migrate_imports(self, project_path: Path) -> List[Dict]:\n        \"\"\"Update import statements for new version.\"\"\"\n        import_mapping = {\n            # v0.x.x \u2192 v1.0.0 import mappings\n            'from codomyrmex.code.data_visualization import': 'from codomyrmex.data_visualization import',\n            'from codomyrmex.analysis.static import': 'from codomyrmex.static_analysis import',\n            'from codomyrmex.ai.code_editing import': 'from codomyrmex.ai_code_editing import',\n            'from codomyrmex.utils.environment import': 'from codomyrmex.environment_setup import',\n        }\n\n        changes = []\n        for python_file in project_path.rglob(\"*.py\"):\n            try:\n                content = python_file.read_text()\n                original_content = content\n\n                for old_import, new_import in import_mapping.items():\n                    if old_import in content:\n                        content = content.replace(old_import, new_import)\n                        changes.append({\n                            'type': 'import_update',\n                            'file': str(python_file),\n                            'old': old_import,\n                            'new': new_import\n                        })\n\n                if content != original_content:\n                    python_file.write_text(content)\n\n            except Exception as e:\n                changes.append({\n                    'type': 'import_error',\n                    'file': str(python_file),\n                    'error': str(e)\n                })\n\n        return changes\n\n    def _migrate_function_calls(self, project_path: Path) -> List[Dict]:\n        \"\"\"Update function calls for new API signatures.\"\"\"\n        function_mapping = {\n            # Function signature changes\n            'run_analysis(': 'analyze_codebase(',\n            'create_plot(': 'create_line_plot(',\n            'enhance_code_sync(': 'enhance_code(',\n        }\n\n        changes = []\n        for python_file in project_path.rglob(\"*.py\"):\n            try:\n                content = python_file.read_text()\n                original_content = content\n\n                for old_func, new_func in function_mapping.items():\n                    if old_func in content:\n                        # This is a simple replacement - for complex cases, use AST\n                        content = content.replace(old_func, new_func)\n                        changes.append({\n                            'type': 'function_update',\n                            'file': str(python_file),\n                            'old': old_func,\n                            'new': new_func,\n                            'note': 'May require parameter adjustments'\n                        })\n\n                if content != original_content:\n                    python_file.write_text(content)\n\n            except Exception as e:\n                changes.append({\n                    'type': 'function_error',\n                    'file': str(python_file),\n                    'error': str(e)\n                })\n\n        return changes\n\n    def _migrate_config_files(self, project_path: Path) -> List[Dict]:\n        \"\"\"Migrate configuration files to new format.\"\"\"\n        changes = []\n\n        # Find configuration files\n        config_files = [\n            *project_path.rglob(\"codomyrmex.yaml\"),\n            *project_path.rglob(\"codomyrmex.yml\"),\n            *project_path.rglob(\"codomyrmex.json\"),\n            *project_path.rglob(\".codomyrmex\"),\n        ]\n\n        for config_file in config_files:\n            try:\n                if config_file.suffix in ['.yaml', '.yml']:\n                    import yaml\n                    config_data = yaml.safe_load(config_file.read_text())\n                elif config_file.suffix == '.json':\n                    config_data = json.loads(config_file.read_text())\n                else:\n                    # Handle other formats\n                    continue\n\n                # Migrate configuration structure\n                migrated_config = self._migrate_config_structure(config_data)\n\n                # Write migrated config\n                if config_file.suffix in ['.yaml', '.yml']:\n                    config_file.write_text(yaml.dump(migrated_config, default_flow_style=False))\n                else:\n                    config_file.write_text(json.dumps(migrated_config, indent=2))\n\n                changes.append({\n                    'type': 'config_migration',\n                    'file': str(config_file),\n                    'changes': 'Structure updated for v1.0.0'\n                })\n\n            except Exception as e:\n                changes.append({\n                    'type': 'config_error',\n                    'file': str(config_file),\n                    'error': str(e)\n                })\n\n        return changes\n\n    def _migrate_config_structure(self, config: Dict) -> Dict:\n        \"\"\"Migrate configuration structure for new version.\"\"\"\n        migrated = {'codomyrmex': {'version': self.target_version}}\n\n        # Migrate old structure to new\n        if 'analysis' in config:\n            migrated['codomyrmex']['static_analysis'] = config['analysis']\n\n        if 'ai' in config:\n            migrated['codomyrmex']['ai_code_editing'] = config['ai']\n\n        if 'visualization' in config:\n            migrated['codomyrmex']['data_visualization'] = config['visualization']\n\n        # Add new default settings\n        migrated['codomyrmex'].setdefault('logging', {'level': 'INFO'})\n        migrated['codomyrmex'].setdefault('caching', {'enabled': True})\n\n        return migrated\n\n    def _create_backup(self, project_path: Path) -> Path:\n        \"\"\"Create backup of project before migration.\"\"\"\n        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n        backup_path = project_path.parent / f\"{project_path.name}_backup_{timestamp}\"\n\n        shutil.copytree(project_path, backup_path, ignore=shutil.ignore_patterns(\n            '*.pyc', '__pycache__', '.git', 'node_modules', '.venv', 'venv'\n        ))\n\n        print(f\"Created backup at: {backup_path}\")\n        return backup_path\n\n    def _generate_migration_summary(self, report: Dict):\n        \"\"\"Generate migration summary report.\"\"\"\n        summary_file = Path(report['project_path']) / 'MIGRATION_SUMMARY.md'\n\n        summary_content = f\"\"\"# Codomyrmex Migration Summary\n\n**Migration Date**: {datetime.now().isoformat()}\n**From Version**: {report['source_version']}\n**To Version**: {report['target_version']}\n**Project**: {report['project_path']}\n**Backup Location**: {report['backup_path']}\n\n## Changes Made\n\n### Import Updates\n\"\"\"\n        import_changes = [c for c in report['changes_made'] if c['type'] == 'import_update']\n        for change in import_changes:\n            summary_content += f\"- **{change['file']}**: `{change['old']}` \u2192 `{change['new']}`\\n\"\n\n        summary_content += \"\\n### Function Updates\\n\"\n        function_changes = [c for c in report['changes_made'] if c['type'] == 'function_update']\n        for change in function_changes:\n            summary_content += f\"- **{change['file']}**: `{change['old']}` \u2192 `{change['new']}`\\n\"\n            if change.get('note'):\n                summary_content += f\"  - \u26a0\ufe0f {change['note']}\\n\"\n\n        summary_content += f\"\\n## Next Steps\\n\\n\"\n        summary_content += f\"1. Run tests: `codomyrmex test --all`\\n\"\n        summary_content += f\"2. Validate configuration: `codomyrmex validate-config`\\n\"\n        summary_content += f\"3. Check for any remaining issues: `codomyrmex check-compatibility`\\n\"\n        summary_content += f\"4. If issues occur, restore backup from: `{report['backup_path']}`\\n\"\n\n        summary_file.write_text(summary_content)\n        print(f\"Migration summary written to: {summary_file}\")\n\ndef main():\n    parser = argparse.ArgumentParser(description='Migrate Codomyrmex project to new version')\n    parser.add_argument('--project-path', required=True, help='Path to project to migrate')\n    parser.add_argument('--from-version', required=True, help='Source version')\n    parser.add_argument('--to-version', required=True, help='Target version')\n    parser.add_argument('--dry-run', action='store_true', help='Show changes without applying')\n\n    args = parser.parse_args()\n\n    project_path = Path(args.project_path).resolve()\n    if not project_path.exists():\n        print(f\"Project path does not exist: {project_path}\")\n        sys.exit(1)\n\n    migrator = CodomyrmexMigrator(args.from_version, args.to_version)\n\n    if args.dry_run:\n        print(\"DRY RUN MODE - No changes will be made\")\n        # Implement dry run logic here\n    else:\n        report = migrator.migrate_project(project_path)\n\n        if 'error' in report:\n            print(f\"Migration failed: {report['error']}\")\n            sys.exit(1)\n        else:\n            print(\"Migration completed successfully!\")\n            print(f\"Changes made: {len(report['changes_made'])}\")\n            print(f\"See MIGRATION_SUMMARY.md for details\")\n\nif __name__ == '__main__':\n    main()",
      "syntax_valid": true,
      "import_errors": [],
      "api_mismatches": []
    },
    {
      "file_path": "docs/reference/migration-guide.md",
      "line_number": 483,
      "language": "python",
      "code": "# test_migration.py - Migration testing framework\nimport pytest\nimport tempfile\nimport shutil\nfrom pathlib import Path\nfrom codomyrmex_migrator import CodomyrmexMigrator\n\nclass MigrationTestSuite:\n    \"\"\"Test suite for migration validation.\"\"\"\n\n    def setup_test_project(self, version: str) -> Path:\n        \"\"\"Set up test project for specific version.\"\"\"\n        test_dir = Path(tempfile.mkdtemp())\n\n        # Create sample project structure\n        (test_dir / \"src\").mkdir()\n        (test_dir / \"tests\").mkdir()\n\n        # Create sample files for different versions\n        if version.startswith('0.1'):\n            self._create_v01_project(test_dir)\n        elif version.startswith('0.2'):\n            self._create_v02_project(test_dir)\n\n        return test_dir\n\n    def _create_v01_project(self, project_dir: Path):\n        \"\"\"Create v0.1.x project structure.\"\"\"\n        main_py = project_dir / \"src\" / \"main.py\"\n        main_py.write_text(\"\"\"\nfrom codomyrmex.code.data_visualization import create_plot\nfrom codomyrmex.analysis.static import run_analysis\n\ndef main():\n    analysis_result = run_analysis(\"./src\", options={'detailed': True})\n    plot_result = create_plot(analysis_result.metrics, \"quality_metrics.png\")\n\nif __name__ == \"__main__\":\n    main()\n\"\"\")\n\n        config_yaml = project_dir / \"codomyrmex.yaml\"\n        config_yaml.write_text(\"\"\"\nanalysis:\n  enabled: true\n  detailed: true\n\nvisualization:\n  format: png\n  theme: default\n\"\"\")\n\n    def test_migration_0_1_to_1_0(self):\n        \"\"\"Test migration from 0.1.x to 1.0.0.\"\"\"\n        # Setup\n        test_project = self.setup_test_project('0.1.0')\n        migrator = CodomyrmexMigrator('0.1.0', '1.0.0')\n\n        # Execute migration\n        report = migrator.migrate_project(test_project)\n\n        # Verify migration results\n        assert 'error' not in report\n        assert len(report['changes_made']) > 0\n\n        # Check specific changes\n        main_py_content = (test_project / \"src\" / \"main.py\").read_text()\n        assert 'from codomyrmex.data_visualization import' in main_py_content\n        assert 'from codomyrmex.static_analysis import' in main_py_content\n        assert 'analyze_codebase(' in main_py_content\n\n        # Verify backup was created\n        assert Path(report['backup_path']).exists()\n\n        # Cleanup\n        shutil.rmtree(test_project)\n        shutil.rmtree(report['backup_path'])\n\ndef run_migration_tests():\n    \"\"\"Run complete migration test suite.\"\"\"\n    test_suite = MigrationTestSuite()\n    test_suite.test_migration_0_1_to_1_0()\n    print(\"All migration tests passed!\")\n\nif __name__ == '__main__':\n    run_migration_tests()",
      "syntax_valid": true,
      "import_errors": [],
      "api_mismatches": []
    },
    {
      "file_path": "docs/reference/migration-guide.md",
      "line_number": 604,
      "language": "python",
      "code": "# Problem\nModuleNotFoundError: No module named 'codomyrmex.code.data_visualization'\n\n# Solution\n# Old import (v0.x.x)\nfrom codomyrmex.code.data_visualization import create_plot\n\n# New import (v1.0.0)\nfrom codomyrmex.data_visualization import create_line_plot",
      "syntax_valid": false,
      "import_errors": [],
      "api_mismatches": []
    },
    {
      "file_path": "docs/reference/migration-guide.md",
      "line_number": 617,
      "language": "python",
      "code": "# Problem\nTypeError: run_analysis() got an unexpected keyword argument 'options'\n\n# Solution\n# Old call (v0.x.x)\nresult = run_analysis(path, options={'detailed': True, 'cache': False})\n\n# New call (v1.0.0)\nresult = analyze_codebase(path, detailed=True, cache=False)",
      "syntax_valid": false,
      "import_errors": [],
      "api_mismatches": []
    },
    {
      "file_path": "docs/reference/performance.md",
      "line_number": 47,
      "language": "python",
      "code": "# Benchmark: data_visualization performance\nimport time\nimport numpy as np\nfrom codomyrmex.data_visualization import create_line_plot, create_bar_chart\n\ndef benchmark_data_visualization():\n    \"\"\"Performance benchmarks for visualization module (ACTUAL IMPLEMENTATION).\"\"\"\n    from codomyrmex.data_visualization.line_plot import create_line_plot\n    import numpy as np\n    import time\n\n    # Small dataset (1K points) - Interactive tier\n    small_x_data = list(np.linspace(0, 10, 1000))\n    small_y_data = list(np.sin(np.array(small_x_data)))\n\n    start_time = time.time()\n    result = create_line_plot(\n        x_data=small_x_data,\n        y_data=small_y_data,\n        title=\"Small Dataset Performance Test\",\n        output_path=\"perf_test_small.png\"\n    )\n    small_duration = time.time() - start_time\n\n    # Medium dataset (100K points) - Should remain interactive\n    medium_x = np.linspace(0, 100, 100000)\n    medium_y = np.sin(medium_x) * np.cos(medium_x / 10)\n\n    start_time = time.time()\n    result = create_line_plot(medium_x, medium_y, title=\"Medium Dataset\")\n    medium_duration = time.time() - start_time\n\n    # Large dataset (1M points) - Batch processing acceptable\n    large_x = np.linspace(0, 1000, 1000000)\n    large_y = np.sin(large_x) + np.random.normal(0, 0.1, 1000000)\n\n    start_time = time.time()\n    result = create_line_plot(large_x, large_y, title=\"Large Dataset\", optimize_large=True)\n    large_duration = time.time() - start_time\n\n    return {\n        'small_dataset': {'points': 1000, 'duration': small_duration, 'target': '<0.1s'},\n        'medium_dataset': {'points': 100000, 'duration': medium_duration, 'target': '<2s'},\n        'large_dataset': {'points': 1000000, 'duration': large_duration, 'target': '<10s'}\n    }",
      "syntax_valid": true,
      "import_errors": [],
      "api_mismatches": []
    },
    {
      "file_path": "docs/reference/performance.md",
      "line_number": 104,
      "language": "python",
      "code": "# Benchmark: static_analysis performance\nfrom codomyrmex.static_analysis import analyze_codebase, analyze_file\nfrom pathlib import Path\n\ndef benchmark_static_analysis():\n    \"\"\"Performance benchmarks for static analysis (ACTUAL IMPLEMENTATION).\"\"\"\n    from codomyrmex.static_analysis.pyrefly_runner import run_pyrefly_analysis, parse_pyrefly_output\n    import time\n    import tempfile\n    from pathlib import Path\n\n    # Single file analysis\n    with tempfile.TemporaryDirectory() as temp_dir:\n        test_file = Path(temp_dir) / \"sample_module.py\"\n        test_file.write_text(\"def sample_function():\\n    return True\\n\" * 250)  # ~500 lines\n\n        start_time = time.time()\n        # Test with actual function signature\n        result = run_pyrefly_analysis(\n            target_paths=[str(test_file)],\n            project_root=temp_dir\n        )\n        single_duration = time.time() - start_time\n\n    # Small codebase (10-50 files)\n    small_codebase = Path(\"test_data/small_project/\")\n    start_time = time.time()\n    result = analyze_codebase(small_codebase, parallel=True)\n    small_duration = time.time() - start_time\n\n    # Medium codebase (100-500 files)\n    medium_codebase = Path(\"test_data/medium_project/\")\n    start_time = time.time()\n    result = analyze_codebase(medium_codebase, parallel=True, cache=True)\n    medium_duration = time.time() - start_time\n\n    # Large codebase (1000+ files)\n    large_codebase = Path(\"test_data/large_project/\")\n    start_time = time.time()\n    result = analyze_codebase(\n        large_codebase,\n        parallel=True,\n        cache=True,\n        incremental=True\n    )\n    large_duration = time.time() - start_time\n\n    return {\n        'single_file': {'lines': 500, 'duration': single_duration, 'target': '<1s'},\n        'small_codebase': {'files': 25, 'duration': small_duration, 'target': '<5s'},\n        'medium_codebase': {'files': 250, 'duration': medium_duration, 'target': '<30s'},\n        'large_codebase': {'files': 2500, 'duration': large_duration, 'target': '<300s'}\n    }",
      "syntax_valid": true,
      "import_errors": [],
      "api_mismatches": []
    },
    {
      "file_path": "docs/reference/performance.md",
      "line_number": 169,
      "language": "python",
      "code": "# Benchmark: ai_code_editing performance\nfrom codomyrmex.ai_code_editing import enhance_code, generate_code\nimport asyncio\n\nasync def benchmark_ai_code_editing():\n    \"\"\"Performance benchmarks for AI code editing (async).\"\"\"\n\n    # Simple code enhancement\n    simple_code = \"\"\"\ndef add_numbers(a, b):\n    return a + b\n\"\"\"\n\n    start_time = time.time()\n    result = await enhance_code(simple_code, enhancement_type=\"documentation\")\n    simple_duration = time.time() - start_time\n\n    # Complex code enhancement\n    complex_code = open(\"test_data/complex_module.py\").read()  # ~200 lines\n\n    start_time = time.time()\n    result = await enhance_code(\n        complex_code,\n        enhancement_type=\"full_optimization\",\n        include_tests=True\n    )\n    complex_duration = time.time() - start_time\n\n    # Batch code generation\n    specifications = [\n        \"Create a function to sort a list of dictionaries by a key\",\n        \"Implement a binary search algorithm\",\n        \"Create a class for managing database connections\"\n    ]\n\n    start_time = time.time()\n    results = await asyncio.gather(*[\n        generate_code(spec, include_tests=True)\n        for spec in specifications\n    ])\n    batch_duration = time.time() - start_time\n\n    return {\n        'simple_enhancement': {'lines': 3, 'duration': simple_duration, 'target': '<5s'},\n        'complex_enhancement': {'lines': 200, 'duration': complex_duration, 'target': '<30s'},\n        'batch_generation': {'tasks': 3, 'duration': batch_duration, 'target': '<45s'}\n    }",
      "syntax_valid": true,
      "import_errors": [],
      "api_mismatches": []
    },
    {
      "file_path": "docs/reference/performance.md",
      "line_number": 230,
      "language": "python",
      "code": "# High-performance caching for expensive operations\nimport hashlib\nimport pickle\nimport redis\nfrom functools import wraps\nfrom codomyrmex.logging_monitoring import get_logger\n\nlogger = get_logger(__name__)\nredis_client = redis.Redis.from_url(os.getenv('REDIS_URL', 'redis://localhost:6379'))\n\nclass PerformanceCache:\n    \"\"\"High-performance caching with compression and TTL.\"\"\"\n\n    def __init__(self, default_ttl=3600, compression=True):\n        self.default_ttl = default_ttl\n        self.compression = compression\n\n    def cache_key(self, func, args, kwargs):\n        \"\"\"Generate consistent cache key.\"\"\"\n        key_data = f\"{func.__module__}.{func.__name__}:{str(args)}:{str(sorted(kwargs.items()))}\"\n        return f\"perf_cache:{hashlib.sha256(key_data.encode()).hexdigest()}\"\n\n    def get(self, key):\n        \"\"\"Get cached value with decompression.\"\"\"\n        try:\n            cached = redis_client.get(key)\n            if cached:\n                if self.compression:\n                    import zlib\n                    cached = zlib.decompress(cached)\n                return pickle.loads(cached)\n        except Exception as e:\n            logger.warning(f\"Cache get failed for {key}: {e}\")\n        return None\n\n    def set(self, key, value, ttl=None):\n        \"\"\"Set cached value with compression.\"\"\"\n        try:\n            data = pickle.dumps(value)\n            if self.compression:\n                import zlib\n                data = zlib.compress(data, level=6)  # Good compression/speed balance\n\n            redis_client.setex(key, ttl or self.default_ttl, data)\n            logger.debug(f\"Cached {key} ({len(data)} bytes)\")\n        except Exception as e:\n            logger.warning(f\"Cache set failed for {key}: {e}\")\n\n    def cached(self, ttl=None):\n        \"\"\"Decorator for automatic caching.\"\"\"\n        def decorator(func):\n            @wraps(func)\n            def wrapper(*args, **kwargs):\n                cache_key = self.cache_key(func, args, kwargs)\n\n                # Try cache first\n                result = self.get(cache_key)\n                if result is not None:\n                    logger.debug(f\"Cache hit for {func.__name__}\")\n                    return result\n\n                # Execute and cache\n                result = func(*args, **kwargs)\n                self.set(cache_key, result, ttl)\n                logger.debug(f\"Cache miss for {func.__name__}\")\n                return result\n            return wrapper\n        return decorator\n\n# Usage example\ncache = PerformanceCache(default_ttl=1800, compression=True)\n\n@cache.cached(ttl=3600)  # 1 hour cache\ndef expensive_static_analysis(codebase_path):\n    \"\"\"Cache expensive static analysis results.\"\"\"\n    from codomyrmex.static_analysis import analyze_codebase\n    return analyze_codebase(codebase_path)",
      "syntax_valid": true,
      "import_errors": [],
      "api_mismatches": []
    },
    {
      "file_path": "docs/reference/performance.md",
      "line_number": 311,
      "language": "python",
      "code": "# Optimized parallel processing for CPU-bound tasks\nimport multiprocessing as mp\nimport concurrent.futures\nfrom typing import List, Callable, Any\nimport psutil\n\nclass OptimizedProcessor:\n    \"\"\"High-performance parallel processor with dynamic scaling.\"\"\"\n\n    def __init__(self, max_workers=None):\n        # Optimize worker count based on system resources\n        cpu_count = psutil.cpu_count(logical=False)  # Physical cores\n        memory_gb = psutil.virtual_memory().total / (1024**3)\n\n        if max_workers is None:\n            # Conservative: leave 1 core free, account for memory\n            max_workers = min(cpu_count - 1, int(memory_gb / 2))\n            max_workers = max(1, max_workers)  # At least 1 worker\n\n        self.max_workers = max_workers\n        logger.info(f\"Initialized processor with {max_workers} workers\")\n\n    def process_batch(self, func: Callable, items: List[Any],\n                     chunk_size: int = None) -> List[Any]:\n        \"\"\"Process items in parallel with optimized chunking.\"\"\"\n\n        if not items:\n            return []\n\n        # Optimize chunk size based on item count and worker count\n        if chunk_size is None:\n            chunk_size = max(1, len(items) // (self.max_workers * 4))\n\n        # Use ProcessPoolExecutor for CPU-bound tasks\n        with concurrent.futures.ProcessPoolExecutor(\n            max_workers=self.max_workers\n        ) as executor:\n            # Submit work in chunks\n            futures = []\n            for i in range(0, len(items), chunk_size):\n                chunk = items[i:i + chunk_size]\n                future = executor.submit(self._process_chunk, func, chunk)\n                futures.append(future)\n\n            # Collect results maintaining order\n            results = []\n            for future in concurrent.futures.as_completed(futures):\n                try:\n                    chunk_results = future.result(timeout=300)  # 5 min timeout\n                    results.extend(chunk_results)\n                except Exception as e:\n                    logger.error(f\"Chunk processing failed: {e}\")\n                    # Continue with other chunks\n\n            return results\n\n    @staticmethod\n    def _process_chunk(func: Callable, chunk: List[Any]) -> List[Any]:\n        \"\"\"Process a chunk of items.\"\"\"\n        return [func(item) for item in chunk]\n\n# Usage example for static analysis\ndef analyze_files_parallel(file_paths: List[str]) -> dict:\n    \"\"\"Analyze multiple files in parallel.\"\"\"\n    processor = OptimizedProcessor()\n\n    def analyze_single_file(file_path):\n        from codomyrmex.static_analysis import analyze_file\n        try:\n            return {'file': file_path, 'result': analyze_file(file_path)}\n        except Exception as e:\n            return {'file': file_path, 'error': str(e)}\n\n    results = processor.process_batch(analyze_single_file, file_paths)\n    return {r['file']: r.get('result', r.get('error')) for r in results}",
      "syntax_valid": true,
      "import_errors": [],
      "api_mismatches": []
    },
    {
      "file_path": "docs/reference/performance.md",
      "line_number": 390,
      "language": "python",
      "code": "# Memory-efficient processing for large datasets\nimport gc\nfrom contextlib import contextmanager\nfrom memory_profiler import profile\nimport psutil\n\nclass MemoryManager:\n    \"\"\"Memory-aware processing with automatic cleanup.\"\"\"\n\n    def __init__(self, max_memory_percent=80):\n        self.max_memory_percent = max_memory_percent\n        self.total_memory = psutil.virtual_memory().total\n        self.max_memory = self.total_memory * (max_memory_percent / 100)\n\n    @contextmanager\n    def memory_limit(self, description=\"operation\"):\n        \"\"\"Context manager that monitors memory usage.\"\"\"\n        initial_memory = psutil.Process().memory_info().rss\n\n        try:\n            yield\n        finally:\n            # Force garbage collection\n            gc.collect()\n\n            final_memory = psutil.Process().memory_info().rss\n            memory_used = final_memory - initial_memory\n\n            logger.info(f\"Memory used for {description}: {memory_used / 1024**2:.1f} MB\")\n\n            # Warn if approaching limits\n            if final_memory > self.max_memory:\n                logger.warning(f\"Memory usage ({final_memory / 1024**2:.1f} MB) approaching limit\")\n\n    def check_memory_available(self, required_mb=None):\n        \"\"\"Check if sufficient memory is available.\"\"\"\n        available = psutil.virtual_memory().available\n        current_process = psutil.Process().memory_info().rss\n\n        if required_mb:\n            required_bytes = required_mb * 1024**2\n            if available < required_bytes:\n                raise MemoryError(f\"Insufficient memory: need {required_mb} MB, have {available / 1024**2:.1f} MB\")\n\n        return available / 1024**2  # Return available MB\n\n# Memory-efficient data processing\ndef process_large_dataset_efficiently(data_source, chunk_size=10000):\n    \"\"\"Process large datasets in memory-efficient chunks.\"\"\"\n    memory_manager = MemoryManager()\n    results = []\n\n    with memory_manager.memory_limit(\"large dataset processing\"):\n        for chunk_start in range(0, len(data_source), chunk_size):\n            # Process chunk\n            chunk = data_source[chunk_start:chunk_start + chunk_size]\n\n            # Check memory before processing\n            memory_manager.check_memory_available(required_mb=100)\n\n            # Process chunk\n            chunk_result = process_data_chunk(chunk)\n            results.append(chunk_result)\n\n            # Force cleanup after each chunk\n            del chunk\n            gc.collect()\n\n    return combine_results(results)",
      "syntax_valid": true,
      "import_errors": [],
      "api_mismatches": []
    },
    {
      "file_path": "docs/reference/performance.md",
      "line_number": 463,
      "language": "python",
      "code": "# High-performance async I/O for AI API calls\nimport asyncio\nimport aiohttp\nimport time\nfrom typing import List, Dict, Any\nfrom dataclasses import dataclass\n\n@dataclass\nclass APIRequest:\n    \"\"\"Structured API request.\"\"\"\n    endpoint: str\n    data: Dict[Any, Any]\n    timeout: int = 30\n    retries: int = 3\n\nclass AsyncAPIProcessor:\n    \"\"\"High-performance async API processor with rate limiting.\"\"\"\n\n    def __init__(self, rate_limit_per_second=10, concurrent_requests=5):\n        self.rate_limit = rate_limit_per_second\n        self.concurrent_requests = concurrent_requests\n        self.session = None\n        self.semaphore = asyncio.Semaphore(concurrent_requests)\n        self.rate_limiter = asyncio.Semaphore(rate_limit_per_second)\n\n    async def __aenter__(self):\n        \"\"\"Async context manager entry.\"\"\"\n        self.session = aiohttp.ClientSession(\n            timeout=aiohttp.ClientTimeout(total=300),\n            connector=aiohttp.TCPConnector(\n                limit=100,  # Connection pool size\n                limit_per_host=20,\n                keepalive_timeout=60\n            )\n        )\n        return self\n\n    async def __aexit__(self, exc_type, exc_val, exc_tb):\n        \"\"\"Async context manager exit.\"\"\"\n        if self.session:\n            await self.session.close()\n\n    async def process_requests(self, requests: List[APIRequest]) -> List[Dict]:\n        \"\"\"Process multiple API requests concurrently.\"\"\"\n        tasks = [self._process_single_request(req) for req in requests]\n\n        # Process with progress tracking\n        results = []\n        completed = 0\n\n        for coro in asyncio.as_completed(tasks):\n            try:\n                result = await coro\n                results.append(result)\n                completed += 1\n\n                if completed % 10 == 0:  # Progress every 10 requests\n                    logger.info(f\"Completed {completed}/{len(requests)} requests\")\n\n            except Exception as e:\n                logger.error(f\"Request failed: {e}\")\n                results.append({'error': str(e)})\n\n        return results\n\n    async def _process_single_request(self, request: APIRequest) -> Dict:\n        \"\"\"Process single API request with rate limiting and retries.\"\"\"\n\n        async with self.semaphore:  # Limit concurrent requests\n            async with self.rate_limiter:  # Rate limiting\n\n                for attempt in range(request.retries + 1):\n                    try:\n                        async with self.session.post(\n                            request.endpoint,\n                            json=request.data,\n                            timeout=request.timeout\n                        ) as response:\n\n                            if response.status == 200:\n                                return await response.json()\n                            elif response.status == 429:  # Rate limited\n                                wait_time = 2 ** attempt  # Exponential backoff\n                                await asyncio.sleep(wait_time)\n                                continue\n                            else:\n                                response.raise_for_status()\n\n                    except asyncio.TimeoutError:\n                        if attempt < request.retries:\n                            await asyncio.sleep(2 ** attempt)\n                            continue\n                        else:\n                            raise\n\n                # If we get here, all retries failed\n                raise Exception(f\"Failed after {request.retries + 1} attempts\")\n\n# Usage example for AI code enhancement\nasync def batch_code_execution(code_snippets: List[str]) -> List[Dict]:\n    \"\"\"Process multiple code executions efficiently (ACTUAL IMPLEMENTATION).\"\"\"\n    from codomyrmex.code_execution_sandbox.code_executor import execute_code\n    import asyncio\n\n    async def execute_single_code(code: str) -> Dict:\n        \"\"\"Execute single code snippet.\"\"\"\n        # Note: execute_code is not async in actual implementation\n        # This is a demonstration of how to make it async-compatible\n        loop = asyncio.get_event_loop()\n        return await loop.run_in_executor(\n            None,\n            execute_code,\n            code,\n            \"python\",\n            30  # timeout\n        )\n\n    # Process code snippets concurrently\n    tasks = [execute_single_code(code) for code in code_snippets]\n    results = await asyncio.gather(*tasks, return_exceptions=True)\n\n    return [r if not isinstance(r, Exception) else {'error': str(r)} for r in results]",
      "syntax_valid": true,
      "import_errors": [],
      "api_mismatches": []
    },
    {
      "file_path": "docs/reference/performance.md",
      "line_number": 591,
      "language": "python",
      "code": "# performance_monitoring.py - Production performance monitoring\nimport time\nimport psutil\nimport threading\nfrom collections import defaultdict, deque\nfrom dataclasses import dataclass\nfrom typing import Dict, List\nimport json\n\n@dataclass\nclass PerformanceMetric:\n    \"\"\"Performance metric data structure.\"\"\"\n    timestamp: float\n    operation: str\n    duration: float\n    memory_used: int\n    cpu_percent: float\n    success: bool\n    metadata: Dict = None\n\nclass PerformanceMonitor:\n    \"\"\"Real-time performance monitoring with historical data.\"\"\"\n\n    def __init__(self, max_history=1000):\n        self.metrics = defaultdict(lambda: deque(maxlen=max_history))\n        self.active_operations = {}\n        self.lock = threading.Lock()\n\n    def start_operation(self, operation_id: str, operation_name: str):\n        \"\"\"Start monitoring an operation.\"\"\"\n        with self.lock:\n            self.active_operations[operation_id] = {\n                'name': operation_name,\n                'start_time': time.time(),\n                'start_memory': psutil.Process().memory_info().rss,\n                'start_cpu': psutil.cpu_percent()\n            }\n\n    def end_operation(self, operation_id: str, success: bool = True, metadata: Dict = None):\n        \"\"\"End monitoring and record metrics.\"\"\"\n        with self.lock:\n            if operation_id not in self.active_operations:\n                return\n\n            op_data = self.active_operations.pop(operation_id)\n            end_time = time.time()\n            end_memory = psutil.Process().memory_info().rss\n            end_cpu = psutil.cpu_percent()\n\n            metric = PerformanceMetric(\n                timestamp=end_time,\n                operation=op_data['name'],\n                duration=end_time - op_data['start_time'],\n                memory_used=end_memory - op_data['start_memory'],\n                cpu_percent=(op_data['start_cpu'] + end_cpu) / 2,\n                success=success,\n                metadata=metadata or {}\n            )\n\n            self.metrics[op_data['name']].append(metric)\n\n    def get_performance_summary(self, operation: str = None) -> Dict:\n        \"\"\"Get performance summary for operations.\"\"\"\n        summary = {}\n\n        operations = [operation] if operation else self.metrics.keys()\n\n        for op_name in operations:\n            metrics = list(self.metrics[op_name])\n            if not metrics:\n                continue\n\n            durations = [m.duration for m in metrics]\n            memory_usage = [m.memory_used for m in metrics]\n            success_rate = sum(1 for m in metrics if m.success) / len(metrics)\n\n            summary[op_name] = {\n                'count': len(metrics),\n                'avg_duration': sum(durations) / len(durations),\n                'min_duration': min(durations),\n                'max_duration': max(durations),\n                'p95_duration': sorted(durations)[int(len(durations) * 0.95)],\n                'avg_memory': sum(memory_usage) / len(memory_usage),\n                'max_memory': max(memory_usage),\n                'success_rate': success_rate,\n                'recent_failures': [\n                    m.metadata for m in metrics[-10:]\n                    if not m.success and m.metadata\n                ]\n            }\n\n        return summary\n\n    def export_metrics(self, filepath: str):\n        \"\"\"Export metrics to JSON file.\"\"\"\n        data = {}\n        for op_name, metrics in self.metrics.items():\n            data[op_name] = [\n                {\n                    'timestamp': m.timestamp,\n                    'duration': m.duration,\n                    'memory_used': m.memory_used,\n                    'cpu_percent': m.cpu_percent,\n                    'success': m.success,\n                    'metadata': m.metadata\n                }\n                for m in metrics\n            ]\n\n        with open(filepath, 'w') as f:\n            json.dump(data, f, indent=2)\n\n# Global performance monitor\nperformance_monitor = PerformanceMonitor()\n\n# Decorator for automatic performance monitoring\ndef monitor_performance(operation_name: str = None):\n    \"\"\"Decorator to automatically monitor function performance.\"\"\"\n    def decorator(func):\n        @wraps(func)\n        def wrapper(*args, **kwargs):\n            op_name = operation_name or f\"{func.__module__}.{func.__name__}\"\n            op_id = f\"{op_name}_{time.time()}_{threading.get_ident()}\"\n\n            performance_monitor.start_operation(op_id, op_name)\n\n            try:\n                result = func(*args, **kwargs)\n                performance_monitor.end_operation(op_id, success=True)\n                return result\n            except Exception as e:\n                performance_monitor.end_operation(\n                    op_id,\n                    success=False,\n                    metadata={'error': str(e)}\n                )\n                raise\n        return wrapper\n    return decorator\n\n# Usage examples\n@monitor_performance(\"data_visualization.create_plot\")\ndef create_monitored_plot(x, y, title):\n    \"\"\"Create plot with performance monitoring.\"\"\"\n    from codomyrmex.data_visualization import create_line_plot\n    return create_line_plot(x, y, title)\n\n@monitor_performance(\"static_analysis.analyze_file\")\ndef analyze_file_monitored(file_path):\n    \"\"\"Analyze file with performance monitoring.\"\"\"\n    from codomyrmex.static_analysis import analyze_file\n    return analyze_file(file_path)",
      "syntax_valid": true,
      "import_errors": [],
      "api_mismatches": []
    },
    {
      "file_path": "docs/reference/performance.md",
      "line_number": 749,
      "language": "python",
      "code": "# benchmark_suite.py - Comprehensive performance testing\nimport pytest\nimport time\nimport numpy as np\nfrom pathlib import Path\nimport tempfile\nimport shutil\n\nclass PerformanceBenchmark:\n    \"\"\"Base class for performance benchmarks.\"\"\"\n\n    def __init__(self, name: str, target_time: float, max_memory_mb: int = None):\n        self.name = name\n        self.target_time = target_time\n        self.max_memory_mb = max_memory_mb\n        self.results = []\n\n    def run_benchmark(self, iterations: int = 5):\n        \"\"\"Run benchmark multiple times and collect results.\"\"\"\n        for i in range(iterations):\n            start_time = time.time()\n            start_memory = psutil.Process().memory_info().rss\n\n            try:\n                self.execute_benchmark()\n                success = True\n            except Exception as e:\n                logger.error(f\"Benchmark {self.name} failed: {e}\")\n                success = False\n\n            end_time = time.time()\n            end_memory = psutil.Process().memory_info().rss\n\n            self.results.append({\n                'iteration': i + 1,\n                'duration': end_time - start_time,\n                'memory_used': end_memory - start_memory,\n                'success': success\n            })\n\n        return self.analyze_results()\n\n    def execute_benchmark(self):\n        \"\"\"Override this method in subclasses.\"\"\"\n        raise NotImplementedError\n\n    def analyze_results(self):\n        \"\"\"Analyze benchmark results.\"\"\"\n        successful_runs = [r for r in self.results if r['success']]\n\n        if not successful_runs:\n            return {'status': 'failed', 'reason': 'No successful runs'}\n\n        durations = [r['duration'] for r in successful_runs]\n        memory_usage = [r['memory_used'] / 1024**2 for r in successful_runs]  # Convert to MB\n\n        avg_duration = sum(durations) / len(durations)\n        max_duration = max(durations)\n        avg_memory = sum(memory_usage) / len(memory_usage)\n        max_memory = max(memory_usage)\n\n        # Performance evaluation\n        time_ok = avg_duration <= self.target_time\n        memory_ok = self.max_memory_mb is None or max_memory <= self.max_memory_mb\n\n        return {\n            'status': 'passed' if time_ok and memory_ok else 'failed',\n            'avg_duration': avg_duration,\n            'max_duration': max_duration,\n            'target_duration': self.target_time,\n            'avg_memory_mb': avg_memory,\n            'max_memory_mb': max_memory,\n            'memory_limit_mb': self.max_memory_mb,\n            'success_rate': len(successful_runs) / len(self.results),\n            'iterations': len(self.results)\n        }\n\nclass DataVisualizationBenchmark(PerformanceBenchmark):\n    \"\"\"Benchmark for data visualization performance.\"\"\"\n\n    def __init__(self, dataset_size: int):\n        self.dataset_size = dataset_size\n\n        # Set targets based on dataset size\n        if dataset_size <= 1000:\n            target_time, max_memory = 0.1, 20\n        elif dataset_size <= 100000:\n            target_time, max_memory = 2.0, 100\n        else:\n            target_time, max_memory = 10.0, 500\n\n        super().__init__(\n            f\"DataVisualization_{dataset_size}_points\",\n            target_time,\n            max_memory\n        )\n\n    def execute_benchmark(self):\n        \"\"\"Execute data visualization benchmark.\"\"\"\n        from codomyrmex.data_visualization import create_line_plot\n\n        x = np.linspace(0, 10, self.dataset_size)\n        y = np.sin(x) * np.random.random(self.dataset_size)\n\n        with tempfile.TemporaryDirectory() as tmp_dir:\n            output_path = Path(tmp_dir) / f\"benchmark_{self.dataset_size}.png\"\n            result = create_line_plot(x, y, f\"Benchmark {self.dataset_size}\", str(output_path))\n\n            # Verify output\n            assert output_path.exists()\n            assert result.success\n\n# Run all benchmarks\ndef run_performance_benchmarks():\n    \"\"\"Run comprehensive performance benchmark suite.\"\"\"\n    benchmarks = [\n        # Data visualization benchmarks\n        DataVisualizationBenchmark(1000),\n        DataVisualizationBenchmark(100000),\n        DataVisualizationBenchmark(1000000),\n\n        # Add more benchmark classes...\n    ]\n\n    results = {}\n    for benchmark in benchmarks:\n        logger.info(f\"Running benchmark: {benchmark.name}\")\n        result = benchmark.run_benchmark()\n        results[benchmark.name] = result\n\n        status_symbol = \"\u2705\" if result['status'] == 'passed' else \"\u274c\"\n        logger.info(f\"{status_symbol} {benchmark.name}: {result['avg_duration']:.2f}s avg\")\n\n    return results\n\n# Pytest integration\n@pytest.mark.performance\ndef test_performance_benchmarks():\n    \"\"\"Run performance benchmarks as tests.\"\"\"\n    results = run_performance_benchmarks()\n\n    failed_benchmarks = [\n        name for name, result in results.items()\n        if result['status'] == 'failed'\n    ]\n\n    if failed_benchmarks:\n        pytest.fail(f\"Performance benchmarks failed: {failed_benchmarks}\")",
      "syntax_valid": true,
      "import_errors": [],
      "api_mismatches": []
    }
  ]
}