name: Performance Benchmarks

on:
  push:
    branches: [main]
  pull_request:
    branches: [main]
    types: [opened, synchronize]
  schedule:
    # Run benchmarks weekly on Wednesdays at 3 AM UTC
    - cron: '0 3 * * 3'
  workflow_dispatch:
    inputs:
      benchmark_type:
        description: 'Type of benchmark to run'
        required: false
        default: 'all'
        type: choice
        options:
          - 'all'
          - 'unit'
          - 'integration'
          - 'memory'

concurrency:
  group: ${{ github.workflow }}-${{ github.ref }}
  cancel-in-progress: true

env:
  PYTHON_VERSION: "3.11"
  UV_VERSION: "0.5.7"

jobs:
  benchmark-setup:
    name: Benchmark Setup
    runs-on: ubuntu-latest
    outputs:
      benchmark-enabled: ${{ steps.check-benchmarks.outputs.enabled }}
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Check if benchmarks exist
        id: check-benchmarks
        run: |
          if [ -d "benchmarks" ] || find . -name "*benchmark*.py" -o -name "*perf*.py" | head -1 | grep -q .; then
            echo "enabled=true" >> $GITHUB_OUTPUT
            echo "✅ Benchmark files found"
          else
            echo "enabled=false" >> $GITHUB_OUTPUT
            echo "⚠️  No benchmark files found, creating basic structure..."
            
            # Create basic benchmark structure
            mkdir -p benchmarks
            echo "enabled=true" >> $GITHUB_OUTPUT
            echo "✅ Basic benchmark structure created"
          fi

  unit-benchmarks:
    name: Unit Performance Benchmarks
    runs-on: ubuntu-latest
    needs: benchmark-setup
    if: needs.benchmark-setup.outputs.benchmark-enabled == 'true'
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Install UV
        uses: astral-sh/setup-uv@v3
        with:
          version: ${{ env.UV_VERSION }}

      - name: Set up Python
        run: uv python install ${{ env.PYTHON_VERSION }}

      - name: Install dependencies
        run: |
          uv sync --all-extras --dev
          pip install pytest-benchmark memory-profiler psutil

      - name: Create output directory
        run: |
          mkdir -p benchmarks/results
          mkdir -p src/codomyrmex/output

      - name: Run unit benchmarks
        run: |
          echo "🏃‍♂️ Running unit performance benchmarks..."
          
          # Run pytest with benchmark markers if they exist
          if find . -name "*test*.py" -exec grep -l "@pytest.mark.benchmark" {} \; | head -1 | grep -q .; then
            uv run pytest -m benchmark --benchmark-json=benchmarks/results/unit-benchmarks.json -v || true
          else
            echo "No pytest benchmarks found, creating basic performance test"
            mkdir -p testing/benchmarks
            cat > testing/benchmarks/test_basic_benchmarks.py << 'EOF'
import pytest
import time

@pytest.mark.benchmark
def test_basic_performance(benchmark):
    def basic_operation():
        # Basic CPU operation
        total = sum(i ** 2 for i in range(1000))
        return total
    
    result = benchmark(basic_operation)
    assert result > 0

@pytest.mark.benchmark  
def test_memory_allocation(benchmark):
    def memory_operation():
        # Basic memory allocation
        data = [f"item_{i}" for i in range(1000)]
        return len(data)
    
    result = benchmark(memory_operation)
    assert result == 1000
EOF
            uv run pytest testing/benchmarks/ -m benchmark --benchmark-json=benchmarks/results/unit-benchmarks.json -v || true
          fi

      - name: Generate benchmark report
        run: |
          echo "📊 Generating benchmark report..."
          
          echo "# 🏃‍♂️ Unit Performance Benchmark Report" > benchmarks/results/unit-report.md
          echo "" >> benchmarks/results/unit-report.md
          echo "Generated on: $(date)" >> benchmarks/results/unit-report.md
          echo "Platform: $(uname -s) $(uname -r)" >> benchmarks/results/unit-report.md
          echo "Python version: $(python --version)" >> benchmarks/results/unit-report.md
          echo "" >> benchmarks/results/unit-report.md
          
          if [ -f "benchmarks/results/unit-benchmarks.json" ]; then
            echo "## Pytest Benchmark Results" >> benchmarks/results/unit-report.md
            echo "" >> benchmarks/results/unit-report.md
            echo '```json' >> benchmarks/results/unit-report.md
            head -50 benchmarks/results/unit-benchmarks.json >> benchmarks/results/unit-report.md
            echo '```' >> benchmarks/results/unit-report.md
          else
            echo "No benchmark results generated" >> benchmarks/results/unit-report.md
          fi

      - name: Upload unit benchmark results
        uses: actions/upload-artifact@v4
        with:
          name: unit-benchmark-results
          path: benchmarks/results/

  integration-benchmarks:
    name: Integration Performance Benchmarks  
    runs-on: ubuntu-latest
    needs: benchmark-setup
    if: needs.benchmark-setup.outputs.benchmark-enabled == 'true'
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Install UV
        uses: astral-sh/setup-uv@v3
        with:
          version: ${{ env.UV_VERSION }}

      - name: Set up Python
        run: uv python install ${{ env.PYTHON_VERSION }}

      - name: Install dependencies
        run: |
          uv sync --all-extras --dev
          pip install pytest-benchmark memory-profiler psutil

      - name: Create output directory
        run: |
          mkdir -p benchmarks/results
          mkdir -p src/codomyrmex/output

      - name: Run integration benchmarks
        run: |
          echo "🔗 Running integration performance benchmarks..."
          
          # Create basic integration benchmark
          mkdir -p testing/benchmarks
          cat > testing/benchmarks/test_integration_benchmarks.py << 'EOF'
import pytest
import time
import sys
import os

@pytest.mark.benchmark
def test_module_import_performance(benchmark):
    def import_modules():
        # Test import performance
        try:
            sys.path.insert(0, 'src')
            import codomyrmex
            return True
        except ImportError:
            return False
    
    result = benchmark(import_modules)
    # Should complete successfully regardless of import success

@pytest.mark.benchmark
def test_workflow_simulation(benchmark):
    def simulate_workflow():
        # Simulate a typical workflow
        data = {}
        for i in range(100):
            data[f"key_{i}"] = [j for j in range(10)]
        
        # Process data
        processed = {k: sum(v) for k, v in data.items()}
        return len(processed)
    
    result = benchmark(simulate_workflow)
    assert result == 100
EOF
          
          uv run pytest testing/benchmarks/test_integration_benchmarks.py -m benchmark --benchmark-json=benchmarks/results/integration-benchmarks.json -v || true

      - name: Generate integration report
        run: |
          echo "# 🔗 Integration Performance Benchmark Report" > benchmarks/results/integration-report.md
          echo "" >> benchmarks/results/integration-report.md
          echo "Generated on: $(date)" >> benchmarks/results/integration-report.md
          echo "" >> benchmarks/results/integration-report.md
          
          if [ -f "benchmarks/results/integration-benchmarks.json" ]; then
            echo "## Integration Benchmark Results" >> benchmarks/results/integration-report.md
            echo '```json' >> benchmarks/results/integration-report.md
            head -50 benchmarks/results/integration-benchmarks.json >> benchmarks/results/integration-report.md
            echo '```' >> benchmarks/results/integration-report.md
          else
            echo "No integration benchmark results generated" >> benchmarks/results/integration-report.md
          fi

      - name: Upload integration benchmark results
        uses: actions/upload-artifact@v4
        with:
          name: integration-benchmark-results
          path: benchmarks/results/

  memory-benchmarks:
    name: Memory Performance Analysis
    runs-on: ubuntu-latest
    needs: benchmark-setup
    if: needs.benchmark-setup.outputs.benchmark-enabled == 'true'
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Install UV
        uses: astral-sh/setup-uv@v3
        with:
          version: ${{ env.UV_VERSION }}

      - name: Set up Python
        run: uv python install ${{ env.PYTHON_VERSION }}

      - name: Install dependencies
        run: |
          uv sync --all-extras --dev
          pip install memory-profiler psutil

      - name: Run memory benchmarks
        run: |
          echo "🧠 Running memory performance analysis..."
          
          mkdir -p benchmarks/results
          
          # Create basic memory profiling script
          cat > benchmarks/memory_profiler.py << 'EOF'
import psutil
import time
import json

def memory_intensive_task():
    data = []
    for i in range(10000):
        data.append(f"Memory test item {i}" * 5)
    return len(data)

def profile_memory():
    process = psutil.Process()
    initial_memory = process.memory_info().rss / (1024 * 1024)
    
    start_time = time.time()
    result = memory_intensive_task()
    end_time = time.time()
    
    final_memory = process.memory_info().rss / (1024 * 1024)
    
    return {
        'initial_memory_mb': initial_memory,
        'final_memory_mb': final_memory,
        'memory_increase_mb': final_memory - initial_memory,
        'execution_time': end_time - start_time,
        'result_size': result
    }

if __name__ == "__main__":
    results = profile_memory()
    with open('benchmarks/results/memory-profile-results.json', 'w') as f:
        json.dump(results, f, indent=2)
    print("Memory profiling completed")
    print(json.dumps(results, indent=2))
EOF
          
          cd benchmarks && python memory_profiler.py

      - name: Generate memory report
        run: |
          echo "# 🧠 Memory Performance Analysis Report" > benchmarks/results/memory-report.md
          echo "" >> benchmarks/results/memory-report.md
          echo "Generated on: $(date)" >> benchmarks/results/memory-report.md
          echo "" >> benchmarks/results/memory-report.md
          
          if [ -f "benchmarks/results/memory-profile-results.json" ]; then
            echo "## Memory Profiling Results" >> benchmarks/results/memory-report.md
            echo '```json' >> benchmarks/results/memory-report.md
            cat benchmarks/results/memory-profile-results.json >> benchmarks/results/memory-report.md
            echo '```' >> benchmarks/results/memory-report.md
          fi
          
          echo "" >> benchmarks/results/memory-report.md
          echo "## System Memory Information" >> benchmarks/results/memory-report.md
          echo '```' >> benchmarks/results/memory-report.md
          free -h >> benchmarks/results/memory-report.md || echo "Memory info not available"
          echo '```' >> benchmarks/results/memory-report.md

      - name: Upload memory benchmark results
        uses: actions/upload-artifact@v4
        with:
          name: memory-benchmark-results
          path: benchmarks/results/

  benchmark-summary:
    name: Benchmark Summary
    runs-on: ubuntu-latest
    needs: [unit-benchmarks, integration-benchmarks, memory-benchmarks]
    if: always()
    steps:
      - name: Download all benchmark results
        uses: actions/download-artifact@v4
        with:
          path: ./benchmark-artifacts

      - name: Generate comprehensive benchmark report
        run: |
          echo "📊 Generating comprehensive benchmark report..."
          
          echo "# 📊 Comprehensive Performance Benchmark Report" > comprehensive-benchmark-report.md
          echo "" >> comprehensive-benchmark-report.md
          echo "Generated on: $(date)" >> comprehensive-benchmark-report.md
          echo "Workflow run: ${{ github.run_id }}" >> comprehensive-benchmark-report.md
          echo "Repository: ${{ github.repository }}" >> comprehensive-benchmark-report.md
          echo "Commit: ${{ github.sha }}" >> comprehensive-benchmark-report.md
          echo "" >> comprehensive-benchmark-report.md
          
          echo "## Summary" >> comprehensive-benchmark-report.md
          echo "" >> comprehensive-benchmark-report.md
          
          # Check for benchmark results
          if [ -d "benchmark-artifacts" ]; then
            echo "Benchmark artifacts found:" >> comprehensive-benchmark-report.md
            find benchmark-artifacts -type f -name "*.md" -o -name "*.json" | while read file; do
              echo "- $file" >> comprehensive-benchmark-report.md
            done
          else
            echo "No benchmark artifacts found" >> comprehensive-benchmark-report.md
          fi
          
          echo "" >> comprehensive-benchmark-report.md
          echo "## Recommendations" >> comprehensive-benchmark-report.md
          echo "" >> comprehensive-benchmark-report.md
          echo "1. Monitor performance trends over time" >> comprehensive-benchmark-report.md
          echo "2. Set up performance regression alerts" >> comprehensive-benchmark-report.md
          echo "3. Consider optimization for functions showing high resource usage" >> comprehensive-benchmark-report.md

      - name: Upload comprehensive benchmark report
        uses: actions/upload-artifact@v4
        with:
          name: comprehensive-benchmark-report
          path: comprehensive-benchmark-report.md

      - name: Generate benchmark summary
        run: |
          echo "# 🏃‍♂️ Benchmark Workflow Status" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "**Trigger**: ${{ github.event_name }}" >> $GITHUB_STEP_SUMMARY
          echo "**Benchmark Type**: ${{ github.event.inputs.benchmark_type || 'all' }}" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "## Benchmark Results" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          
          if [[ "${{ needs.unit-benchmarks.result }}" == "success" ]]; then
            echo "✅ **Unit Benchmarks**: Completed" >> $GITHUB_STEP_SUMMARY
          else
            echo "❌ **Unit Benchmarks**: Failed or Skipped" >> $GITHUB_STEP_SUMMARY
          fi
          
          if [[ "${{ needs.integration-benchmarks.result }}" == "success" ]]; then
            echo "✅ **Integration Benchmarks**: Completed" >> $GITHUB_STEP_SUMMARY
          else
            echo "❌ **Integration Benchmarks**: Failed or Skipped" >> $GITHUB_STEP_SUMMARY
          fi
          
          if [[ "${{ needs.memory-benchmarks.result }}" == "success" ]]; then
            echo "✅ **Memory Analysis**: Completed" >> $GITHUB_STEP_SUMMARY
          else
            echo "❌ **Memory Analysis**: Failed or Skipped" >> $GITHUB_STEP_SUMMARY
          fi