name: Performance Benchmarks

on:
  push:
    branches: [main]
  pull_request:
    branches: [main]
    types: [opened, synchronize]
  schedule:
    # Run benchmarks weekly on Wednesdays at 3 AM UTC
    - cron: '0 3 * * 3'
  workflow_dispatch:
    inputs:
      benchmark_type:
        description: 'Type of benchmark to run'
        required: false
        default: 'all'
        type: choice
        options:
          - 'all'
          - 'unit'
          - 'integration'
          - 'memory'

concurrency:
  group: ${{ github.workflow }}-${{ github.ref }}
  cancel-in-progress: true

permissions: {}

env:
  PYTHON_VERSION: "3.11"
  UV_VERSION: "0.5.7"

jobs:
  benchmark-setup:
    name: Benchmark Setup
    runs-on: ubuntu-latest
    outputs:
      benchmark-enabled: ${{ steps.check-benchmarks.outputs.enabled }}
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Check if benchmarks exist
        id: check-benchmarks
        run: |
          # Look for real benchmark tests (not auto-generated ones)
          if find . -path "*/tests/*" -name "*benchmark*.py" -not -path "*/.git/*" | head -1 | grep -q . || \
             find . -name "*benchmark*.py" -not -path "*/.git/*" -not -path "*/node_modules/*" | head -1 | grep -q .; then
            echo "enabled=true" >> $GITHUB_OUTPUT
            echo "âœ… Benchmark files found"
          else
            echo "enabled=false" >> $GITHUB_OUTPUT
            echo "â„¹ï¸  No benchmark files found â€” skipping benchmark jobs (add @pytest.mark.benchmark tests to enable)"
          fi

  unit-benchmarks:
    name: Unit Performance Benchmarks
    runs-on: ubuntu-latest
    needs: benchmark-setup
    if: needs.benchmark-setup.outputs.benchmark-enabled == 'true'
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Install UV
        uses: astral-sh/setup-uv@v5
        with:
          version: ${{ env.UV_VERSION }}

      - name: Set up Python
        run: uv python install ${{ env.PYTHON_VERSION }}

      - name: Install dependencies
        run: |
          uv sync --all-extras --dev
          uv pip install pytest-benchmark memory-profiler psutil

      - name: Create output directory
        run: |
          mkdir -p benchmarks/results
          mkdir -p src/codomyrmex/output

      - name: Run unit benchmarks
        run: |
          echo "ðŸƒâ€â™‚ï¸ Running unit performance benchmarks..."
          uv run pytest -m benchmark --benchmark-json=benchmarks/results/unit-benchmarks.json -v || true

      - name: Generate benchmark report
        run: |
          echo "ðŸ“Š Generating benchmark report..."
          
          echo "# ðŸƒâ€â™‚ï¸ Unit Performance Benchmark Report" > benchmarks/results/unit-report.md
          echo "" >> benchmarks/results/unit-report.md
          echo "Generated on: $(date)" >> benchmarks/results/unit-report.md
          echo "Platform: $(uname -s) $(uname -r)" >> benchmarks/results/unit-report.md
          echo "Python version: $(python --version)" >> benchmarks/results/unit-report.md
          echo "" >> benchmarks/results/unit-report.md
          
          if [ -f "benchmarks/results/unit-benchmarks.json" ]; then
            echo "## Pytest Benchmark Results" >> benchmarks/results/unit-report.md
            echo "" >> benchmarks/results/unit-report.md
            echo '```json' >> benchmarks/results/unit-report.md
            head -50 benchmarks/results/unit-benchmarks.json >> benchmarks/results/unit-report.md
            echo '```' >> benchmarks/results/unit-report.md
          else
            echo "No benchmark results generated" >> benchmarks/results/unit-report.md
          fi

      - name: Upload unit benchmark results
        uses: actions/upload-artifact@v4
        with:
          name: unit-benchmark-results
          path: benchmarks/results/

  integration-benchmarks:
    name: Integration Performance Benchmarks  
    runs-on: ubuntu-latest
    needs: benchmark-setup
    if: needs.benchmark-setup.outputs.benchmark-enabled == 'true'
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Install UV
        uses: astral-sh/setup-uv@v5
        with:
          version: ${{ env.UV_VERSION }}

      - name: Set up Python
        run: uv python install ${{ env.PYTHON_VERSION }}

      - name: Install dependencies
        run: |
          uv sync --all-extras --dev
          uv pip install pytest-benchmark memory-profiler psutil

      - name: Create output directory
        run: |
          mkdir -p benchmarks/results
          mkdir -p src/codomyrmex/output

      - name: Run integration benchmarks
        run: |
          echo "ðŸ”— Running integration performance benchmarks..."
          uv run pytest -m "benchmark and integration" --benchmark-json=benchmarks/results/integration-benchmarks.json -v || true

      - name: Generate integration report
        run: |
          echo "# ðŸ”— Integration Performance Benchmark Report" > benchmarks/results/integration-report.md
          echo "" >> benchmarks/results/integration-report.md
          echo "Generated on: $(date)" >> benchmarks/results/integration-report.md
          echo "" >> benchmarks/results/integration-report.md
          
          if [ -f "benchmarks/results/integration-benchmarks.json" ]; then
            echo "## Integration Benchmark Results" >> benchmarks/results/integration-report.md
            echo '```json' >> benchmarks/results/integration-report.md
            head -50 benchmarks/results/integration-benchmarks.json >> benchmarks/results/integration-report.md
            echo '```' >> benchmarks/results/integration-report.md
          else
            echo "No integration benchmark results generated" >> benchmarks/results/integration-report.md
          fi

      - name: Upload integration benchmark results
        uses: actions/upload-artifact@v4
        with:
          name: integration-benchmark-results
          path: benchmarks/results/

  memory-benchmarks:
    name: Memory Performance Analysis
    runs-on: ubuntu-latest
    needs: benchmark-setup
    if: needs.benchmark-setup.outputs.benchmark-enabled == 'true'
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Install UV
        uses: astral-sh/setup-uv@v5
        with:
          version: ${{ env.UV_VERSION }}

      - name: Set up Python
        run: uv python install ${{ env.PYTHON_VERSION }}

      - name: Install dependencies
        run: |
          uv sync --all-extras --dev
          uv pip install memory-profiler psutil

      - name: Run memory benchmarks
        run: |
          echo "ðŸ§  Running memory performance analysis..."
          
          mkdir -p benchmarks/results
          
          # Create basic memory profiling script
          cat > benchmarks/memory_profiler.py << 'EOF'
import psutil
import time
import json

def memory_intensive_task():
    data = []
    for i in range(10000):
        data.append(f"Memory test item {i}" * 5)
    return len(data)

def profile_memory():
    process = psutil.Process()
    initial_memory = process.memory_info().rss / (1024 * 1024)
    
    start_time = time.time()
    result = memory_intensive_task()
    end_time = time.time()
    
    final_memory = process.memory_info().rss / (1024 * 1024)
    
    return {
        'initial_memory_mb': initial_memory,
        'final_memory_mb': final_memory,
        'memory_increase_mb': final_memory - initial_memory,
        'execution_time': end_time - start_time,
        'result_size': result
    }

if __name__ == "__main__":
    results = profile_memory()
    with open('benchmarks/results/memory-profile-results.json', 'w') as f:
        json.dump(results, f, indent=2)
    print("Memory profiling completed")
    print(json.dumps(results, indent=2))
EOF
          
          cd benchmarks && python memory_profiler.py

      - name: Generate memory report
        run: |
          echo "# ðŸ§  Memory Performance Analysis Report" > benchmarks/results/memory-report.md
          echo "" >> benchmarks/results/memory-report.md
          echo "Generated on: $(date)" >> benchmarks/results/memory-report.md
          echo "" >> benchmarks/results/memory-report.md
          
          if [ -f "benchmarks/results/memory-profile-results.json" ]; then
            echo "## Memory Profiling Results" >> benchmarks/results/memory-report.md
            echo '```json' >> benchmarks/results/memory-report.md
            cat benchmarks/results/memory-profile-results.json >> benchmarks/results/memory-report.md
            echo '```' >> benchmarks/results/memory-report.md
          fi
          
          echo "" >> benchmarks/results/memory-report.md
          echo "## System Memory Information" >> benchmarks/results/memory-report.md
          echo '```' >> benchmarks/results/memory-report.md
          free -h >> benchmarks/results/memory-report.md || echo "Memory info not available"
          echo '```' >> benchmarks/results/memory-report.md

      - name: Upload memory benchmark results
        uses: actions/upload-artifact@v4
        with:
          name: memory-benchmark-results
          path: benchmarks/results/

  benchmark-summary:
    name: Benchmark Summary
    runs-on: ubuntu-latest
    needs: [unit-benchmarks, integration-benchmarks, memory-benchmarks]
    if: always()
    steps:
      - name: Download all benchmark results
        uses: actions/download-artifact@v4
        with:
          path: ./benchmark-artifacts

      - name: Generate comprehensive benchmark report
        run: |
          echo "ðŸ“Š Generating comprehensive benchmark report..."
          
          echo "# ðŸ“Š Comprehensive Performance Benchmark Report" > comprehensive-benchmark-report.md
          echo "" >> comprehensive-benchmark-report.md
          echo "Generated on: $(date)" >> comprehensive-benchmark-report.md
          echo "Workflow run: ${{ github.run_id }}" >> comprehensive-benchmark-report.md
          echo "Repository: ${{ github.repository }}" >> comprehensive-benchmark-report.md
          echo "Commit: ${{ github.sha }}" >> comprehensive-benchmark-report.md
          echo "" >> comprehensive-benchmark-report.md
          
          echo "## Summary" >> comprehensive-benchmark-report.md
          echo "" >> comprehensive-benchmark-report.md
          
          # Check for benchmark results
          if [ -d "benchmark-artifacts" ]; then
            echo "Benchmark artifacts found:" >> comprehensive-benchmark-report.md
            find benchmark-artifacts -type f -name "*.md" -o -name "*.json" | while read file; do
              echo "- $file" >> comprehensive-benchmark-report.md
            done
          else
            echo "No benchmark artifacts found" >> comprehensive-benchmark-report.md
          fi
          
          echo "" >> comprehensive-benchmark-report.md
          echo "## Recommendations" >> comprehensive-benchmark-report.md
          echo "" >> comprehensive-benchmark-report.md
          echo "1. Monitor performance trends over time" >> comprehensive-benchmark-report.md
          echo "2. Set up performance regression alerts" >> comprehensive-benchmark-report.md
          echo "3. Consider optimization for functions showing high resource usage" >> comprehensive-benchmark-report.md

      - name: Upload comprehensive benchmark report
        uses: actions/upload-artifact@v4
        with:
          name: comprehensive-benchmark-report
          path: comprehensive-benchmark-report.md

      - name: Generate benchmark summary
        run: |
          echo "# ðŸƒâ€â™‚ï¸ Benchmark Workflow Status" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "**Trigger**: ${{ github.event_name }}" >> $GITHUB_STEP_SUMMARY
          echo "**Benchmark Type**: ${{ github.event.inputs.benchmark_type || 'all' }}" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "## Benchmark Results" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          
          if [[ "${{ needs.unit-benchmarks.result }}" == "success" ]]; then
            echo "âœ… **Unit Benchmarks**: Completed" >> $GITHUB_STEP_SUMMARY
          else
            echo "âŒ **Unit Benchmarks**: Failed or Skipped" >> $GITHUB_STEP_SUMMARY
          fi
          
          if [[ "${{ needs.integration-benchmarks.result }}" == "success" ]]; then
            echo "âœ… **Integration Benchmarks**: Completed" >> $GITHUB_STEP_SUMMARY
          else
            echo "âŒ **Integration Benchmarks**: Failed or Skipped" >> $GITHUB_STEP_SUMMARY
          fi
          
          if [[ "${{ needs.memory-benchmarks.result }}" == "success" ]]; then
            echo "âœ… **Memory Analysis**: Completed" >> $GITHUB_STEP_SUMMARY
          else
            echo "âŒ **Memory Analysis**: Failed or Skipped" >> $GITHUB_STEP_SUMMARY
          fi