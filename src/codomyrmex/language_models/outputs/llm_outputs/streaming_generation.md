# Test Output: streaming_generation

**Generated:** 2025-09-29 08:32:57

**Configuration:** Model: llama3.1:latest, Temperature: 0.7, Max Tokens: 1000

## Test Name

streaming_generation

## Status

passed

## Timestamp

1759159977.099884

## Prompt

Count to 5 slowly.

## Chunks Count

11

## Total Length

36

## Response

One... two... three... four... five.

