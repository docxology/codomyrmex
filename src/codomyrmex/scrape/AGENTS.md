# Codomyrmex Agents â€” src/codomyrmex/scrape

## Signposting
- **Parent**: [codomyrmex](../AGENTS.md)
- **Self**: [Agents](AGENTS.md)
- **Children**:
    - [docs](docs/AGENTS.md)
    - [firecrawl](firecrawl/AGENTS.md)
    - [tests](tests/AGENTS.md)
- **Key Artifacts**:
    - [Functional Spec](SPEC.md)
    - [Human Readme](README.md)

**Version**: v0.1.0 | **Status**: Active | **Last Updated**: January 2026

## Purpose
Web data extraction engine providing a unified interface for scraping web content, crawling websites, mapping site structures, and extracting structured data. Abstracts different scraping providers (e.g., Firecrawl) behind a consistent Pythonic interface with support for multiple formats (markdown, HTML, JSON, screenshots, metadata), batch operations, JavaScript-rendered content, and LLM-powered structured data extraction.

## Active Components
- `CHANGELOG.md` â€“ Project file
- `NO_MOCKS_VERIFICATION.md` â€“ Project file
- `README.md` â€“ Project file
- `SECURITY.md` â€“ Project file
- `SPEC.md` â€“ Project file
- `TESTING.md` â€“ Project file
- `__init__.py` â€“ Module exports and public API
- `config.py` â€“ Configuration management for scraping operations
- `core.py` â€“ Core scraping abstractions and data structures
- `docs/` â€“ Directory containing docs components
- `exceptions.py` â€“ Scraping-specific exceptions
- `firecrawl/` â€“ Directory containing Firecrawl adapter implementation
- `requirements.txt` â€“ Project file
- `scraper.py` â€“ Main scraper class implementing the core scraping interface
- `tests/` â€“ Directory containing tests components

## Key Classes and Functions

### Scraper (`scraper.py`)
- `Scraper(config: Optional[ScrapeConfig] = None, adapter: Optional[BaseScraper] = None)` â€“ Main scraper class providing unified interface
- `scrape(url: str, options: Optional[ScrapeOptions] = None) -> ScrapeResult` â€“ Scrape a single URL
- `crawl(url: str, options: Optional[ScrapeOptions] = None) -> CrawlResult` â€“ Crawl a website starting from a URL
- `map(url: str, search: Optional[str] = None) -> MapResult` â€“ Map the structure of a website
- `search(query: str, options: Optional[ScrapeOptions] = None) -> SearchResult` â€“ Search the web and optionally scrape results
- `extract(urls: List[str], schema: Optional[Dict[str, Any]] = None, prompt: Optional[str] = None) -> ExtractResult` â€“ Extract structured data from URLs using LLM

### BaseScraper (`core.py`)
- `BaseScraper` (ABC) â€“ Abstract base class for scraper implementations
- `scrape(url: str, options: Optional[ScrapeOptions] = None) -> ScrapeResult` â€“ Abstract method for scraping
- `crawl(url: str, options: Optional[ScrapeOptions] = None) -> CrawlResult` â€“ Abstract method for crawling
- `map(url: str, search: Optional[str] = None) -> MapResult` â€“ Abstract method for mapping
- `search(query: str, options: Optional[ScrapeOptions] = None) -> SearchResult` â€“ Abstract method for searching
- `extract(urls: List[str], schema: Optional[Dict[str, Any]] = None, prompt: Optional[str] = None) -> ExtractResult` â€“ Abstract method for extraction

### ScrapeResult (`core.py`)
- `ScrapeResult` (dataclass) â€“ Standard result structure:
  - `url: str` â€“ The URL that was scraped
  - `content: str` â€“ The main content (markdown or HTML)
  - `formats: Dict[str, Any]` â€“ Dictionary mapping format types to their content
  - `metadata: Dict[str, Any]` â€“ Additional metadata about the scraped page
  - `status_code: Optional[int]` â€“ HTTP status code if available
  - `success: bool` â€“ Whether the scrape operation was successful
  - `error: Optional[str]` â€“ Error message if the operation failed
- `get_format(format_type: ScrapeFormat | str) -> Any` â€“ Get content in a specific format
- `has_format(format_type: ScrapeFormat | str) -> bool` â€“ Check if a specific format is available

### ScrapeOptions (`core.py`)
- `ScrapeOptions` (dataclass) â€“ Configuration options:
  - `formats: List[ScrapeFormat | str]` â€“ List of formats to request
  - `timeout: Optional[float]` â€“ Request timeout in seconds
  - `headers: Dict[str, str]` â€“ Custom HTTP headers
  - `wait_for: Optional[str]` â€“ CSS selector or time to wait for
  - `actions: List[Dict[str, Any]]` â€“ Actions to perform before scraping
  - `exclude_tags: List[str]` â€“ HTML tags to exclude
  - `include_tags: List[str]` â€“ HTML tags to include
  - `max_depth: Optional[int]` â€“ Maximum crawl depth
  - `limit: Optional[int]` â€“ Maximum number of pages
  - `follow_links: bool` â€“ Whether to follow links
  - `respect_robots_txt: bool` â€“ Whether to respect robots.txt
- `to_dict() -> Dict[str, Any]` â€“ Convert options to dictionary format

### ScrapeConfig (`config.py`)
- `ScrapeConfig` (dataclass) â€“ Configuration for scraping operations:
  - `api_key: Optional[str]` â€“ API key for the scraping service
  - `base_url: str` â€“ Base URL for the scraping API
  - `default_timeout: float` â€“ Default timeout for requests
  - `default_formats: list[str]` â€“ Default formats to request
  - `max_retries: int` â€“ Maximum number of retry attempts
  - `retry_delay: float` â€“ Delay between retries
  - `rate_limit: Optional[float]` â€“ Rate limit (requests per second)
  - `user_agent: str` â€“ User agent string
  - `respect_robots_txt: bool` â€“ Whether to respect robots.txt
- `from_env() -> ScrapeConfig` (classmethod) â€“ Create configuration from environment variables
- `validate() -> None` â€“ Validate the configuration
- `to_dict() -> Dict[str, Any]` â€“ Convert configuration to dictionary

### ScrapeFormat (`core.py`)
- `ScrapeFormat` (Enum) â€“ Supported output formats: MARKDOWN, HTML, JSON, LINKS, SCREENSHOT, METADATA

### Module Functions (`__init__.py`)
- `get_config() -> ScrapeConfig` â€“ Get the current scraping configuration
- `set_config(config: ScrapeConfig) -> None` â€“ Set the scraping configuration
- `reset_config() -> None` â€“ Reset configuration to defaults

### Exceptions (`exceptions.py`)
- `ScrapeError` â€“ Base exception for scraping operations
- `ScrapeConnectionError` â€“ Raised when connection fails
- `ScrapeTimeoutError` â€“ Raised when operation times out
- `ScrapeValidationError` â€“ Raised when validation fails
- `FirecrawlError` â€“ Raised when Firecrawl-specific errors occur

## Operating Contracts
- Maintain alignment between code, documentation, and configured workflows.
- Ensure Model Context Protocol interfaces remain available for sibling agents.
- Record outcomes in shared telemetry and update TODO queues when necessary.

## Navigation Links
- **Human Documentation**: [README.md](README.md)
- **Functional Specification**: [SPEC.md](SPEC.md)
- **ğŸ“ Parent Directory**: [codomyrmex](../README.md) - Parent directory documentation
- **ğŸ  Project Root**: [README](../../../README.md) - Main project documentation