# Codomyrmex Agents â€” src/codomyrmex/llm/outputs/llm_outputs

**Version**: v0.1.0 | **Status**: Active | **Last Updated**: January 2026

## Purpose

Stores sample LLM-generated outputs demonstrating various generation capabilities and configurations. Contains markdown files showcasing different generation modes, parameters, and use cases.

## Active Components

- `text_generation.md` - Basic text generation samples
- `text_generation_with_options.md` - Generation with custom parameters
- `chat_completion.md` - Conversational chat samples
- `streaming_generation.md` - Streaming output demonstrations
- `manager_generation.md` - Manager-class generation samples
- `parameter_configuration.md` - Parameter effect demonstrations
- `SPEC.md` - Directory specification
- `README.md` - Directory documentation

## Sample Types

### text_generation.md
Basic completion examples:
- Simple prompts and responses
- Default parameter outputs
- Various prompt types (question, instruction, creative)

### text_generation_with_options.md
Parameter variation demonstrations:
- Temperature effects (0.1 vs 0.9)
- Top-p sampling variations
- Max token limits
- Stop sequence usage

### chat_completion.md
Conversational examples:
- Multi-turn conversations
- System prompt effects
- Context window management
- Role-based interactions

### streaming_generation.md
Streaming mode samples:
- Chunk-by-chunk output capture
- Timing information
- Buffer management examples

### manager_generation.md
OllamaManager class examples:
- Direct manager usage patterns
- Configuration integration
- Error handling demonstrations

### parameter_configuration.md
Configuration effects:
- Model-specific parameter tuning
- Optimal settings for different tasks
- Parameter interaction effects

## Operating Contracts

- Samples include prompts and full responses
- Metadata includes model, parameters, timestamp
- Sensitive content redacted
- Samples represent typical output quality
- Version tracked with model version

## Signposting

- **Generated By**: `ollama/model_runner.py`, `fabric/fabric_manager.py`
- **Parent Directory**: [outputs](../README.md) - Parent outputs directory
- **Related Modules**:
  - `prompt_templates/` - Source prompts for samples
  - `performance/` - Performance context for samples
- **Project Root**: [../../../../../README.md](../../../../../README.md) - Main project documentation
