#!/bin/bash
# ğŸœ Codomyrmex Development Workflow Orchestrator
# 
# This comprehensive orchestrator demonstrates a complete development workflow:
# 1. Environment setup and project initialization
# 2. AI-assisted feature development and code generation  
# 3. Code quality analysis and validation
# 4. Secure testing and execution
# 5. Git integration with commit and documentation
# 6. Visualization of development metrics and progress
#
# Prerequisites: API keys recommended, Git repository
# Duration: ~8 minutes
# Modules: ALL - environment_setup + ai_code_editing + static_analysis + code + git_operations + data_visualization + logging_monitoring

set -e  # Exit on any error

# Colors for terminal output
RED='\033[0;31m'
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
BLUE='\033[0;34m'
CYAN='\033[0;36m'
WHITE='\033[1;37m'
NC='\033[0m' # No Color

# Configuration
SCRIPT_DIR="$(cd "$(dirname "${BASH_SOURCE[0]}")" && pwd)"
PROJECT_ROOT="$(cd "$SCRIPT_DIR/../.." && pwd)"
OUTPUT_DIR="$PROJECT_ROOT/scripts/output/development-workflow"
DEMO_START_TIME=$(date +%s)

# Banner
echo -e "${CYAN}â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—${NC}"
echo -e "${CYAN}â•‘    ğŸš€ Codomyrmex Development Workflow Orchestrator ğŸš€   â•‘${NC}"
echo -e "${CYAN}â•‘  Complete AI-Enhanced Development Lifecycle Demo        â•‘${NC}"
echo -e "${CYAN}â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•${NC}\n"

# Helper functions
show_progress() {
    echo -e "${BLUE}[$(date +'%H:%M:%S')] Phase $1: $2${NC}"
}

show_step() {
    echo -e "${WHITE}  Step $1: $2${NC}"
}

show_success() {
    echo -e "${GREEN}  âœ… $1${NC}"
}

show_warning() {
    echo -e "${YELLOW}  âš ï¸  $1${NC}"
}

show_error() {
    echo -e "${RED}  âŒ $1${NC}"
}

show_info() {
    echo -e "${CYAN}  â„¹ï¸  $1${NC}"
}

# Create output directory
mkdir -p "$OUTPUT_DIR"

# Introduction
echo -e "${WHITE}This orchestrator will walk you through a complete development workflow,${NC}"
echo -e "${WHITE}demonstrating how Codomyrmex modules work together in a real scenario.${NC}"
echo ""
echo -e "${CYAN}Workflow Overview:${NC}"
echo "  ğŸ”§ Environment validation and setup"
echo "  ğŸ¤– AI-assisted feature development"
echo "  ğŸ” Code quality analysis and validation"  
echo "  âš¡ Secure testing and execution"
echo "  ğŸ“Š Development metrics visualization"
echo "  ğŸŒ³ Git integration and documentation"
echo ""
read -p "Press Enter to begin the workflow..." -r

cd "$PROJECT_ROOT"

# Phase 1: Environment Setup and Validation
show_progress "1" "Environment Setup and Validation"
show_step "1.1" "Validating development environment..."

# Test all required modules
python3 -c "
import sys
import os
sys.path.insert(0, 'src')

print('ğŸ” Testing module availability...')
modules_status = {}

required_modules = [
    ('environment_setup.env_checker', 'Environment Setup'),
    ('logging_monitoring.logger_config', 'Logging & Monitoring'),
    ('ai_code_editing.ai_code_helpers', 'AI Code Editing'),
    ('static_analysis.pyrefly_runner', 'Static Analysis'),
    ('code.execution.executor', 'Code Execution'),
    ('data_visualization', 'Data Visualization'),
    ('git_operations.git_manager', 'Git Operations')
]

available_modules = []
missing_modules = []

for module_path, module_name in required_modules:
    try:
        __import__(f'codomyrmex.{module_path}')
        available_modules.append(module_name)
        print(f'  âœ… {module_name}')
    except ImportError as e:
        missing_modules.append((module_name, str(e)))
        print(f'  âš ï¸  {module_name}: {e}')

print(f'\\nğŸ“Š Module Status: {len(available_modules)}/{len(required_modules)} available')

if len(available_modules) < 4:
    print('âŒ Insufficient modules available for comprehensive workflow')
    print('Please ensure Codomyrmex is properly installed: pip install -e .')
    exit(1)
else:
    print('âœ… Sufficient modules available for workflow demonstration')
"

if [ $? -ne 0 ]; then
    show_error "Environment validation failed"
    exit 1
fi

show_success "Environment validation completed"

show_step "1.2" "Setting up logging and project workspace..."

# Initialize logging and create workspace
python3 -c "
import sys
sys.path.insert(0, 'src')

from codomyrmex.logging_monitoring import setup_logging, get_logger
from pathlib import Path

# Setup comprehensive logging
setup_logging()
logger = get_logger('workflow_orchestrator')

output_dir = '$OUTPUT_DIR'
Path(output_dir).mkdir(parents=True, exist_ok=True)

logger.info('ğŸš€ Development Workflow Orchestrator Started')
logger.info(f'Output directory: {output_dir}')
logger.info('Phase 1: Environment setup completed successfully')

print('âœ… Logging system initialized')
print(f'ğŸ“ Workspace created: {output_dir}')
"

show_success "Project workspace initialized"

# Phase 2: Project Planning and Feature Selection
show_progress "2" "Project Planning and Feature Selection"
show_step "2.1" "Selecting development project type..."

echo ""
echo "Choose a development project to work on:"
echo "  1. ğŸ§® Math Utilities Library - Create mathematical functions and calculators"
echo "  2. ğŸ“ File Management Tools - Build file organization and search utilities"
echo "  3. ğŸŒ Web API Client - Create a REST API client with data processing"
echo "  4. ğŸ“Š Data Analysis Pipeline - Build data processing and visualization tools"
echo "  5. ğŸ¯ Custom Project - Specify your own project requirements"
echo ""

read -p "Choose project type (1-5): " project_choice

case $project_choice in
    1)
        PROJECT_TYPE="math_utilities"
        PROJECT_DESCRIPTION="A comprehensive mathematical utilities library with functions for basic arithmetic, statistical calculations, geometric computations, and number theory operations. Include proper error handling, unit tests, and documentation."
        PROJECT_NAME="MathUtils Library"
        ;;
    2)
        PROJECT_TYPE="file_management"
        PROJECT_DESCRIPTION="File management tools including file search by various criteria, batch renaming utilities, duplicate file detection, directory organization helpers, and file metadata analysis. Include recursive operations and pattern matching."
        PROJECT_NAME="FileManager Tools"
        ;;
    3)
        PROJECT_TYPE="web_api_client"
        PROJECT_DESCRIPTION="A REST API client library with authentication, request/response handling, data serialization, error handling, rate limiting, and caching. Include examples for popular APIs and comprehensive error management."
        PROJECT_NAME="WebAPI Client"
        ;;
    4)
        PROJECT_TYPE="data_analysis"
        PROJECT_DESCRIPTION="Data analysis pipeline with CSV/JSON file loading, data cleaning and preprocessing, statistical analysis functions, data visualization generation, and report creation. Include support for common data operations."
        PROJECT_NAME="DataAnalyzer Pipeline"
        ;;
    5)
        read -p "Enter your project description: " custom_description
        if [ -z "$custom_description" ]; then
            show_error "Project description cannot be empty"
            exit 1
        fi
        PROJECT_TYPE="custom_project"
        PROJECT_DESCRIPTION="$custom_description"
        PROJECT_NAME="Custom Project"
        ;;
    *)
        show_error "Invalid project choice"
        exit 1
        ;;
esac

show_success "Selected: $PROJECT_NAME"
show_info "Description: $PROJECT_DESCRIPTION"

# Phase 3: AI-Powered Development
show_progress "3" "AI-Powered Development"
show_step "3.1" "Generating project structure and core functionality..."

python3 -c "
import sys
import json
import time
from pathlib import Path
sys.path.insert(0, 'src')

from codomyrmex.logging_monitoring import get_logger
from codomyrmex.agents.ai_code_editing.ai_code_helpers import generate_code_snippet

logger = get_logger('ai_development')
logger.info('ğŸ¤– Starting AI-powered development phase')

project_type = '$PROJECT_TYPE'
project_name = '$PROJECT_NAME'
project_description = '''$PROJECT_DESCRIPTION'''
output_dir = '$OUTPUT_DIR'

# Create project directory
project_dir = Path(output_dir) / 'generated_project'
project_dir.mkdir(exist_ok=True)

logger.info(f'Generating code for project: {project_name}')

try:
    # Generate main module code
    print('ğŸ¯ Generating main project code...')
    
    main_code_prompt = f'''
    Create a Python module for: {project_description}
    
    Requirements:
    - Use proper Python class structure and organization
    - Include comprehensive docstrings and type hints
    - Implement proper error handling and validation
    - Create at least 3-5 main functions/methods
    - Include a main() function for demonstration
    - Add proper imports and dependencies
    - Make the code modular and well-structured
    '''
    
    main_result = generate_code_snippet(
        prompt=main_code_prompt,
        language='python',
        llm_provider='openai'
    )
    
    if main_result['status'] == 'success':
        main_code = main_result['generated_code']
        print('  âœ… Main module code generated')
        
        # Save main module
        with open(project_dir / f'{project_type}.py', 'w') as f:
            f.write(main_code)
        
        logger.info(f'Main module saved: {project_type}.py')
    else:
        print(f'  âŒ Main code generation failed: {main_result.get(\"error\", \"Unknown error\")}')
        main_code = ''
    
    # Generate test code
    print('ğŸ§ª Generating unit tests...')
    
    test_prompt = f'''
    Create comprehensive unit tests for the following Python code:
    
    {main_code[:2000]}...
    
    Requirements:
    - Use pytest framework
    - Create test cases for all main functions/methods
    - Include edge cases and error condition tests
    - Use proper test organization and naming
    - Add test fixtures if appropriate
    - Include both positive and negative test cases
    '''
    
    test_result = generate_code_snippet(
        prompt=test_prompt,
        language='python',
        llm_provider='openai'
    )
    
    if test_result['status'] == 'success':
        test_code = test_result['generated_code']
        print('  âœ… Unit tests generated')
        
        # Save test module
        with open(project_dir / f'test_{project_type}.py', 'w') as f:
            f.write(test_code)
            
        logger.info(f'Test module saved: test_{project_type}.py')
    else:
        print(f'  âš ï¸  Test generation failed: {test_result.get(\"error\", \"Unknown error\")}')
    
    # Generate documentation
    print('ğŸ“š Generating project documentation...')
    
    doc_prompt = f'''
    Create a comprehensive README.md file for this Python project:
    
    Project: {project_name}
    Description: {project_description}
    
    Include:
    - Project title and description
    - Installation instructions
    - Usage examples with code snippets
    - API documentation for main functions
    - Testing instructions
    - Contributing guidelines
    - License information
    
    Make it professional and well-formatted with proper Markdown syntax.
    '''
    
    doc_result = generate_code_snippet(
        prompt=doc_prompt,
        language='markdown',
        llm_provider='openai'
    )
    
    if doc_result['status'] == 'success':
        readme_content = doc_result['generated_code']
        print('  âœ… Documentation generated')
        
        # Save documentation
        with open(project_dir / 'README.md', 'w') as f:
            f.write(readme_content)
            
        logger.info('Documentation saved: README.md')
    else:
        print(f'  âš ï¸  Documentation generation failed: {doc_result.get(\"error\", \"Unknown error\")}')
    
    # Create requirements.txt
    requirements = '''pytest>=7.0.0
requests>=2.28.0
numpy>=1.21.0
matplotlib>=3.5.0'''
    
    with open(project_dir / 'requirements.txt', 'w') as f:
        f.write(requirements)
    print('  âœ… Requirements file created')
    
    logger.info('ğŸ‰ AI development phase completed successfully')
    print(f'\\nğŸ“ Generated project files in: {project_dir}')
    
    # List generated files
    generated_files = list(project_dir.glob('*'))
    print(f'ğŸ“‹ Files created: {len(generated_files)}')
    for file in generated_files:
        print(f'  â€¢ {file.name}')

except Exception as e:
    logger.error(f'AI development failed: {e}')
    import traceback
    logger.error(traceback.format_exc())
    print(f'âŒ AI development failed: {e}')
    exit(1)
"

if [ $? -ne 0 ]; then
    show_error "AI development phase failed"
    exit 1
fi

show_success "AI-powered development completed"

# Phase 4: Code Quality Analysis
show_progress "4" "Code Quality Analysis and Validation"
show_step "4.1" "Running static analysis on generated code..."

python3 -c "
import sys
import json
import subprocess
from pathlib import Path
sys.path.insert(0, 'src')

from codomyrmex.logging_monitoring import get_logger
from codomyrmex.data_visualization import create_bar_chart, create_line_plot

logger = get_logger('quality_analysis')
logger.info('ğŸ” Starting code quality analysis')

output_dir = '$OUTPUT_DIR'
project_dir = Path(output_dir) / 'generated_project'

# Quality analysis results
quality_results = {
    'files_analyzed': [],
    'analysis_tools': {},
    'metrics': {},
    'issues_found': 0,
    'recommendations': []
}

try:
    # Find Python files to analyze
    python_files = list(project_dir.glob('*.py'))
    quality_results['files_analyzed'] = [str(f.name) for f in python_files]
    
    print(f'ğŸ“Š Analyzing {len(python_files)} Python files...')
    logger.info(f'Files to analyze: {quality_results[\"files_analyzed\"]}')
    
    if not python_files:
        print('âš ï¸  No Python files found to analyze')
        exit(0)
    
    # Basic file metrics
    total_lines = 0
    total_chars = 0
    function_count = 0
    class_count = 0
    
    for py_file in python_files:
        try:
            with open(py_file, 'r', encoding='utf-8') as f:
                content = f.read()
                lines = content.split('\\n')
                total_lines += len(lines)
                total_chars += len(content)
                
                # Simple pattern counting
                function_count += content.count('def ')
                class_count += content.count('class ')
                
        except Exception as e:
            logger.warning(f'Could not read {py_file}: {e}')
    
    quality_results['metrics'] = {
        'total_lines': total_lines,
        'total_characters': total_chars,
        'function_count': function_count,
        'class_count': class_count,
        'avg_file_size': total_lines / len(python_files) if python_files else 0
    }
    
    print(f'  ğŸ“ Total lines: {total_lines}')
    print(f'  ğŸ”§ Functions: {function_count}')
    print(f'  ğŸ—ï¸  Classes: {class_count}')
    print(f'  ğŸ“Š Avg file size: {quality_results[\"metrics\"][\"avg_file_size\"]:.1f} lines')
    
    # Run analysis tools
    analysis_results = {}
    
    # Try flake8
    try:
        print('  ğŸ” Running flake8 analysis...')
        result = subprocess.run(
            ['flake8', '--max-line-length=100', '--ignore=E501,W503'] + [str(f) for f in python_files],
            capture_output=True, text=True, timeout=30, cwd=str(project_dir)
        )
        
        flake8_issues = result.stdout.strip().split('\\n') if result.stdout.strip() else []
        flake8_issues = [issue for issue in flake8_issues if issue]
        
        analysis_results['flake8'] = {
            'issues_count': len(flake8_issues),
            'issues': flake8_issues[:10],  # First 10 issues
            'status': 'success'
        }
        
        print(f'    âœ… Flake8: {len(flake8_issues)} issues found')
        quality_results['issues_found'] += len(flake8_issues)
        
    except subprocess.TimeoutExpired:
        print('    âš ï¸  Flake8: Analysis timed out')
        analysis_results['flake8'] = {'status': 'timeout'}
    except FileNotFoundError:
        print('    âš ï¸  Flake8: Not available')
        analysis_results['flake8'] = {'status': 'not_found'}
    except Exception as e:
        print(f'    âš ï¸  Flake8: {e}')
        analysis_results['flake8'] = {'status': 'error', 'error': str(e)}
    
    # Try basic Python syntax check
    try:
        print('  ğŸ Running Python syntax validation...')
        syntax_issues = 0
        
        for py_file in python_files:
            try:
                with open(py_file, 'r') as f:
                    compile(f.read(), py_file, 'exec')
            except SyntaxError as e:
                syntax_issues += 1
                logger.warning(f'Syntax error in {py_file}: {e}')
        
        analysis_results['syntax_check'] = {
            'issues_count': syntax_issues,
            'status': 'success'
        }
        
        if syntax_issues == 0:
            print('    âœ… Syntax: All files valid')
        else:
            print(f'    âŒ Syntax: {syntax_issues} files with issues')
            quality_results['issues_found'] += syntax_issues
        
    except Exception as e:
        print(f'    âš ï¸  Syntax check failed: {e}')
        analysis_results['syntax_check'] = {'status': 'error', 'error': str(e)}
    
    quality_results['analysis_tools'] = analysis_results
    
    # Generate quality visualizations
    print('  ğŸ“Š Generating quality metrics visualizations...')
    
    # Code metrics chart
    if quality_results['metrics']:
        metrics = quality_results['metrics']
        create_bar_chart(
            categories=['Lines', 'Functions', 'Classes'],
            values=[metrics['total_lines'], metrics['function_count'], metrics['class_count']],
            title='Generated Code Metrics',
            x_label='Code Elements',
            y_label='Count',
            output_path=f'{output_dir}/code_metrics.png',
            bar_color='lightblue'
        )
        print('    âœ… Code metrics chart generated')
    
    # Analysis results chart
    tool_names = []
    issue_counts = []
    
    for tool, results in analysis_results.items():
        if results.get('status') == 'success':
            tool_names.append(tool.replace('_', ' ').title())
            issue_counts.append(results.get('issues_count', 0))
    
    if tool_names:
        create_bar_chart(
            categories=tool_names,
            values=issue_counts,
            title='Code Quality Analysis Results',
            x_label='Analysis Tools',
            y_label='Issues Found',
            output_path=f'{output_dir}/quality_analysis.png',
            bar_color='lightcoral'
        )
        print('    âœ… Quality analysis chart generated')
    
    # Save quality report
    with open(f'{output_dir}/quality_report.json', 'w') as f:
        json.dump(quality_results, f, indent=2)
    
    # Determine overall quality score
    if quality_results['issues_found'] == 0:
        quality_score = 'Excellent'
        quality_emoji = 'ğŸŸ¢'
    elif quality_results['issues_found'] <= 5:
        quality_score = 'Good'
        quality_emoji = 'ğŸŸ¡'
    elif quality_results['issues_found'] <= 15:
        quality_score = 'Fair'
        quality_emoji = 'ğŸŸ '
    else:
        quality_score = 'Needs Improvement'
        quality_emoji = 'ğŸ”´'
    
    print(f'\\n{quality_emoji} Overall Quality: {quality_score} ({quality_results[\"issues_found\"]} issues)')
    logger.info(f'Code quality analysis completed: {quality_score}')

except Exception as e:
    logger.error(f'Quality analysis failed: {e}')
    import traceback
    logger.error(traceback.format_exc())
    print(f'âŒ Quality analysis failed: {e}')
    exit(1)
"

if [ $? -ne 0 ]; then
    show_warning "Quality analysis encountered issues but continuing..."
fi

show_success "Code quality analysis completed"

# Phase 5: Code Execution and Testing
show_progress "5" "Code Execution and Testing"
show_step "5.1" "Testing generated code in secure sandbox..."

python3 -c "
import sys
from pathlib import Path
sys.path.insert(0, 'src')

from codomyrmex.logging_monitoring import get_logger
from codomyrmex.code.execution.executor import execute_code

logger = get_logger('code_execution')
logger.info('âš¡ Starting code execution and testing phase')

output_dir = '$OUTPUT_DIR'
project_dir = Path(output_dir) / 'generated_project'
project_type = '$PROJECT_TYPE'

execution_results = []

try:
    # Find main Python file
    main_file = project_dir / f'{project_type}.py'
    
    if main_file.exists():
        print(f'ğŸš€ Testing main module: {main_file.name}')
        
        # Read the main file
        with open(main_file, 'r') as f:
            main_code = f.read()
        
        # Execute main module
        print('  âš¡ Running main module in sandbox...')
        
        result = execute_code(
            language='python',
            code=main_code,
            timeout=30
        )
        
        execution_results.append({
            'file': main_file.name,
            'result': result
        })
        
        print(f'    Exit code: {result.get(\"exit_code\", \"unknown\")}')
        
        if result.get('exit_code') == 0:
            print('    âœ… Execution successful')
            
            if result.get('stdout'):
                print('    ğŸ“‹ Output:')
                output_lines = result['stdout'].split('\\n')[:10]  # First 10 lines
                for line in output_lines:
                    if line.strip():
                        print(f'      {line}')
                if len(result['stdout'].split('\\n')) > 10:
                    print('      ... (truncated)')
        else:
            print('    âŒ Execution failed')
            if result.get('stderr'):
                print(f'    Error: {result[\"stderr\"][:200]}...')
    else:
        print(f'âš ï¸  Main file not found: {main_file.name}')
    
    # Try to run tests if available
    test_file = project_dir / f'test_{project_type}.py'
    
    if test_file.exists():
        print(f'ğŸ§ª Running unit tests: {test_file.name}')
        
        with open(test_file, 'r') as f:
            test_code = f.read()
        
        # Add imports and basic test runner
        test_runner_code = f'''
{test_code}

# Basic test runner
if __name__ == '__main__':
    import inspect
    
    # Find test functions
    current_module = sys.modules[__name__]
    test_functions = [name for name, obj in inspect.getmembers(current_module)
                     if inspect.isfunction(obj) and name.startswith('test_')]
    
    print(f\"Running {{len(test_functions)}} tests...\")
    
    passed = 0
    failed = 0
    
    for test_func_name in test_functions:
        try:
            test_func = getattr(current_module, test_func_name)
            test_func()
            print(f\"âœ… {{test_func_name}}: PASSED\")
            passed += 1
        except Exception as e:
            print(f\"âŒ {{test_func_name}}: FAILED - {{str(e)[:100]}}\")
            failed += 1
    
    print(f\"\\nTest Results: {{passed}} passed, {{failed}} failed\")
'''
        
        print('  ğŸ§ª Executing tests in sandbox...')
        
        test_result = execute_code(
            language='python',
            code=test_runner_code,
            timeout=45
        )
        
        execution_results.append({
            'file': test_file.name,
            'result': test_result
        })
        
        print(f'    Exit code: {test_result.get(\"exit_code\", \"unknown\")}')
        
        if test_result.get('exit_code') == 0:
            print('    âœ… Tests executed successfully')
            
            if test_result.get('stdout'):
                print('    ğŸ“‹ Test output:')
                test_lines = test_result['stdout'].split('\\n')[-10:]  # Last 10 lines
                for line in test_lines:
                    if line.strip():
                        print(f'      {line}')
        else:
            print('    âš ï¸  Test execution had issues')
            if test_result.get('stderr'):
                print(f'    Error: {test_result[\"stderr\"][:200]}...')
    else:
        print(f'âš ï¸  Test file not found: {test_file.name}')
    
    # Save execution results
    import json
    with open(f'{output_dir}/execution_results.json', 'w') as f:
        json.dump(execution_results, f, indent=2)
    
    logger.info('âœ… Code execution phase completed')
    print(f'\\nğŸ“Š Execution Summary: {len(execution_results)} files tested')

except Exception as e:
    logger.error(f'Code execution failed: {e}')
    import traceback
    logger.error(traceback.format_exc())
    print(f'âŒ Code execution failed: {e}')
"

show_success "Code execution and testing completed"

# Phase 6: Development Metrics and Visualization
show_progress "6" "Development Metrics and Visualization"
show_step "6.1" "Generating development dashboard..."

python3 -c "
import sys
import json
import time
from pathlib import Path
sys.path.insert(0, 'src')

from codomyrmex.logging_monitoring import get_logger
from codomyrmex.data_visualization import create_bar_chart, create_line_plot, create_pie_chart

logger = get_logger('development_metrics')
logger.info('ğŸ“Š Generating development metrics dashboard')

output_dir = '$OUTPUT_DIR'
project_type = '$PROJECT_TYPE'
project_name = '$PROJECT_NAME'

# Collect metrics from all previous phases
metrics_data = {
    'project_info': {
        'name': project_name,
        'type': project_type,
        'generated_at': time.strftime('%Y-%m-%d %H:%M:%S')
    },
    'development_metrics': {},
    'quality_metrics': {},
    'execution_metrics': {}
}

try:
    # Load quality results if available
    quality_file = Path(output_dir) / 'quality_report.json'
    if quality_file.exists():
        with open(quality_file, 'r') as f:
            quality_data = json.load(f)
            metrics_data['quality_metrics'] = quality_data['metrics']
    
    # Load execution results if available
    execution_file = Path(output_dir) / 'execution_results.json'
    execution_success_rate = 0
    if execution_file.exists():
        with open(execution_file, 'r') as f:
            execution_data = json.load(f)
            successful_executions = sum(1 for result in execution_data 
                                      if result['result'].get('exit_code') == 0)
            execution_success_rate = (successful_executions / len(execution_data) * 100) if execution_data else 0
            metrics_data['execution_metrics'] = {
                'total_files_tested': len(execution_data),
                'successful_executions': successful_executions,
                'success_rate': execution_success_rate
            }
    
    # Count generated files
    project_dir = Path(output_dir) / 'generated_project'
    generated_files = list(project_dir.glob('*')) if project_dir.exists() else []
    
    metrics_data['development_metrics'] = {
        'files_generated': len(generated_files),
        'file_types': {},
        'workflow_duration': time.time() - $DEMO_START_TIME
    }
    
    # Analyze file types
    file_types = {}
    for file in generated_files:
        ext = file.suffix or 'no_extension'
        file_types[ext] = file_types.get(ext, 0) + 1
    
    metrics_data['development_metrics']['file_types'] = file_types
    
    print('ğŸ“Š Creating development metrics visualizations...')
    
    # Development Overview Chart
    if metrics_data['development_metrics']:
        dev_metrics = metrics_data['development_metrics']
        create_bar_chart(
            categories=['Files Generated', 'Workflow Time (min)'],
            values=[dev_metrics['files_generated'], int(dev_metrics['workflow_duration'] / 60)],
            title='Development Overview',
            x_label='Metrics',
            y_label='Count/Time',
            output_path=f'{output_dir}/development_overview.png',
            bar_color='lightgreen'
        )
        print('  âœ… Development overview chart created')
    
    # File Types Distribution
    if file_types:
        file_extensions = list(file_types.keys())
        file_counts = list(file_types.values())
        
        create_pie_chart(
            labels=[ext.replace('.', '') or 'no_ext' for ext in file_extensions],
            sizes=file_counts,
            title='Generated Files by Type',
            output_path=f'{output_dir}/file_types_distribution.png'
        )
        print('  âœ… File types distribution chart created')
    
    # Quality vs Execution Success
    if metrics_data['quality_metrics'] and metrics_data['execution_metrics']:
        quality = metrics_data['quality_metrics']
        execution = metrics_data['execution_metrics']
        
        create_bar_chart(
            categories=['Lines of Code', 'Functions', 'Classes', 'Success Rate (%)'],
            values=[
                quality.get('total_lines', 0),
                quality.get('function_count', 0), 
                quality.get('class_count', 0),
                int(execution.get('success_rate', 0))
            ],
            title='Code Quality and Execution Metrics',
            x_label='Metrics',
            y_label='Count/Percentage',
            output_path=f'{output_dir}/quality_execution_metrics.png',
            bar_color='gold'
        )
        print('  âœ… Quality vs execution metrics chart created')
    
    # Workflow Timeline (simulated)
    timeline_phases = ['Setup', 'AI Generation', 'Quality Check', 'Testing', 'Visualization']
    timeline_duration = [30, 120, 60, 90, 45]  # Simulated durations in seconds
    
    create_line_plot(
        x_data=list(range(len(timeline_phases))),
        y_data=timeline_duration,
        title='Development Workflow Timeline',
        x_label='Workflow Phase',
        y_label='Duration (seconds)',
        output_path=f'{output_dir}/workflow_timeline.png'
    )
    print('  âœ… Workflow timeline chart created')
    
    # Generate comprehensive dashboard report
    with open(f'{output_dir}/development_dashboard.md', 'w') as f:
        f.write(f'# Development Metrics Dashboard\\n\\n')
        f.write(f'**Project**: {project_name}\\n')
        f.write(f'**Type**: {project_type}\\n')
        f.write(f'**Generated**: {metrics_data[\"project_info\"][\"generated_at\"]}\\n\\n')
        
        f.write('## ğŸ“Š Development Summary\\n\\n')
        if metrics_data['development_metrics']:
            dev = metrics_data['development_metrics']
            f.write(f'- **Files Generated**: {dev[\"files_generated\"]}\\n')
            f.write(f'- **Workflow Duration**: {dev[\"workflow_duration\"] / 60:.1f} minutes\\n')
            f.write(f'- **File Types**: {\", \".join(file_types.keys())}\\n\\n')
        
        f.write('## ğŸ” Code Quality\\n\\n')
        if metrics_data['quality_metrics']:
            quality = metrics_data['quality_metrics']
            f.write(f'- **Total Lines**: {quality.get(\"total_lines\", 0)}\\n')
            f.write(f'- **Functions**: {quality.get(\"function_count\", 0)}\\n')
            f.write(f'- **Classes**: {quality.get(\"class_count\", 0)}\\n')
            f.write(f'- **Average File Size**: {quality.get(\"avg_file_size\", 0):.1f} lines\\n\\n')
        
        f.write('## âš¡ Execution Results\\n\\n')
        if metrics_data['execution_metrics']:
            execution = metrics_data['execution_metrics']
            f.write(f'- **Files Tested**: {execution[\"total_files_tested\"]}\\n')
            f.write(f'- **Successful Executions**: {execution[\"successful_executions\"]}\\n')
            f.write(f'- **Success Rate**: {execution[\"success_rate\"]:.1f}%\\n\\n')
        
        f.write('## ğŸ“ˆ Generated Visualizations\\n\\n')
        f.write('- `development_overview.png` - Overall development metrics\\n')
        f.write('- `file_types_distribution.png` - File types breakdown\\n')
        f.write('- `quality_execution_metrics.png` - Quality and execution analysis\\n')
        f.write('- `workflow_timeline.png` - Development workflow timeline\\n')
        f.write('- `code_metrics.png` - Generated code analysis\\n')
        f.write('- `quality_analysis.png` - Quality analysis results\\n')
    
    # Save complete metrics
    with open(f'{output_dir}/development_metrics.json', 'w') as f:
        json.dump(metrics_data, f, indent=2)
    
    logger.info('ğŸ“Š Development metrics dashboard completed')
    print(f'\\nğŸ¨ Generated 6 visualization charts')
    print(f'ğŸ“„ Created development dashboard report')

except Exception as e:
    logger.error(f'Metrics visualization failed: {e}')
    import traceback
    logger.error(traceback.format_exc())
    print(f'âŒ Metrics visualization failed: {e}')
"

show_success "Development metrics dashboard generated"

# Phase 7: Git Integration and Documentation
show_progress "7" "Git Integration and Documentation"
show_step "7.1" "Git integration and project documentation..."

# Check if we're in a git repository
if git rev-parse --git-dir > /dev/null 2>&1; then
    show_info "In Git repository - creating development branch"
    
    # Create a feature branch for the workflow
    BRANCH_NAME="feature/workflow-demo-$(date +%Y%m%d-%H%M%S)"
    git checkout -b "$BRANCH_NAME" 2>/dev/null || show_warning "Could not create branch $BRANCH_NAME"
    
    # Add generated files to git (if user wants)
    echo ""
    read -p "Add generated project files to Git? (y/n): " add_to_git
    
    if [[ "$add_to_git" =~ ^[Yy]$ ]]; then
        git add "$OUTPUT_DIR/" 2>/dev/null || show_warning "Could not add files to git"
        
        git commit -m "feat: Add AI-generated development workflow results

Generated project: $PROJECT_NAME
- AI-powered code generation
- Quality analysis and validation  
- Secure execution testing
- Comprehensive metrics dashboard
- Complete development workflow demonstration

Files generated:
- Main project code and tests
- Quality analysis reports
- Execution results
- Development metrics visualizations
- Comprehensive documentation" 2>/dev/null || show_warning "Could not create commit"

        show_success "Changes committed to branch $BRANCH_NAME"
    else
        show_info "Skipped Git integration"
    fi
else
    show_warning "Not in Git repository - skipping Git integration"
fi

show_step "7.2" "Final documentation and summary generation..."

python3 -c "
import sys
from pathlib import Path
import time
sys.path.insert(0, 'src')

from codomyrmex.logging_monitoring import get_logger

logger = get_logger('final_documentation')
logger.info('ğŸ“š Generating final workflow documentation')

output_dir = '$OUTPUT_DIR'
project_name = '$PROJECT_NAME'
project_type = '$PROJECT_TYPE'
workflow_start = $DEMO_START_TIME
total_duration = time.time() - workflow_start

# Create comprehensive workflow report
with open(f'{output_dir}/WORKFLOW_COMPLETE.md', 'w') as f:
    f.write(f'# ğŸš€ Codomyrmex Development Workflow - COMPLETE\\n\\n')
    f.write(f'**Project**: {project_name}\\n')
    f.write(f'**Duration**: {total_duration / 60:.1f} minutes\\n')
    f.write(f'**Completed**: {time.strftime(\"%Y-%m-%d %H:%M:%S\")}\\n\\n')
    
    f.write('## ğŸ¯ Workflow Phases Completed\\n\\n')
    f.write('âœ… **Phase 1**: Environment Setup and Validation\\n')
    f.write('âœ… **Phase 2**: Project Planning and Feature Selection\\n')
    f.write('âœ… **Phase 3**: AI-Powered Development\\n')
    f.write('âœ… **Phase 4**: Code Quality Analysis and Validation\\n')
    f.write('âœ… **Phase 5**: Code Execution and Testing\\n')
    f.write('âœ… **Phase 6**: Development Metrics and Visualization\\n')
    f.write('âœ… **Phase 7**: Git Integration and Documentation\\n\\n')
    
    f.write('## ğŸ“‹ Codomyrmex Modules Demonstrated\\n\\n')
    f.write('- ğŸ”§ **environment_setup**: Environment validation and setup\\n')
    f.write('- ğŸ“ **logging_monitoring**: Comprehensive workflow logging\\n')
    f.write('- ğŸ¤– **ai_code_editing**: AI-powered code generation and refinement\\n')
    f.write('- ğŸ” **static_analysis**: Code quality analysis and validation\\n')
    f.write('- âš¡ **code**: Code execution, sandboxing, review, and monitoring\\n')
    f.write('- ğŸ“Š **data_visualization**: Metrics dashboards and development charts\\n')
    f.write('- ğŸŒ³ **git_operations**: Version control integration\\n\\n')
    
    f.write('## ğŸ“ Generated Artifacts\\n\\n')
    
    # List all files in output directory
    output_path = Path(output_dir)
    all_files = []
    
    for file_path in output_path.rglob('*'):
        if file_path.is_file():
            relative_path = file_path.relative_to(output_path)
            all_files.append(str(relative_path))
    
    all_files.sort()
    
    f.write('### ğŸ¯ Generated Project Files\\n')
    project_files = [f for f in all_files if f.startswith('generated_project/')]
    for file in project_files:
        f.write(f'- `{file}`\\n')
    
    f.write('\\n### ğŸ“Š Analysis and Reports\\n')
    report_files = [f for f in all_files if not f.startswith('generated_project/')]
    for file in report_files:
        f.write(f'- `{file}`\\n')
    
    f.write('\\n## ğŸ‰ Workflow Success\\n\\n')
    f.write('This comprehensive workflow demonstrated the power of Codomyrmex\\'s\\n')
    f.write('modular architecture by seamlessly integrating multiple modules\\n')
    f.write('into a complete AI-enhanced development lifecycle.\\n\\n')
    
    f.write('**Key Achievements:**\\n')
    f.write('- âœ… End-to-end AI-powered development\\n')
    f.write('- âœ… Comprehensive quality analysis\\n') 
    f.write('- âœ… Secure code execution and validation\\n')
    f.write('- âœ… Rich metrics and visualization dashboards\\n')
    f.write('- âœ… Complete documentation and reporting\\n')
    f.write('- âœ… Git integration and version control\\n\\n')
    
    f.write('## ğŸ”— Next Steps\\n\\n')
    f.write('1. **Review Generated Code**: Examine the AI-generated project files\\n')
    f.write('2. **Analyze Metrics**: Study the development metrics and visualizations\\n')
    f.write('3. **Quality Improvements**: Address any issues found in quality analysis\\n')
    f.write('4. **Extend Functionality**: Build upon the generated foundation\\n')
    f.write('5. **Share Results**: Use insights for future development projects\\n')

logger.info('ğŸ“š Final documentation generated')
print(f'\\nğŸ“„ Comprehensive workflow report: WORKFLOW_COMPLETE.md')
print(f'ğŸ“Š Total files generated: {len(all_files)}')
"

show_success "Final documentation completed"

# Final Summary
DEMO_END_TIME=$(date +%s)
DEMO_DURATION=$((DEMO_END_TIME - DEMO_START_TIME))

echo ""
echo -e "${CYAN}â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—${NC}"
echo -e "${CYAN}â•‘               ğŸ‰ WORKFLOW ORCHESTRATION COMPLETE ğŸ‰       â•‘${NC}"
echo -e "${CYAN}â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•${NC}"
echo ""

echo -e "${GREEN}ğŸš€ Complete development workflow orchestrated in $(($DEMO_DURATION / 60)) minutes $(($DEMO_DURATION % 60)) seconds${NC}"
echo -e "${GREEN}ğŸ“ All results and artifacts available at: $OUTPUT_DIR${NC}"
echo ""

echo -e "${WHITE}ğŸ¯ Workflow Accomplishments:${NC}"
echo -e "${GREEN}  âœ… AI-powered project generation ($PROJECT_NAME)${NC}"
echo -e "${GREEN}  âœ… Comprehensive code quality analysis and validation${NC}"
echo -e "${GREEN}  âœ… Secure code execution and testing in sandbox environment${NC}"
echo -e "${GREEN}  âœ… Rich development metrics and visualization dashboards${NC}"
echo -e "${GREEN}  âœ… Complete documentation and reporting generation${NC}"
echo -e "${GREEN}  âœ… Git integration with branch creation and commit history${NC}"
echo ""

echo -e "${YELLOW}ğŸ’ Codomyrmex Modules Demonstrated:${NC}"
echo -e "${CYAN}  ğŸ”§ environment_setup${NC} - Environment validation and configuration"
echo -e "${CYAN}  ğŸ“ logging_monitoring${NC} - Structured logging throughout workflow"
echo -e "${CYAN}  ğŸ¤– ai_code_editing${NC} - AI-powered code generation and refinement"
echo -e "${CYAN}  ğŸ” static_analysis${NC} - Code quality analysis and validation"
echo -e "${CYAN}  âš¡ code${NC} - Code execution, sandboxing, review, and monitoring"
echo -e "${CYAN}  ğŸ“Š data_visualization${NC} - Metrics dashboards and charts"
echo -e "${CYAN}  ğŸŒ³ git_operations${NC} - Version control integration"
echo ""

echo -e "${BLUE}ğŸ”— Key Artifacts Generated:${NC}"
echo "  ğŸ“ Generated project code with tests and documentation"
echo "  ğŸ“Š Development metrics dashboard with 6+ visualizations" 
echo "  ğŸ” Code quality analysis reports and recommendations"
echo "  âš¡ Execution results and testing validation"
echo "  ğŸ“š Comprehensive workflow documentation and summary"
echo ""

echo -e "${BLUE}ğŸ“– Next Steps:${NC}"
echo "  ğŸ” Explore: open $OUTPUT_DIR"
echo "  ğŸ“„ Read: $OUTPUT_DIR/WORKFLOW_COMPLETE.md"
echo "  ğŸ“Š Review: $OUTPUT_DIR/development_dashboard.md" 
echo "  ğŸš€ Build: Extend the generated project with additional features"
echo ""

echo -e "${GREEN}ğŸ‰ This orchestrator showcased the full power of Codomyrmex's modular${NC}"
echo -e "${GREEN}   architecture in a real-world AI-enhanced development workflow!${NC}"

