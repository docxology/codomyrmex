{
  "output": {
    "format": "json",
    "file": "output/language_models_results.json",
    "directory": "output/language_models"
  },
  "logging": {
    "level": "INFO",
    "file": "logs/language_models_example.log",
    "format": "%(asctime)s - %(name)s - %(levelname)s - %(levelname)s - %(message)s"
  },
  "ollama": {
    "host": "localhost",
    "port": 11434,
    "timeout": 300,
    "max_retries": 3,
    "default_model": "llama3.1:latest",
    "auto_pull_models": false,
    "model_cache_dir": "~/.ollama/models",
    "preferred_models": [
      "llama3.1:latest",
      "codellama:latest",
      "mistral:latest"
    ],
    "max_concurrent_requests": 1,
    "request_timeout": 300,
    "stream_responses": false
  },
  "llm_config": {
    "temperature": 0.7,
    "max_tokens": 1000,
    "top_p": 0.9,
    "top_k": 40,
    "repeat_penalty": 1.1,
    "context_window": 4096,
    "batch_size": 1,
    "use_gpu": true,
    "output_format": "json",
    "include_metadata": true,
    "save_responses": true
  },
  "model_testing": {
    "run_generation_tests": true,
    "run_chat_tests": true,
    "test_streaming": true,
    "compare_models": false,
    "prompts_count": 4,
    "chat_scenarios_count": 2,
    "configurations_count": 4,
    "measure_response_times": true,
    "collect_token_counts": true,
    "track_model_usage": true
  },
  "demonstration": {
    "show_configuration_management": true,
    "demonstrate_generation": true,
    "demonstrate_chat": true,
    "demonstrate_streaming": true,
    "use_mock_data_when_unavailable": true,
    "mock_response_length": 150,
    "mock_generation_time": 0.1,
    "export_results": true,
    "export_summaries": true,
    "include_performance_metrics": true
  },
  "integration": {
    "use_logging_monitoring": true,
    "use_performance_monitoring": true,
    "graceful_failures": true,
    "detailed_error_reporting": true,
    "cleanup_temp_files": true,
    "memory_efficient_mode": false
  }
}
