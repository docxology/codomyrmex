#!/usr/bin/env python3
"""
Basic Ollama Usage Example

This script demonstrates the simplest way to use the Codomyrmex Ollama integration
for basic model execution and output management.
"""

import sys
from pathlib import Path

# Add src to path for imports
sys.path.insert(0, str(Path(__file__).parent / "src"))

try:
    from codomyrmex.ollama_integration import OllamaManager, ModelRunner
    from codomyrmex.ollama_integration.model_runner import ExecutionOptions
except ImportError as e:
    print(f"‚ùå Ollama integration not available: {e}")
    print("Please ensure Ollama is installed and the integration module is properly set up")
    sys.exit(1)


def basic_model_execution():
    """Demonstrate basic model execution."""
    print("ü§ñ Basic Ollama Model Execution")
    print("=" * 40)

    # Initialize managers
    manager = OllamaManager()
    runner = ModelRunner(manager)

    # List available models
    models = manager.list_models()
    print(f"üìã Available models: {len(models)}")

    if not models:
        print("‚ùå No models available. Please download some models first:")
        print("   ollama run llama3.1:8b")
        return

    # Use the first available model
    model_name = models[0].name
    print(f"üß™ Using model: {model_name}")

    # Simple prompt
    prompt = "Explain what artificial intelligence is in one sentence."

    print(f"\nüìù Prompt: {prompt}")

    # Run the model
    result = runner.run_with_options(
        model_name,
        prompt,
        ExecutionOptions(temperature=0.7, max_tokens=100)
    )

    if result.success:
        print("\n‚úÖ Response:")
        print(result.response)
        print(f"\n‚è±Ô∏è  Execution time: {result.execution_time:.2f} seconds")
    else:
        print(f"\n‚ùå Execution failed: {result.error_message}")


def model_comparison_example():
    """Demonstrate model comparison."""
    print("\n\n‚öñÔ∏è  Model Comparison Example")
    print("=" * 40)

    manager = OllamaManager()
    runner = ModelRunner(manager)

    # Get available models
    models = manager.list_models()

    if len(models) < 2:
        print("‚ö†Ô∏è  Need at least 2 models for comparison")
        return

    # Compare first two models
    model_names = [models[0].name, models[1].name]
    prompt = "What is the capital of France?"

    print(f"üîç Comparing: {model_names}")
    print(f"üìù Prompt: {prompt}")

    comparison = runner.create_model_comparison(
        model_names,
        prompt,
        ExecutionOptions(max_tokens=50, temperature=0.1)
    )

    print("\nüìä Results:")
    for model_name, result in comparison['results'].items():
        if result['success']:
            print(f"   ‚úÖ {model_name}: {result['response_preview']}")
        else:
            print(f"   ‚ùå {model_name}: {result['error']}")


def batch_processing_example():
    """Demonstrate batch processing."""
    print("\n\nüì¶ Batch Processing Example")
    print("=" * 40)

    manager = OllamaManager()
    runner = ModelRunner(manager)

    # Use a small model for quick demo
    test_model = "smollm:135m"

    if not manager.is_model_available(test_model):
        print(f"‚ö†Ô∏è  Model {test_model} not available, using first available model")
        models = manager.list_models()
        if models:
            test_model = models[0].name
        else:
            print("‚ùå No models available")
            return

    print(f"üöÄ Running batch with model: {test_model}")

    # Multiple prompts
    prompts = [
        "What is Python?",
        "Explain machine learning.",
        "What are the benefits of open source?",
        "How does the internet work?"
    ]

    print(f"üìù Processing {len(prompts)} prompts...")

    results = runner.run_batch(
        test_model,
        prompts,
        ExecutionOptions(max_tokens=75, temperature=0.7),
        max_concurrent=2
    )

    print("\n‚úÖ Results:")
    for i, result in enumerate(results):
        if result.success:
            print(f"   {i+1}. ‚úÖ {len(result.response)} chars in {result.execution_time:.2f}s")
        else:
            print(f"   {i+1}. ‚ùå {result.error_message}")


def main():
    """Run all basic usage examples."""
    print("üêô CODOMYRMEX OLLAMA BASIC USAGE EXAMPLES")
    print("=" * 50)
    print("Simple demonstrations of Ollama integration")
    print("=" * 50)

    # Run examples
    basic_model_execution()
    model_comparison_example()
    batch_processing_example()

    print("\nüéâ Basic usage examples completed!")
    print("üí° For advanced usage, see: examples/ollama_integration_demo.py")
    print("üìö For configuration management, see the ConfigManager class")


if __name__ == "__main__":
    main()
