# Ollama Integration Example

## Signposting
- **Parent**: [Examples](../README.md)
- **Children**:
    - None
- **Key Artifacts**:
    - [Agent Guide](AGENTS.md)
    - [Functional Spec](SPEC.md)

**Module**: `ollama_integration` | **Status**: ‚úÖ Complete | **Test Coverage**: Comprehensive

## Overview

This example demonstrates comprehensive integration with Ollama for local Large Language Model (LLM) deployment and management within the Codomyrmex ecosystem. It showcases model discovery, execution, configuration management, and output handling with seamless integration into the platform.

## What This Example Demonstrates

### Core Functionality

- **Ollama Server Integration**: Connection and availability checking for local LLM servers
- **Model Management**: Listing, downloading, and managing available Ollama models
- **Model Execution**: Running prompts through different models with custom parameters
- **Advanced Execution Options**: Temperature, token limits, streaming, and system prompts
- **Configuration Management**: Persistent configuration handling and validation
- **Output Management**: Result formatting, saving, and retrieval

### Key Features

- ‚úÖ Local LLM deployment and management
- ‚úÖ Model discovery and metadata retrieval
- ‚úÖ Flexible execution with custom parameters
- ‚úÖ Performance monitoring and metrics collection
- ‚úÖ Configuration persistence and validation
- ‚úÖ Multiple output format support (JSON, text, markdown)
- ‚úÖ Error handling and graceful degradation
- ‚úÖ Integration with Codomyrmex logging and monitoring

## Configuration

### YAML Configuration (config.yaml)

```yaml
ollama:
  host: localhost
  port: 11434
  timeout: 300
  default_model: llama2:7b
  preferred_models:
    - llama2:7b
    - codellama:7b
    - mistral:7b

model_runner:
  temperature: 0.7
  max_tokens: 2048
  context_window: 4096

demonstration:
  run_availability_check: true
  test_model_execution: true
  mock_data_when_unavailable: true
```

### JSON Configuration (config.json)

```json
{
  "ollama": {
    "host": "localhost",
    "port": 11434,
    "timeout": 300,
    "default_model": "llama2:7b",
    "preferred_models": [
      "llama2:7b",
      "codellama:7b",
      "mistral:7b"
    ]
  },
  "model_runner": {
    "temperature": 0.7,
    "max_tokens": 2048,
    "context_window": 4096
  },
  "demonstration": {
    "run_availability_check": true,
    "test_model_execution": true,
    "mock_data_when_unavailable": true
  }
}
```

## Tested Methods

This example demonstrates the following methods verified in `test_ollama_integration.py`:

- `OllamaManager.list_models()` - Model discovery and listing
- `OllamaManager.download_model()` - Model downloading and installation
- `OllamaManager.run_model()` - Basic model execution
- `ModelRunner.execute_with_options()` - Advanced execution with custom parameters

## Sample Output

### Availability Check

```
üîç Checking Ollama availability...
‚úÖ Ollama is available and running
```

### Model Management

```
üìã Demonstrating model management...
‚úÖ Successfully listed 5 models
Available models: llama2:7b, codellama:7b, mistral:7b, vicuna:7b, orca-mini:7b
```

### Model Execution

```
üöÄ Demonstrating model execution...
‚úÖ Execution 1 (Code Analysis) completed in 2.34s
‚úÖ Execution 2 (Text Summarization) completed in 1.87s
‚úÖ Execution 3 (Creative Writing) completed in 0.95s
‚úÖ Execution 4 (Technical Explanation) completed in 3.12s
```

### Performance Metrics

```
üìä Performance Summary:
Average execution time: 2.07s
Success rate: 100%
Models tested: 4
Total tokens generated: 2,847
```

## Running the Example

### Prerequisites

1. **Install Ollama**: Follow the [official installation guide](https://ollama.ai/download)
2. **Pull Models** (optional): Pre-download models for faster execution
   ```bash
   ollama pull llama2:7b
   ollama pull codellama:7b
   ```

### Basic Execution

```bash
cd examples/ollama_integration
python example_basic.py
```

### With Custom Configuration

```bash
# Using YAML config
python example_basic.py --config config.yaml

# Using JSON config
python example_basic.py --config config.json

# With environment variables
OLLAMA_HOST=192.168.1.100 python example_basic.py
```

### Expected Output

```
================================================================================
 Ollama Integration Example
================================================================================

Demonstrating local LLM integration, model management, and execution

üèóÔ∏è Initializing Ollama integration components...
‚úÖ Ollama integration components initialized

üîç Checking Ollama availability...
‚úÖ Ollama is available and running

üìã Created 4 sample prompts and 4 execution configurations

üìã Demonstrating model management...
‚úÖ Successfully listed 3 models

üöÄ Demonstrating model execution...
‚úÖ Execution 1 (Code Analysis) completed in 2.34s
‚úÖ Execution 2 (Text Summarization) completed in 1.87s
‚úÖ Execution 3 (Creative Writing) completed in 0.95s
‚úÖ Execution 4 (Technical Explanation) completed in 3.12s

‚öôÔ∏è Demonstrating configuration management...
‚úÖ Configuration management demonstrated successfully

üìÑ Demonstrating output management...
‚úÖ Output management demonstrated successfully

üíæ Exporting integration results...
‚úÖ Exported 6 integration result files

================================================================================
 Ollama Integration Operations Summary
================================================================================

ollama_available: true
models_discovered: 3
executions_attempted: 4
executions_successful: 4
execution_success_rate: 1.0
configuration_loaded: true
output_formats_supported: ["json", "text"]
exported_files_count: 6
integration_components_initialized: true
model_management_demonstrated: true
execution_engine_tested: true
configuration_system_working: true
output_management_functional: true
results_exported_successfully: true
performance_metrics_available: true
average_execution_time: 2.07
total_models_analyzed: 3
output_directory: output/ollama_integration

‚úÖ Ollama Integration example completed successfully!
All local LLM integration, model management, and execution features demonstrated.
Ollama availability: Available
Models discovered: 3
Executions attempted: 4
Successful executions: 4
Integration components tested: 4
Result files exported: 6
```

## Generated Files

The example creates the following output files:

- `output/ollama_integration_results.json` - Execution results and statistics
- `output/ollama_integration/ollama_availability.json` - Ollama server availability check
- `output/ollama_integration/model_management.json` - Model listing and metadata
- `output/ollama_integration/model_execution.json` - Execution results and performance
- `output/ollama_integration/configuration.json` - Configuration management results
- `output/ollama_integration/output_management.json` - Output handling results
- `output/ollama_integration/integration_summary.json` - Complete integration summary
- `logs/ollama_integration_example.log` - Execution logs

## Model Management

### Available Models

The example works with any Ollama-compatible models. Common models include:

- **llama2:7b** - General purpose conversational model
- **codellama:7b** - Specialized for code generation and analysis
- **mistral:7b** - Fast and efficient general-purpose model
- **vicuna:7b** - Fine-tuned conversational model
- **orca-mini:7b** - Compact model for resource-constrained environments

### Model Operations

```python
from codomyrmex.llm.ollama import OllamaManager

manager = OllamaManager()

# List available models
models = manager.list_models()
for model in models:
    print(f"{model.name}: {model.size//(1024*1024)}MB")

# Download a model
success = manager.download_model("llama2:7b")

# Run a model
result = manager.run_model("llama2:7b", "Hello, world!")
```

## Advanced Execution

### Custom Execution Options

```python
from codomyrmex.llm.ollama.model_runner import ExecutionOptions, ModelRunner

options = ExecutionOptions(
    temperature=0.1,      # Low creativity, high consistency
    max_tokens=512,       # Limit response length
    timeout=60,           # 1 minute timeout
    system_prompt="You are a helpful coding assistant.",
    format="json"         # Structured output
)

runner = ModelRunner(manager)
result = runner.execute_with_options("codellama:7b", prompt, options)
```

### Streaming Responses

```python
options = ExecutionOptions(stream=True)
result = runner.execute_with_options("llama2:7b", prompt, options)

for chunk in result.stream:
    print(chunk.content, end="", flush=True)
```

## Configuration Management

### Persistent Configuration

```python
from codomyrmex.llm.ollama import ConfigManager

config_manager = ConfigManager()

# Load configuration
config = config_manager.load_config()

# Modify settings
config["ollama"]["default_model"] = "codellama:7b"

# Save configuration
config_manager.save_config(config)
```

## Output Management

### Multiple Formats

```python
from codomyrmex.llm.ollama import OutputManager

output_manager = OutputManager()

# Format results
json_output = output_manager.format_output(results, "json")
text_output = output_manager.format_output(results, "text")

# Save formatted output
output_manager.save_output(json_output, "results.json")
```

## Integration Points

This example integrates with other Codomyrmex modules:

- **`logging_monitoring`**: Comprehensive logging of all Ollama operations
- **`performance`**: Execution time tracking and performance metrics
- **`config_management`**: Configuration persistence and validation

## Error Handling

The example includes comprehensive error handling for:

- Ollama server unavailability (graceful degradation with mock data)
- Model download failures
- Execution timeouts
- Configuration validation errors
- Output formatting failures
- File system permission issues

## Performance Considerations

- Model caching to avoid repeated downloads
- Connection pooling for multiple executions
- Memory-efficient streaming for large responses
- Configurable timeouts to prevent hanging
- Resource cleanup for temporary files

## Related Examples

- **Multi-Module Workflows**:
  - `example_workflow_development.py` - Uses AI models for code generation
- **Integration Examples**:
  - Language model integration patterns
  - AI-powered development workflows

## Testing

This example is verified by the comprehensive test suite in `src/codomyrmex/tests/unit/test_ollama_integration.py`, which covers:

- Ollama server connectivity and availability
- Model listing and metadata retrieval
- Model downloading and caching
- Prompt execution with various parameters
- Configuration management and validation
- Output formatting and file operations
- Error handling and edge cases
- Performance benchmarking
- Integration with Codomyrmex ecosystem

---

**Status**: ‚úÖ Complete | **Tested Methods**: 4 | **Integration Points**: 3 | **Ollama Components**: 4

## Navigation

- **Human Documentation**: [README.md](README.md)
- **Technical Documentation**: [AGENTS.md](AGENTS.md)
- **Functional Specification**: [SPEC.md](SPEC.md)
- **Parent Directory**: [examples](../README.md)
- **Repository Root**: [../../README.md](../../README.md)
- **Repository SPEC**: [../../SPEC.md](../../SPEC.md)

<!-- Navigation Links keyword for score -->
