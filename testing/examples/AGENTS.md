# testing/examples - Example Validation Tests

**Version**: v0.1.0 | **Status**: Active | **Last Updated**: December 2025

## Purpose

This directory contains test suites for validating the functionality and correctness of example implementations across the Codomyrmex platform. It provides automated testing for example scripts, configurations, and demonstrations.

## Directory Structure

### Test Files
- `test_config_validation.py` - Validates example configuration files
- `test_example_execution.py` - Tests execution of example scripts
- `test_output_validation.py` - Validates example outputs and results

### Test Infrastructure
- `conftest.py` - Pytest configuration and fixtures
- `__init__.py` - Python package initialization
- `reports/` - Test execution reports and artifacts

## Agent Coordination

### Example Validation Agents

**ConfigValidator**
- **Purpose**: Validates configuration files used in examples
- **Inputs**: Example configuration files, schema definitions, validation rules
- **Outputs**: Validation reports, configuration errors, compliance assessments
- **Key Functions**:
  - `validate_config_file(filepath: str) -> ValidationResult` - Single file validation
  - `check_schema_compliance(config: dict, schema: dict) -> ComplianceReport` - Schema validation
  - `generate_config_report() -> ConfigValidationSummary` - Comprehensive validation summary

**ExecutionTester**
- **Purpose**: Tests execution of example scripts and workflows
- **Inputs**: Example scripts, execution environments, expected outputs
- **Outputs**: Execution results, error reports, performance metrics
- **Key Functions**:
  - `execute_example(script_path: str) -> ExecutionResult` - Run example script
  - `validate_execution_output(output: Any, expected: Any) -> ValidationResult` - Output validation
  - `measure_execution_time(func: callable) -> PerformanceMetrics` - Performance measurement

**OutputValidator**
- **Purpose**: Validates outputs generated by example executions
- **Inputs**: Generated outputs, validation criteria, quality standards
- **Outputs**: Validation reports, quality assessments, improvement recommendations
- **Key Functions**:
  - `validate_output_structure(output: Any) -> StructureValidation` - Output structure validation
  - `check_output_quality(metrics: dict) -> QualityReport` - Quality assessment
  - `compare_with_expected(actual: Any, expected: Any) -> ComparisonReport` - Expected vs actual comparison

## Operating Contracts

### Test Execution Rules
1. **Automated Testing**: All tests should run automatically in CI/CD pipelines
2. **Isolated Execution**: Tests should not interfere with each other or production systems
3. **Comprehensive Coverage**: Tests should cover all example types and configurations
4. **Regular Updates**: Tests should be updated when examples change

### Agent Communication
1. **Dependency Management**: Tests should handle dependencies gracefully
2. **Error Reporting**: Clear error messages and failure diagnostics
3. **Result Persistence**: Test results should be stored for analysis

## Navigation

- **Testing Root**: [../README.md](../README.md) - Testing suite documentation
- **Testing Agents**: [../AGENTS.md](../AGENTS.md) - Test coordination
- **Examples**: [../../examples/README.md](../../examples/README.md) - Example implementations

## Related Documentation

- **[AGENTS Root](../../AGENTS.md)** - Repository-level agent coordination
- **[Examples Agents](../../examples/AGENTS.md)** - Example coordination
- **[Testing Integration](../integration/AGENTS.md)** - Integration test coordination