"""
Comprehensive Ollama Integration Tests

Tests all aspects of the Ollama integration including:
- Model management and listing
- Model execution and response handling
- Configuration management and saving
- Output management and file operations
- Integration with Codomyrmex modules
- Real execution tests (no mocks)
"""

import unittest
import time
import json
import tempfile
import shutil
from pathlib import Path
from unittest.mock import patch, MagicMock

# Import Ollama integration
try:
    from codomyrmex.ollama_integration import (
        OllamaManager,
        ModelRunner,
        OutputManager,
        ConfigManager
    )
    from codomyrmex.ollama_integration.model_runner import ExecutionOptions
    OLLAMA_AVAILABLE = True
except ImportError as e:
    print(f"Ollama integration not available: {e}")
    OLLAMA_AVAILABLE = False

# Import Codomyrmex modules for integration testing
try:
    from codomyrmex.logging_monitoring import get_logger
    LOGGING_AVAILABLE = True
except ImportError:
    LOGGING_AVAILABLE = False


class TestOllamaIntegration(unittest.TestCase):
    """Comprehensive tests for Ollama integration."""

    def setUp(self):
        """Set up test environment."""
        if not OLLAMA_AVAILABLE:
            self.skipTest("Ollama integration not available")

        self.test_output_dir = Path("testing/output/ollama_tests")
        self.test_output_dir.mkdir(parents=True, exist_ok=True)

        # Initialize managers
        self.ollama_manager = OllamaManager()
        self.model_runner = ModelRunner(self.ollama_manager)
        self.output_manager = OutputManager(str(self.test_output_dir))
        self.config_manager = ConfigManager()

        # Test data
        self.test_model = "smollm:135m"  # Small model for quick testing
        self.test_prompts = [
            "What is artificial intelligence?",
            "Explain machine learning in simple terms.",
            "What are the benefits of using local LLMs?"
        ]

    def tearDown(self):
        """Clean up after tests."""
        # Clean up test outputs
        if self.test_output_dir.exists():
            shutil.rmtree(self.test_output_dir, ignore_errors=True)

    def test_ollama_server_connectivity(self):
        """Test that Ollama server is running and accessible."""
        # This is a basic connectivity test
        try:
            models = self.ollama_manager.list_models()
            self.assertIsInstance(models, list)
            # Should have at least one model if server is working
            self.assertGreaterEqual(len(models), 0)
        except Exception as e:
            self.fail(f"Ollama server connectivity test failed: {e}")

    def test_model_listing(self):
        """Test model listing functionality."""
        models = self.ollama_manager.list_models()

        # Should return a list
        self.assertIsInstance(models, list)

        # If models exist, validate structure
        if models:
            model = models[0]
            self.assertTrue(hasattr(model, 'name'))
            self.assertTrue(hasattr(model, 'size'))
            self.assertTrue(hasattr(model, 'modified'))
            self.assertTrue(hasattr(model, 'status'))

    def test_model_availability_checking(self):
        """Test model availability checking."""
        # Test with known available model
        if self.test_model:
            available = self.ollama_manager.is_model_available(self.test_model)
            self.assertIsInstance(available, bool)

        # Test with non-existent model
        not_available = self.ollama_manager.is_model_available("nonexistent_model_xyz123")
        self.assertFalse(not_available)

    def test_model_statistics(self):
        """Test model statistics generation."""
        stats = self.ollama_manager.get_model_stats()

        self.assertIsInstance(stats, dict)
        self.assertIn('total_models', stats)
        self.assertIn('total_size_mb', stats)
        self.assertIn('models_by_family', stats)

        # Validate data types
        self.assertIsInstance(stats['total_models'], int)
        self.assertIsInstance(stats['total_size_mb'], float)
        self.assertIsInstance(stats['models_by_family'], dict)

    def test_model_execution_basic(self):
        """Test basic model execution."""
        if not self.ollama_manager.is_model_available(self.test_model):
            self.skipTest(f"Test model {self.test_model} not available")

        result = self.ollama_manager.run_model(
            self.test_model,
            "Hello, how are you?",
            save_output=False  # Don't save for basic test
        )

        # Validate result structure
        self.assertIsNotNone(result)
        self.assertTrue(hasattr(result, 'model_name'))
        self.assertTrue(hasattr(result, 'prompt'))
        self.assertTrue(hasattr(result, 'response'))
        self.assertTrue(hasattr(result, 'execution_time'))
        self.assertTrue(hasattr(result, 'success'))

        # Validate data
        self.assertEqual(result.model_name, self.test_model)
        self.assertEqual(result.prompt, "Hello, how are you?")
        self.assertIsInstance(result.response, str)
        self.assertIsInstance(result.execution_time, float)
        self.assertIsInstance(result.success, bool)

        if result.success:
            self.assertGreater(len(result.response), 0)
            self.assertGreater(result.execution_time, 0)

    def test_model_execution_with_options(self):
        """Test model execution with execution options."""
        if not self.ollama_manager.is_model_available(self.test_model):
            self.skipTest(f"Test model {self.test_model} not available")

        options = ExecutionOptions(
            temperature=0.7,
            max_tokens=100,
            timeout=60
        )

        result = self.model_runner.run_with_options(
            self.test_model,
            "Explain quantum computing briefly.",
            options,
            save_output=False
        )

        self.assertIsNotNone(result)
        self.assertTrue(result.success)
        self.assertGreater(len(result.response), 0)

    def test_batch_execution(self):
        """Test batch execution of multiple prompts."""
        if not self.ollama_manager.is_model_available(self.test_model):
            self.skipTest(f"Test model {self.test_model} not available")

        results = self.model_runner.run_batch(
            self.test_model,
            self.test_prompts[:2],  # Test with first 2 prompts
            ExecutionOptions(max_tokens=50, temperature=0.7),
            max_concurrent=2
        )

        # Validate results
        self.assertIsInstance(results, list)
        self.assertEqual(len(results), 2)

        for result in results:
            self.assertIsNotNone(result)
            self.assertEqual(result.model_name, self.test_model)
            self.assertIn(result.prompt, self.test_prompts[:2])
            self.assertIsInstance(result.success, bool)

    def test_model_comparison(self):
        """Test model comparison functionality."""
        models = self.ollama_manager.list_models()

        if len(models) < 2:
            self.skipTest("Need at least 2 models for comparison")

        # Use first two models for comparison
        model_names = [models[0].name, models[1].name]
        test_prompt = "What is the capital of France?"

        comparison = self.model_runner.create_model_comparison(
            model_names,
            test_prompt,
            ExecutionOptions(max_tokens=50, temperature=0.1)
        )

        # Validate comparison structure
        self.assertIsInstance(comparison, dict)
        self.assertIn('test_prompt', comparison)
        self.assertIn('models_compared', comparison)
        self.assertIn('results', comparison)
        self.assertIn('summary', comparison)

        self.assertEqual(comparison['test_prompt'], test_prompt)
        self.assertEqual(comparison['models_compared'], len(comparison['results']))

        # Validate results for each model
        for model_name in model_names:
            self.assertIn(model_name, comparison['results'])
            model_result = comparison['results'][model_name]
            self.assertIn('success', model_result)
            self.assertIn('execution_time', model_result)
            self.assertIn('response_length', model_result)

    def test_output_saving(self):
        """Test output saving functionality."""
        # Save a test output
        test_response = "This is a test response for output saving."
        test_prompt = "Test prompt for output saving."

        output_path = self.output_manager.save_model_output(
            self.test_model,
            test_prompt,
            test_response,
            1.5,
            metadata={'test': True, 'category': 'unit_test'}
        )

        # Validate file was created
        self.assertIsInstance(output_path, str)
        output_file = Path(output_path)
        self.assertTrue(output_file.exists())
        self.assertTrue(output_file.is_file())

        # Validate file contents
        with open(output_file, 'r', encoding='utf-8') as f:
            content = f.read()

        self.assertIn(self.test_model, content)
        self.assertIn(test_prompt, content)
        self.assertIn(test_response, content)
        self.assertIn("1.5", content)  # execution time

    def test_configuration_management(self):
        """Test configuration management functionality."""
        # Test default configuration
        self.assertIsNotNone(self.config_manager.config)
        self.assertEqual(self.config_manager.config.ollama_binary, "ollama")

        # Test configuration update
        test_config = {
            'default_model': 'test_model',
            'auto_start_server': False,
            'base_output_dir': str(self.test_output_dir / 'custom')
        }

        success = self.config_manager.update_config(**test_config)
        self.assertTrue(success)

        # Verify changes
        self.assertEqual(self.config_manager.config.default_model, 'test_model')
        self.assertFalse(self.config_manager.config.auto_start_server)

        # Test configuration validation
        validation = self.config_manager.validate_config()
        self.assertIsInstance(validation, dict)
        self.assertIn('valid', validation)

    def test_model_configuration_save_load(self):
        """Test saving and loading model-specific configurations."""
        test_model = "test_model_123"
        test_config = {
            'temperature': 0.8,
            'max_tokens': 512,
            'timeout': 120,
            'description': 'Test model configuration'
        }

        # Save configuration
        config_path = self.output_manager.save_model_config(test_model, test_config, "test_config")
        self.assertIsInstance(config_path, str)
        self.assertTrue(Path(config_path).exists())

        # Load configuration
        loaded_config = self.output_manager.load_model_config(test_model, "test_config")
        self.assertIsNotNone(loaded_config)
        self.assertEqual(loaded_config['temperature'], 0.8)
        self.assertEqual(loaded_config['max_tokens'], 512)

    def test_execution_presets(self):
        """Test execution option presets."""
        presets = self.config_manager.get_execution_presets()

        self.assertIsInstance(presets, dict)
        self.assertIn('fast', presets)
        self.assertIn('creative', presets)
        self.assertIn('balanced', presets)

        # Validate preset structure
        for preset_name, preset_options in presets.items():
            self.assertIsInstance(preset_options, ExecutionOptions)
            self.assertIsInstance(preset_options.temperature, float)
            self.assertIsInstance(preset_options.max_tokens, int)
            self.assertIsInstance(preset_options.timeout, int)

    def test_output_statistics(self):
        """Test output statistics generation."""
        # Create some test outputs first
        for i in range(3):
            self.output_manager.save_model_output(
                f"test_model_{i}",
                f"test_prompt_{i}",
                f"test_response_{i}",
                1.0,
                metadata={'test_run': i}
            )

        stats = self.output_manager.get_output_stats()

        self.assertIsInstance(stats, dict)
        self.assertIn('total_outputs', stats)
        self.assertIn('total_size', stats)
        self.assertIn('by_type', stats)
        self.assertIn('by_model', stats)

        # Should have at least some outputs (may be more due to other tests)
        self.assertGreaterEqual(stats['total_outputs'], 0)

    def test_conversation_execution(self):
        """Test conversational model execution."""
        if not self.ollama_manager.is_model_available(self.test_model):
            self.skipTest(f"Test model {self.test_model} not available")

        # Test conversation format
        messages = [
            {"role": "system", "content": "You are a helpful assistant."},
            {"role": "user", "content": "Hello, how are you?"},
            {"role": "assistant", "content": "I'm doing well, thank you for asking!"},
            {"role": "user", "content": "What's the weather like?"}
        ]

        result = self.model_runner.run_conversation(
            self.test_model,
            messages,
            ExecutionOptions(max_tokens=100, temperature=0.7)
        )

        self.assertIsNotNone(result)
        if result.success:
            self.assertGreater(len(result.response), 0)

    def test_context_execution(self):
        """Test execution with additional context."""
        if not self.ollama_manager.is_model_available(self.test_model):
            self.skipTest(f"Test model {self.test_model} not available")

        context_docs = [
            "Context 1: Machine learning is a subset of AI.",
            "Context 2: Neural networks are inspired by biological brains."
        ]

        result = self.model_runner.run_with_context(
            self.test_model,
            "Explain how neural networks work.",
            context_docs,
            ExecutionOptions(max_tokens=100, temperature=0.7)
        )

        self.assertIsNotNone(result)
        if result.success:
            self.assertGreater(len(result.response), 0)

    def test_benchmarking(self):
        """Test model benchmarking functionality."""
        if not self.ollama_manager.is_model_available(self.test_model):
            self.skipTest(f"Test model {self.test_model} not available")

        # Test with small prompts for quick benchmarking
        test_prompts = [
            "What is 2+2?",
            "Explain gravity.",
            "What is recursion?"
        ]

        benchmark_results = self.model_runner.benchmark_model(
            self.test_model,
            test_prompts,
            ExecutionOptions(max_tokens=50, temperature=0.7)
        )

        # Validate benchmark structure
        self.assertIsInstance(benchmark_results, dict)
        self.assertIn('model_name', benchmark_results)
        self.assertIn('total_prompts', benchmark_results)
        self.assertIn('successful_runs', benchmark_results)
        self.assertIn('detailed_results', benchmark_results)

        self.assertEqual(benchmark_results['model_name'], self.test_model)
        self.assertEqual(benchmark_results['total_prompts'], len(test_prompts))
        self.assertIsInstance(benchmark_results['detailed_results'], list)

    def test_config_export_import(self):
        """Test configuration export and import."""
        # Export current configuration
        export_path = str(self.test_output_dir / "exported_config.json")
        success = self.config_manager.export_config(export_path)

        self.assertTrue(success)
        self.assertTrue(Path(export_path).exists())

        # Verify export file structure
        with open(export_path, 'r', encoding='utf-8') as f:
            export_data = json.load(f)

        self.assertIn('export_timestamp', export_data)
        self.assertIn('main_config', export_data)
        self.assertIn('execution_presets', export_data)

        # Test import (create new config manager for clean import)
        import_config_manager = ConfigManager()
        import_success = import_config_manager.import_config(export_path)

        self.assertTrue(import_success)

    def test_error_handling(self):
        """Test error handling in various scenarios."""
        # Test with non-existent model
        result = self.ollama_manager.run_model(
            "nonexistent_model_xyz123",
            "test prompt",
            save_output=False
        )

        self.assertFalse(result.success)
        self.assertIn("not available", result.error_message.lower())

        # Test with empty prompt
        result = self.ollama_manager.run_model(
            self.test_model,
            "",
            save_output=False
        )

        # Should handle empty prompt gracefully
        self.assertIsNotNone(result)

    def test_integration_with_logging(self):
        """Test integration with Codomyrmex logging."""
        if not LOGGING_AVAILABLE:
            self.skipTest("Logging module not available")

        # Test that logging integration works
        logger = get_logger("test_ollama_integration")

        # This should not raise an exception
        logger.info("Testing Ollama integration logging")
        logger.warning("This is a test warning")

        # Verify logging doesn't interfere with Ollama operations
        models = self.ollama_manager.list_models()
        self.assertIsInstance(models, list)


class TestOllamaIntegrationRealExecution(unittest.TestCase):
    """Real execution tests that actually run models."""

    def setUp(self):
        """Set up for real execution tests."""
        if not OLLAMA_AVAILABLE:
            self.skipTest("Ollama integration not available")

        self.ollama_manager = OllamaManager()
        self.model_runner = ModelRunner(self.ollama_manager)
        self.output_manager = OutputManager()

        # Use smallest available model for tests
        models = self.ollama_manager.list_models()
        if models:
            self.test_model = min(models, key=lambda m: m.size).name
        else:
            self.test_model = None

    def test_real_model_execution(self):
        """Test real model execution with actual responses."""
        if not self.test_model:
            self.skipTest("No models available for testing")

        print(f"Testing real execution with model: {self.test_model}")

        result = self.ollama_manager.run_model(
            self.test_model,
            "What is the meaning of life?",
            save_output=True,
            output_dir=str(self.output_manager.outputs_dir)
        )

        # Validate real execution
        self.assertTrue(result.success, f"Execution failed: {result.error_message}")
        self.assertGreater(len(result.response), 10)  # Should have substantial response
        self.assertGreater(result.execution_time, 0)

        print(f"✅ Real execution successful: {len(result.response)} characters in {result.execution_time:.2f}s")

    def test_real_batch_execution(self):
        """Test real batch execution."""
        if not self.test_model:
            self.skipTest("No models available for testing")

        print(f"Testing real batch execution with model: {self.test_model}")

        prompts = [
            "Count to 5.",
            "What is 10 + 15?",
            "Explain photosynthesis briefly."
        ]

        results = self.model_runner.run_batch(
            self.test_model,
            prompts,
            ExecutionOptions(max_tokens=50, temperature=0.7),
            max_concurrent=2
        )

        # Validate all executions succeeded
        successful = sum(1 for r in results if r.success)
        self.assertEqual(successful, len(prompts), "Not all batch executions succeeded")

        print(f"✅ Real batch execution: {successful}/{len(prompts)} successful")

    def test_real_model_comparison(self):
        """Test real model comparison."""
        models = self.ollama_manager.list_models()

        if len(models) < 2:
            self.skipTest("Need at least 2 models for comparison")

        print(f"Testing real model comparison with: {[m.name for m in models[:2]]}")

        model_names = [models[0].name, models[1].name]
        prompt = "What is artificial intelligence?"

        comparison = self.model_runner.create_model_comparison(
            model_names,
            prompt,
            ExecutionOptions(max_tokens=75, temperature=0.7)
        )

        # Should have results for both models
        self.assertEqual(comparison['models_compared'], 2)

        successful_comparisons = sum(1 for r in comparison['results'].values() if r['success'])
        self.assertGreater(successful_comparisons, 0, "No successful model comparisons")

        print(f"✅ Real model comparison: {successful_comparisons}/2 successful")

    def test_output_persistence(self):
        """Test that outputs are properly saved and persisted."""
        if not self.test_model:
            self.skipTest("No models available for testing")

        print(f"Testing output persistence with model: {self.test_model}")

        # Run model and save output
        test_prompt = f"Test prompt for output persistence at {time.time()}"
        result = self.ollama_manager.run_model(
            self.test_model,
            test_prompt,
            save_output=True,
            output_dir=str(self.output_manager.outputs_dir)
        )

        self.assertTrue(result.success)

        # Check that output file was created
        output_files = list(self.output_manager.outputs_dir.glob("*.txt"))
        self.assertGreater(len(output_files), 0, "No output files created")

        # Find the most recent output file (should be the one we just created)
        output_file = max(output_files, key=lambda f: f.stat().st_mtime)

        # Verify file contents
        with open(output_file, 'r', encoding='utf-8') as f:
            content = f.read()

        self.assertIn(self.test_model, content)
        # Check for the prompt structure
        self.assertIn("PROMPT:", content)
        self.assertIn("RESPONSE:", content)

        # The response might be different from what we expect due to model behavior,
        # but we should verify that some response was saved
        response_section = content.split("RESPONSE:")[1] if "RESPONSE:" in content else ""
        self.assertGreater(len(response_section.strip()), 0, "No response content found")

        # Check that execution time is recorded
        self.assertIn("EXECUTION TIME:", content)

        print(f"✅ Output persistence verified: {output_file.name}")
        print(f"   Response length: {len(response_section.strip())} characters")


def run_ollama_integration_tests():
    """Run all Ollama integration tests."""
    print("🧪 Running Comprehensive Ollama Integration Tests")
    print("=" * 60)

    # Create test suite
    test_suite = unittest.TestSuite()

    # Add test cases
    test_suite.addTest(unittest.makeSuite(TestOllamaIntegration))
    test_suite.addTest(unittest.makeSuite(TestOllamaIntegrationRealExecution))

    # Run tests with detailed output
    runner = unittest.TextTestRunner(verbosity=2, stream=sys.stdout)
    result = runner.run(test_suite)

    # Print summary
    print("\n" + "=" * 60)
    print("TEST SUMMARY")
    print("=" * 60)

    print(f"Tests run: {result.testsRun}")
    print(f"Failures: {len(result.failures)}")
    print(f"Errors: {len(result.errors)}")
    print(f"Success rate: {((result.testsRun - len(result.failures) - len(result.errors)) / result.testsRun * 100):.1f}%")

    if result.failures:
        print("\n❌ FAILURES:")
        for test, traceback in result.failures:
            print(f"  • {test}: {traceback}")

    if result.errors:
        print("\n💥 ERRORS:")
        for test, traceback in result.errors:
            print(f"  • {test}: {traceback}")

    return result.wasSuccessful()


if __name__ == "__main__":
    import sys
    success = run_ollama_integration_tests()
    sys.exit(0 if success else 1)
